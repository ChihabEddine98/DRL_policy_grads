{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwo9bpaVgxXF"
      },
      "source": [
        "##Setup\n",
        "\n",
        "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CAdiyTKi4Se",
        "outputId": "d9443f0b-7672-4af5-dbd9-b68c453b1fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title mount your Google Drive\n",
        "#@markdown Your work will be stored in a folder called `cs285_f2021` by default to prevent Colab instance timeouts from deleting your edits.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BKE5nA1Fgwwy"
      },
      "outputs": [],
      "source": [
        "#@title set up mount symlink\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs285_f2021'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs285_f2021'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FGK4kbpg3iP",
        "outputId": "4d0e562e-ab0b-4887-f021-4d62fc98956b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [760 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,466 kB]\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,544 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [917 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,986 kB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [42.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,244 kB]\n",
            "Fetched 11.2 MB in 3s (3,489 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "#@title apt install requirements\n",
        "\n",
        "#@markdown Run each section with Shift+Enter\n",
        "\n",
        "#@markdown Double-click on section headers to show code.\n",
        "\n",
        "!apt update \n",
        "!apt install -y --no-install-recommends \\\n",
        "        build-essential \\\n",
        "        curl \\\n",
        "        git \\\n",
        "        gnupg2 \\\n",
        "        make \\\n",
        "        cmake \\\n",
        "        ffmpeg \\\n",
        "        swig \\\n",
        "        libz-dev \\\n",
        "        unzip \\\n",
        "        zlib1g-dev \\\n",
        "        libglfw3 \\\n",
        "        libglfw3-dev \\\n",
        "        libxrandr2 \\\n",
        "        libxinerama-dev \\\n",
        "        libxi6 \\\n",
        "        libxcursor-dev \\\n",
        "        libgl1-mesa-dev \\\n",
        "        libgl1-mesa-glx \\\n",
        "        libglew-dev \\\n",
        "        libosmesa6-dev \\\n",
        "        lsb-release \\\n",
        "        ack-grep \\\n",
        "        patchelf \\\n",
        "        wget \\\n",
        "        xpra \\\n",
        "        xserver-xorg-dev \\\n",
        "        xvfb \\\n",
        "        python-opengl \\\n",
        "        ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNGuuABeg99q",
        "outputId": "9ed15c22-c227-4137-a2c5-ffff46b5a8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/cs285_f2021/mujoco\n"
          ]
        }
      ],
      "source": [
        "#@title download mujoco\n",
        "\n",
        "MJC_PATH = '{}/mujoco'.format(SYM_PATH)\n",
        "if not os.path.exists(MJC_PATH):\n",
        "  %mkdir $MJC_PATH\n",
        "%cd $MJC_PATH\n",
        "if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\n",
        "  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\n",
        "  !unzip -q mujoco200_linux.zip\n",
        "  %mv mujoco200_linux mujoco200\n",
        "  %rm mujoco200_linux.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y0MiuTJ4hT5z"
      },
      "outputs": [],
      "source": [
        "#@title update mujoco paths\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\n",
        "os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\n",
        "os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\n",
        "\n",
        "## installation on colab does not find *.so files\n",
        "## in LD_LIBRARY_PATH, copy over manually instead\n",
        "!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GQvbeuV1hi5I",
        "outputId": "bc5d956c-330e-4bb8-8e58-1b75d8cccef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/cs285_f2021\n",
            "Cloning into 'DRL_policy_grads'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 60 (delta 19), reused 47 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n",
            "/content/gdrive/My Drive/cs285_f2021/DRL_policy_grads\n",
            "Collecting gym==0.17.2\n",
            "  Downloading gym-0.17.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting tensorboard==2.3.0\n",
            "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 24.2 MB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.8\n",
            "  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting matplotlib==2.2.2\n",
            "  Downloading matplotlib-2.2.2-cp37-cp37m-manylinux1_x86_64.whl (12.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.6 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting ipython==6.4.0\n",
            "  Downloading ipython-6.4.0-py3-none-any.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting moviepy==1.0.0\n",
            "  Downloading moviepy-1.0.0.tar.gz (398 kB)\n",
            "\u001b[K     |████████████████████████████████| 398 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting pyvirtualdisplay==1.3.2\n",
            "  Downloading PyVirtualDisplay-1.3.2-py2.py3-none-any.whl (14 kB)\n",
            "Collecting torch==1.6.0\n",
            "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 20 kB/s \n",
            "\u001b[?25hCollecting opencv-python==4.4.0.42\n",
            "  Downloading opencv_python-4.4.0.42-cp37-cp37m-manylinux2014_x86_64.whl (49.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.4 MB 275 kB/s \n",
            "\u001b[?25hCollecting ipdb==0.13.3\n",
            "  Downloading ipdb-0.13.3.tar.gz (14 kB)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 20.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->-r requirements_colab.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->-r requirements_colab.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->-r requirements_colab.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2->-r requirements_colab.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.43.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements_colab.txt (line 4)) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements_colab.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements_colab.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements_colab.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements_colab.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (1.0.18)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements_colab.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.0->-r requirements_colab.txt (line 6)) (4.62.3)\n",
            "Collecting proglog<=1.0.0\n",
            "  Downloading proglog-0.1.9.tar.gz (10 kB)\n",
            "Collecting imageio<3.0,>=2.5\n",
            "  Downloading imageio-2.14.1-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 31.3 MB/s \n",
            "\u001b[?25hCollecting imageio_ffmpeg>=0.2.0\n",
            "  Downloading imageio_ffmpeg-0.4.5-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 190 kB/s \n",
            "\u001b[?25hCollecting EasyProcess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements_colab.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.3.0)\n",
            "Collecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython==6.4.0->-r requirements_colab.txt (line 5)) (0.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython==6.4.0->-r requirements_colab.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements_colab.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython==6.4.0->-r requirements_colab.txt (line 5)) (0.7.0)\n",
            "Building wheels for collected packages: gym, moviepy, ipdb, proglog\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.2-py3-none-any.whl size=1650890 sha256=f659b907fc156036bee198b21db363d46a9a37566950176aa92fc8fa52204dc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/e1/58/89a2aa24e6c2cc800204fc02010612afdf200926c4d6bfe315\n",
            "  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for moviepy: filename=moviepy-1.0.0-py3-none-any.whl size=131387 sha256=961c95e3768b038d0fe86608ad42285f9ff488401f8c535bc2a19795c92fbb96\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c1/e6/2ca4ff00d07d206bf2d5d19056fa530c4e54ecd1b2f6fcfcdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.3-py3-none-any.whl size=10875 sha256=8baadbb374529f22a081c72f4b26e088cf38cbfad58e419f5cbf8a4806dd57db\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/2b/e0/4932698c94c886d9d476e90916b43af63d5e708e146eb8b273\n",
            "  Building wheel for proglog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proglog: filename=proglog-0.1.9-py3-none-any.whl size=6157 sha256=e44efef5005c94bf16eca241dc5cab94e378f6b8a315dd757d139ee907de6cd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/36/1f/dc61e6ac10781d63cf6fa045eb09fa613a667384e12cb6e6e0\n",
            "Successfully built gym moviepy ipdb proglog\n",
            "Installing collected packages: pillow, proglog, ipython, imageio-ffmpeg, imageio, EasyProcess, torch, tensorboardX, tensorboard, pyvirtualdisplay, opencv-python, moviepy, matplotlib, ipdb, gym, box2d-py\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Attempting uninstall: moviepy\n",
            "    Found existing installation: moviepy 0.2.3.5\n",
            "    Uninstalling moviepy-0.2.3.5:\n",
            "      Successfully uninstalled moviepy-0.2.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.6.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 2.3.0 which is incompatible.\n",
            "plotnine 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.2.2 which is incompatible.\n",
            "mizani 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 6.4.0 which is incompatible.\n",
            "arviz 0.11.4 requires matplotlib>=3.0, but you have matplotlib 2.2.2 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed EasyProcess-1.1 box2d-py-2.3.8 gym-0.17.2 imageio-2.14.1 imageio-ffmpeg-0.4.5 ipdb-0.13.3 ipython-6.4.0 matplotlib-2.2.2 moviepy-1.0.0 opencv-python-4.4.0.42 pillow-9.0.1 proglog-0.1.9 pyvirtualdisplay-1.3.2 tensorboard-2.3.0 tensorboardX-1.8 torch-1.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/gdrive/My%20Drive/cs285_f2021/DRL_policy_grads\n",
            "Installing collected packages: cs285\n",
            "  Running setup.py develop for cs285\n",
            "Successfully installed cs285-0.1.0\n"
          ]
        }
      ],
      "source": [
        "#@title clone homework repo\n",
        "#@markdown Note that this is the same codebase from homework 1,\n",
        "#@markdown so you may need to move your old `homework_fall2021`\n",
        "#@markdown folder in order to clone the repo again.\n",
        "\n",
        "#@markdown **Don't delete your old work though!**\n",
        "#@markdown You will need it for this assignment.\n",
        "\n",
        "%cd $SYM_PATH\n",
        "!git clone https://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@github.com/ChihabEddine98/DRL_policy_grads.git\n",
        "%cd DRL_policy_grads\n",
        "%pip install -r requirements_colab.txt\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title clone and install mujoco-py\n",
        "\n",
        "!pip install mujoco-py==2.0.2.2\n",
        "\n",
        "## cythonize at the first import\n",
        "import mujoco_py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cHNC4hu4QDU",
        "outputId": "40f506e9-0c0f-44b0-d6fd-8d0412a11d28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mujoco-py==2.0.2.2\n",
            "  Downloading mujoco-py-2.0.2.2.tar.gz (771 kB)\n",
            "\u001b[K     |████████████████████████████████| 771 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting glfw>=1.4.0\n",
            "  Downloading glfw-2.5.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.2) (1.19.5)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.2) (0.29.26)\n",
            "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.2) (2.14.1)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.2) (1.15.0)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.2) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio>=2.1.2->mujoco-py==2.0.2.2) (9.0.1)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: lockfile, glfw, mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: mujoco-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "Successfully installed glfw-2.5.0 lockfile-0.12.2 mujoco-py-2.0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noinfUbHiHW2",
        "outputId": "27c86952-ca00-4dce-ca4e-222274bafa09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fa50f82f850>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#@title set up virtual display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "COqsZLeliU9Y",
        "outputId": "9c00bc3d-ebcc-4a7f-a731-cb3669adf5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading video...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title test virtual display\n",
        "\n",
        "#@markdown If you see a video of a four-legged ant fumbling about, setup is complete!\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from cs285.infrastructure.colab_utils import (\n",
        "    wrap_env,\n",
        "    show_video\n",
        ")\n",
        "\n",
        "env = wrap_env(gym.make(\"CartPole-v0\"))\n",
        "\n",
        "observation = env.reset()\n",
        "for i in range(100):\n",
        "    env.render(mode='rgb_array')\n",
        "    obs, rew, term, _ = env.step(env.action_space.sample() ) \n",
        "    if term:\n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "print('Loading video...')\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as request\n",
        "DATA_DIRECTORY = \"cs285/expert_data\"\n",
        "CARTPOLE_URL = \"https://www.dropbox.com/s/2tmo7ul00268l3e/cartpole.pkl?dl=1\"\n",
        "data_path = os.path.join(DATA_DIRECTORY, \"cartpole.pkl\")\n",
        "request.urlretrieve(CARTPOLE_URL, data_path)\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8SnbV5WJNNH",
        "outputId": "ea555daf-0cbb-4712-ac18-0ca867ad5c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('cs285/expert_data/cartpole.pkl', <http.client.HTTPMessage at 0x7f4900c7c050>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygs968BbiYHr"
      },
      "source": [
        "## Editing Code\n",
        "\n",
        "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2021/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qUmV93fif6S"
      },
      "source": [
        "## Run Policy Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lN-gZkqiijnR"
      },
      "outputs": [],
      "source": [
        "#@title imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
        "from cs285.agents.pg_agent import PGAgent\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q6NaOWhOinnU"
      },
      "outputs": [],
      "source": [
        "#@title runtime arguments\n",
        "\n",
        "class Args:\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    return getattr(self, key)\n",
        "\n",
        "  def __setitem__(self, key, val):\n",
        "    setattr(self, key, val)\n",
        "\n",
        "  def __contains__(self, key):\n",
        "    return hasattr(self, key)\n",
        "\n",
        "  env_name = 'CartPole-v0' #@param\n",
        "  exp_name = 'q1_sb_rtg_na' #@param\n",
        "  #@markdown main parameters of interest\n",
        "  n_iter = 100 #@param {type: \"integer\"}\n",
        "\n",
        "  ## PDF will tell you how to set ep_len\n",
        "  ## and discount for each environment\n",
        "  ep_len = 200 #@param {type: \"integer\"}\n",
        "  discount = 0.95 #@param {type: \"number\"}\n",
        "\n",
        "  reward_to_go = True #@param {type: \"boolean\"}\n",
        "  nn_baseline = False #@param {type: \"boolean\"}\n",
        "  gae_lambda = 0.01 #@param {type: \"number\"}\n",
        "  dont_standardize_advantages = False #@param {type: \"boolean\"}\n",
        "\n",
        "  #@markdown batches and steps\n",
        "  batch_size = 1000 #@param {type: \"integer\"}\n",
        "  eval_batch_size = 400 #@param {type: \"integer\"}\n",
        "\n",
        "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
        "  learning_rate =  5e-3 #@param {type: \"number\"}\n",
        "\n",
        "  #@markdown MLP parameters\n",
        "  n_layers = 2 #@param {type: \"integer\"}\n",
        "  size = 64 #@param {type: \"integer\"}\n",
        "\n",
        "  #@markdown system\n",
        "  save_params = False #@param {type: \"boolean\"}\n",
        "  no_gpu = False #@param {type: \"boolean\"}\n",
        "  which_gpu = 0 #@param {type: \"integer\"}\n",
        "  seed = 1 #@param {type: \"integer\"}\n",
        "    \n",
        "  action_noise_std = 0 #@param {type: \"number\"}\n",
        "\n",
        "  #@markdown logging\n",
        "  ## default is to not log video so\n",
        "  ## that logs are small enough to be\n",
        "  ## uploaded to gradscope\n",
        "  video_log_freq =  -1#@param {type: \"integer\"}\n",
        "  scalar_log_freq =  1#@param {type: \"integer\"}\n",
        "\n",
        "\n",
        "args = Args()\n",
        "\n",
        "## ensure compatibility with hw1 code\n",
        "args['train_batch_size'] = args['batch_size']\n",
        "\n",
        "if args['video_log_freq'] > 0:\n",
        "  import warnings\n",
        "  warnings.warn(\n",
        "      '''\\nLogging videos will make eventfiles too'''\n",
        "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
        "      '''\\nfor the runs you intend to submit.''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eScWwHhnsYkd"
      },
      "outputs": [],
      "source": [
        "#@title create directory for logging\n",
        "\n",
        "data_path = '''/content/cs285_f2021/''' \\\n",
        "            '''DRL_policy_grads/data'''\n",
        "\n",
        "if not (os.path.exists(data_path)):\n",
        "    os.makedirs(data_path)\n",
        "\n",
        "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join(data_path, logdir)\n",
        "args['logdir'] = logdir\n",
        "if not(os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aljzrLdAsvNu"
      },
      "outputs": [],
      "source": [
        "## define policy gradient trainer\n",
        "\n",
        "class PG_Trainer(object):\n",
        "\n",
        "    def __init__(self, params):\n",
        "\n",
        "        #####################\n",
        "        ## SET AGENT PARAMS\n",
        "        #####################\n",
        "\n",
        "        computation_graph_args = {\n",
        "            'n_layers': params['n_layers'],\n",
        "            'size': params['size'],\n",
        "            'learning_rate': params['learning_rate'],\n",
        "            }\n",
        "\n",
        "        estimate_advantage_args = {\n",
        "            'gamma': params['discount'],\n",
        "            'standardize_advantages': not(params['dont_standardize_advantages']),\n",
        "            'reward_to_go': params['reward_to_go'],\n",
        "            'nn_baseline': params['nn_baseline'],\n",
        "            'gae_lambda': params['gae_lambda'],\n",
        "        }\n",
        "\n",
        "        train_args = {\n",
        "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
        "        }\n",
        "\n",
        "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
        "\n",
        "        self.params = params\n",
        "        self.params['agent_class'] = PGAgent\n",
        "        self.params['agent_params'] = agent_params\n",
        "        self.params['batch_size_initial'] = self.params['batch_size']\n",
        "\n",
        "        ################\n",
        "        ## RL TRAINER\n",
        "        ################\n",
        "\n",
        "        self.rl_trainer = RL_Trainer(self.params)\n",
        "\n",
        "    def run_training_loop(self):\n",
        "\n",
        "        self.rl_trainer.run_training_loop(\n",
        "            self.params['n_iter'],\n",
        "            collect_policy = self.rl_trainer.agent.actor,\n",
        "            eval_policy = self.rl_trainer.agent.actor,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2rCuQsRsd3N"
      },
      "outputs": [],
      "source": [
        "## run training\n",
        "\n",
        "print(args.logdir)\n",
        "trainer = PG_Trainer(args)\n",
        "trainer.run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "NUlZXLTNB5Mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f9630a-413a-4b07-a33f-eec3787843ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cs285\t\tdata\t LICENSE    README.md\t\t    requirements.txt\n",
            "cs285.egg-info\thw2.pdf  mjkey.txt  requirements_colab.txt  setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 1"
      ],
      "metadata": {
        "id": "-KZaumnvm3Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 \\\n",
        "-dsa --exp_name q1_sb_no_rtg_dsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6TB423e-8-A",
        "outputId": "25a95928-a0e6-435e-a147-90d37ff9fc35"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q1_sb_no_rtg_dsa_CartPole-v0_04-02-2022_16-37-40\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 31.153846740722656\n",
            "Eval_StdReturn : 15.917315483093262\n",
            "Eval_MaxReturn : 77.0\n",
            "Eval_MinReturn : 12.0\n",
            "Eval_AverageEpLen : 31.153846153846153\n",
            "Train_AverageReturn : 27.648649215698242\n",
            "Train_StdReturn : 12.643160820007324\n",
            "Train_MaxReturn : 65.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 27.64864864864865\n",
            "Train_EnvstepsSoFar : 1023\n",
            "TimeSinceStart : 1.4175140857696533\n",
            "Training Loss : 23544.501953125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 37.45454406738281\n",
            "Eval_StdReturn : 17.773134231567383\n",
            "Eval_MaxReturn : 71.0\n",
            "Eval_MinReturn : 12.0\n",
            "Eval_AverageEpLen : 37.45454545454545\n",
            "Train_AverageReturn : 38.46154022216797\n",
            "Train_StdReturn : 25.42595672607422\n",
            "Train_MaxReturn : 98.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 38.46153846153846\n",
            "Train_EnvstepsSoFar : 2023\n",
            "TimeSinceStart : 2.7895359992980957\n",
            "Training Loss : 36905.1328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 70.16666412353516\n",
            "Eval_StdReturn : 29.29969596862793\n",
            "Eval_MaxReturn : 112.0\n",
            "Eval_MinReturn : 16.0\n",
            "Eval_AverageEpLen : 70.16666666666667\n",
            "Train_AverageReturn : 38.88888931274414\n",
            "Train_StdReturn : 26.084383010864258\n",
            "Train_MaxReturn : 108.0\n",
            "Train_MinReturn : 12.0\n",
            "Train_AverageEpLen : 38.888888888888886\n",
            "Train_EnvstepsSoFar : 3073\n",
            "TimeSinceStart : 4.297277927398682\n",
            "Training Loss : 37563.1015625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 61.85714340209961\n",
            "Eval_StdReturn : 27.513261795043945\n",
            "Eval_MaxReturn : 101.0\n",
            "Eval_MinReturn : 27.0\n",
            "Eval_AverageEpLen : 61.857142857142854\n",
            "Train_AverageReturn : 50.25\n",
            "Train_StdReturn : 23.134119033813477\n",
            "Train_MaxReturn : 109.0\n",
            "Train_MinReturn : 18.0\n",
            "Train_AverageEpLen : 50.25\n",
            "Train_EnvstepsSoFar : 4078\n",
            "TimeSinceStart : 5.753105640411377\n",
            "Training Loss : 38431.3828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 76.57142639160156\n",
            "Eval_StdReturn : 32.28824234008789\n",
            "Eval_MaxReturn : 150.0\n",
            "Eval_MinReturn : 39.0\n",
            "Eval_AverageEpLen : 76.57142857142857\n",
            "Train_AverageReturn : 48.904762268066406\n",
            "Train_StdReturn : 23.36644744873047\n",
            "Train_MaxReturn : 98.0\n",
            "Train_MinReturn : 18.0\n",
            "Train_AverageEpLen : 48.904761904761905\n",
            "Train_EnvstepsSoFar : 5105\n",
            "TimeSinceStart : 7.343592166900635\n",
            "Training Loss : 38461.76171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.16666412353516\n",
            "Eval_StdReturn : 14.814594268798828\n",
            "Eval_MaxReturn : 85.0\n",
            "Eval_MinReturn : 39.0\n",
            "Eval_AverageEpLen : 67.16666666666667\n",
            "Train_AverageReturn : 57.05555725097656\n",
            "Train_StdReturn : 15.970361709594727\n",
            "Train_MaxReturn : 88.0\n",
            "Train_MinReturn : 31.0\n",
            "Train_AverageEpLen : 57.05555555555556\n",
            "Train_EnvstepsSoFar : 6132\n",
            "TimeSinceStart : 8.754396915435791\n",
            "Training Loss : 38407.1484375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 63.71428680419922\n",
            "Eval_StdReturn : 16.619388580322266\n",
            "Eval_MaxReturn : 92.0\n",
            "Eval_MinReturn : 43.0\n",
            "Eval_AverageEpLen : 63.714285714285715\n",
            "Train_AverageReturn : 56.38888931274414\n",
            "Train_StdReturn : 24.774850845336914\n",
            "Train_MaxReturn : 104.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 56.388888888888886\n",
            "Train_EnvstepsSoFar : 7147\n",
            "TimeSinceStart : 10.428910255432129\n",
            "Training Loss : 40664.421875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 52.33333206176758\n",
            "Eval_StdReturn : 19.675140380859375\n",
            "Eval_MaxReturn : 88.0\n",
            "Eval_MinReturn : 26.0\n",
            "Eval_AverageEpLen : 52.333333333333336\n",
            "Train_AverageReturn : 58.82352828979492\n",
            "Train_StdReturn : 26.535736083984375\n",
            "Train_MaxReturn : 142.0\n",
            "Train_MinReturn : 24.0\n",
            "Train_AverageEpLen : 58.8235294117647\n",
            "Train_EnvstepsSoFar : 8147\n",
            "TimeSinceStart : 11.96250867843628\n",
            "Training Loss : 40998.6875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 38.6363639831543\n",
            "Eval_StdReturn : 11.039373397827148\n",
            "Eval_MaxReturn : 61.0\n",
            "Eval_MinReturn : 26.0\n",
            "Eval_AverageEpLen : 38.63636363636363\n",
            "Train_AverageReturn : 55.05263137817383\n",
            "Train_StdReturn : 19.176393508911133\n",
            "Train_MaxReturn : 104.0\n",
            "Train_MinReturn : 33.0\n",
            "Train_AverageEpLen : 55.05263157894737\n",
            "Train_EnvstepsSoFar : 9193\n",
            "TimeSinceStart : 13.465378761291504\n",
            "Training Loss : 36320.90234375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 46.66666793823242\n",
            "Eval_StdReturn : 18.11077117919922\n",
            "Eval_MaxReturn : 92.0\n",
            "Eval_MinReturn : 31.0\n",
            "Eval_AverageEpLen : 46.666666666666664\n",
            "Train_AverageReturn : 48.14285659790039\n",
            "Train_StdReturn : 19.292943954467773\n",
            "Train_MaxReturn : 110.0\n",
            "Train_MinReturn : 22.0\n",
            "Train_AverageEpLen : 48.142857142857146\n",
            "Train_EnvstepsSoFar : 10204\n",
            "TimeSinceStart : 14.914441347122192\n",
            "Training Loss : 32450.06640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 78.33333587646484\n",
            "Eval_StdReturn : 48.77043914794922\n",
            "Eval_MaxReturn : 183.0\n",
            "Eval_MinReturn : 47.0\n",
            "Eval_AverageEpLen : 78.33333333333333\n",
            "Train_AverageReturn : 52.099998474121094\n",
            "Train_StdReturn : 23.379262924194336\n",
            "Train_MaxReturn : 130.0\n",
            "Train_MinReturn : 22.0\n",
            "Train_AverageEpLen : 52.1\n",
            "Train_EnvstepsSoFar : 11246\n",
            "TimeSinceStart : 16.430760145187378\n",
            "Training Loss : 37427.5859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 53.375\n",
            "Eval_StdReturn : 13.591702461242676\n",
            "Eval_MaxReturn : 79.0\n",
            "Eval_MinReturn : 32.0\n",
            "Eval_AverageEpLen : 53.375\n",
            "Train_AverageReturn : 55.77777862548828\n",
            "Train_StdReturn : 19.65410804748535\n",
            "Train_MaxReturn : 106.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 55.77777777777778\n",
            "Train_EnvstepsSoFar : 12250\n",
            "TimeSinceStart : 18.201220512390137\n",
            "Training Loss : 34437.87890625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 59.28571319580078\n",
            "Eval_StdReturn : 25.098175048828125\n",
            "Eval_MaxReturn : 93.0\n",
            "Eval_MinReturn : 19.0\n",
            "Eval_AverageEpLen : 59.285714285714285\n",
            "Train_AverageReturn : 61.29411697387695\n",
            "Train_StdReturn : 30.764312744140625\n",
            "Train_MaxReturn : 139.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 61.294117647058826\n",
            "Train_EnvstepsSoFar : 13292\n",
            "TimeSinceStart : 19.670225620269775\n",
            "Training Loss : 43861.05859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 85.5999984741211\n",
            "Eval_StdReturn : 58.32186508178711\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 39.0\n",
            "Eval_AverageEpLen : 85.6\n",
            "Train_AverageReturn : 68.4000015258789\n",
            "Train_StdReturn : 36.60018539428711\n",
            "Train_MaxReturn : 171.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 68.4\n",
            "Train_EnvstepsSoFar : 14318\n",
            "TimeSinceStart : 21.53205704689026\n",
            "Training Loss : 49319.6484375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 78.5\n",
            "Eval_StdReturn : 23.048860549926758\n",
            "Eval_MaxReturn : 118.0\n",
            "Eval_MinReturn : 44.0\n",
            "Eval_AverageEpLen : 78.5\n",
            "Train_AverageReturn : 62.82352828979492\n",
            "Train_StdReturn : 20.697357177734375\n",
            "Train_MaxReturn : 104.0\n",
            "Train_MinReturn : 31.0\n",
            "Train_AverageEpLen : 62.8235294117647\n",
            "Train_EnvstepsSoFar : 15386\n",
            "TimeSinceStart : 23.041168451309204\n",
            "Training Loss : 37801.0078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 60.71428680419922\n",
            "Eval_StdReturn : 10.779799461364746\n",
            "Eval_MaxReturn : 75.0\n",
            "Eval_MinReturn : 45.0\n",
            "Eval_AverageEpLen : 60.714285714285715\n",
            "Train_AverageReturn : 72.35713958740234\n",
            "Train_StdReturn : 32.12610626220703\n",
            "Train_MaxReturn : 153.0\n",
            "Train_MinReturn : 36.0\n",
            "Train_AverageEpLen : 72.35714285714286\n",
            "Train_EnvstepsSoFar : 16399\n",
            "TimeSinceStart : 24.477938175201416\n",
            "Training Loss : 46159.68359375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 70.57142639160156\n",
            "Eval_StdReturn : 29.7314510345459\n",
            "Eval_MaxReturn : 127.0\n",
            "Eval_MinReturn : 30.0\n",
            "Eval_AverageEpLen : 70.57142857142857\n",
            "Train_AverageReturn : 71.66666412353516\n",
            "Train_StdReturn : 25.880924224853516\n",
            "Train_MaxReturn : 132.0\n",
            "Train_MinReturn : 38.0\n",
            "Train_AverageEpLen : 71.66666666666667\n",
            "Train_EnvstepsSoFar : 17474\n",
            "TimeSinceStart : 26.046114683151245\n",
            "Training Loss : 45836.3671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 70.83333587646484\n",
            "Eval_StdReturn : 27.78738784790039\n",
            "Eval_MaxReturn : 122.0\n",
            "Eval_MinReturn : 48.0\n",
            "Eval_AverageEpLen : 70.83333333333333\n",
            "Train_AverageReturn : 73.71428680419922\n",
            "Train_StdReturn : 20.67237091064453\n",
            "Train_MaxReturn : 99.0\n",
            "Train_MinReturn : 34.0\n",
            "Train_AverageEpLen : 73.71428571428571\n",
            "Train_EnvstepsSoFar : 18506\n",
            "TimeSinceStart : 27.775652170181274\n",
            "Training Loss : 42211.33203125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 83.5999984741211\n",
            "Eval_StdReturn : 34.6963996887207\n",
            "Eval_MaxReturn : 129.0\n",
            "Eval_MinReturn : 35.0\n",
            "Eval_AverageEpLen : 83.6\n",
            "Train_AverageReturn : 74.78571319580078\n",
            "Train_StdReturn : 33.89727783203125\n",
            "Train_MaxReturn : 145.0\n",
            "Train_MinReturn : 44.0\n",
            "Train_AverageEpLen : 74.78571428571429\n",
            "Train_EnvstepsSoFar : 19553\n",
            "TimeSinceStart : 29.37753653526306\n",
            "Training Loss : 47848.28515625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.66666412353516\n",
            "Eval_StdReturn : 23.633779525756836\n",
            "Eval_MaxReturn : 105.0\n",
            "Eval_MinReturn : 28.0\n",
            "Eval_AverageEpLen : 67.66666666666667\n",
            "Train_AverageReturn : 94.09091186523438\n",
            "Train_StdReturn : 31.534841537475586\n",
            "Train_MaxReturn : 154.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 94.0909090909091\n",
            "Train_EnvstepsSoFar : 20588\n",
            "TimeSinceStart : 31.035971879959106\n",
            "Training Loss : 54826.46875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 62.14285659790039\n",
            "Eval_StdReturn : 19.006980895996094\n",
            "Eval_MaxReturn : 83.0\n",
            "Eval_MinReturn : 39.0\n",
            "Eval_AverageEpLen : 62.142857142857146\n",
            "Train_AverageReturn : 71.33333587646484\n",
            "Train_StdReturn : 24.743125915527344\n",
            "Train_MaxReturn : 132.0\n",
            "Train_MinReturn : 32.0\n",
            "Train_AverageEpLen : 71.33333333333333\n",
            "Train_EnvstepsSoFar : 21658\n",
            "TimeSinceStart : 32.55814051628113\n",
            "Training Loss : 39803.07421875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.5\n",
            "Eval_StdReturn : 25.630386352539062\n",
            "Eval_MaxReturn : 107.0\n",
            "Eval_MinReturn : 30.0\n",
            "Eval_AverageEpLen : 67.5\n",
            "Train_AverageReturn : 67.19999694824219\n",
            "Train_StdReturn : 26.523448944091797\n",
            "Train_MaxReturn : 131.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 67.2\n",
            "Train_EnvstepsSoFar : 22666\n",
            "TimeSinceStart : 34.12749648094177\n",
            "Training Loss : 37556.5625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.66666412353516\n",
            "Eval_StdReturn : 13.695091247558594\n",
            "Eval_MaxReturn : 96.0\n",
            "Eval_MinReturn : 51.0\n",
            "Eval_AverageEpLen : 67.66666666666667\n",
            "Train_AverageReturn : 68.86666870117188\n",
            "Train_StdReturn : 28.892597198486328\n",
            "Train_MaxReturn : 130.0\n",
            "Train_MinReturn : 32.0\n",
            "Train_AverageEpLen : 68.86666666666666\n",
            "Train_EnvstepsSoFar : 23699\n",
            "TimeSinceStart : 35.72400212287903\n",
            "Training Loss : 38747.1484375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.0\n",
            "Eval_StdReturn : 27.766887664794922\n",
            "Eval_MaxReturn : 107.0\n",
            "Eval_MinReturn : 32.0\n",
            "Eval_AverageEpLen : 67.0\n",
            "Train_AverageReturn : 100.5999984741211\n",
            "Train_StdReturn : 37.427799224853516\n",
            "Train_MaxReturn : 167.0\n",
            "Train_MinReturn : 47.0\n",
            "Train_AverageEpLen : 100.6\n",
            "Train_EnvstepsSoFar : 24705\n",
            "TimeSinceStart : 37.19745445251465\n",
            "Training Loss : 55167.54296875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 73.83333587646484\n",
            "Eval_StdReturn : 17.78029441833496\n",
            "Eval_MaxReturn : 99.0\n",
            "Eval_MinReturn : 48.0\n",
            "Eval_AverageEpLen : 73.83333333333333\n",
            "Train_AverageReturn : 86.15384674072266\n",
            "Train_StdReturn : 31.01822280883789\n",
            "Train_MaxReturn : 156.0\n",
            "Train_MinReturn : 48.0\n",
            "Train_AverageEpLen : 86.15384615384616\n",
            "Train_EnvstepsSoFar : 25825\n",
            "TimeSinceStart : 38.75237560272217\n",
            "Training Loss : 46895.125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 73.5\n",
            "Eval_StdReturn : 23.73288917541504\n",
            "Eval_MaxReturn : 121.0\n",
            "Eval_MinReturn : 46.0\n",
            "Eval_AverageEpLen : 73.5\n",
            "Train_AverageReturn : 101.80000305175781\n",
            "Train_StdReturn : 38.52998733520508\n",
            "Train_MaxReturn : 191.0\n",
            "Train_MinReturn : 49.0\n",
            "Train_AverageEpLen : 101.8\n",
            "Train_EnvstepsSoFar : 26843\n",
            "TimeSinceStart : 40.22607111930847\n",
            "Training Loss : 53013.3671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 77.16666412353516\n",
            "Eval_StdReturn : 18.040849685668945\n",
            "Eval_MaxReturn : 115.0\n",
            "Eval_MinReturn : 65.0\n",
            "Eval_AverageEpLen : 77.16666666666667\n",
            "Train_AverageReturn : 82.69230651855469\n",
            "Train_StdReturn : 20.25371742248535\n",
            "Train_MaxReturn : 119.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 82.6923076923077\n",
            "Train_EnvstepsSoFar : 27918\n",
            "TimeSinceStart : 41.753974199295044\n",
            "Training Loss : 40478.1484375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 89.0\n",
            "Eval_StdReturn : 23.93324089050293\n",
            "Eval_MaxReturn : 128.0\n",
            "Eval_MinReturn : 58.0\n",
            "Eval_AverageEpLen : 89.0\n",
            "Train_AverageReturn : 79.21428680419922\n",
            "Train_StdReturn : 15.88880443572998\n",
            "Train_MaxReturn : 117.0\n",
            "Train_MinReturn : 54.0\n",
            "Train_AverageEpLen : 79.21428571428571\n",
            "Train_EnvstepsSoFar : 29027\n",
            "TimeSinceStart : 43.274781227111816\n",
            "Training Loss : 36380.39453125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 72.16666412353516\n",
            "Eval_StdReturn : 24.08607292175293\n",
            "Eval_MaxReturn : 122.0\n",
            "Eval_MinReturn : 51.0\n",
            "Eval_AverageEpLen : 72.16666666666667\n",
            "Train_AverageReturn : 101.80000305175781\n",
            "Train_StdReturn : 36.646419525146484\n",
            "Train_MaxReturn : 167.0\n",
            "Train_MinReturn : 68.0\n",
            "Train_AverageEpLen : 101.8\n",
            "Train_EnvstepsSoFar : 30045\n",
            "TimeSinceStart : 44.88069987297058\n",
            "Training Loss : 49382.296875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 71.66666412353516\n",
            "Eval_StdReturn : 21.692293167114258\n",
            "Eval_MaxReturn : 118.0\n",
            "Eval_MinReturn : 49.0\n",
            "Eval_AverageEpLen : 71.66666666666667\n",
            "Train_AverageReturn : 100.9000015258789\n",
            "Train_StdReturn : 40.29007339477539\n",
            "Train_MaxReturn : 175.0\n",
            "Train_MinReturn : 56.0\n",
            "Train_AverageEpLen : 100.9\n",
            "Train_EnvstepsSoFar : 31054\n",
            "TimeSinceStart : 46.57045269012451\n",
            "Training Loss : 46538.109375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 56.375\n",
            "Eval_StdReturn : 10.17272663116455\n",
            "Eval_MaxReturn : 76.0\n",
            "Eval_MinReturn : 42.0\n",
            "Eval_AverageEpLen : 56.375\n",
            "Train_AverageReturn : 73.92857360839844\n",
            "Train_StdReturn : 21.359054565429688\n",
            "Train_MaxReturn : 125.0\n",
            "Train_MinReturn : 46.0\n",
            "Train_AverageEpLen : 73.92857142857143\n",
            "Train_EnvstepsSoFar : 32089\n",
            "TimeSinceStart : 48.07903575897217\n",
            "Training Loss : 30514.20703125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 64.85713958740234\n",
            "Eval_StdReturn : 15.596964836120605\n",
            "Eval_MaxReturn : 95.0\n",
            "Eval_MinReturn : 43.0\n",
            "Eval_AverageEpLen : 64.85714285714286\n",
            "Train_AverageReturn : 68.26667022705078\n",
            "Train_StdReturn : 12.850248336791992\n",
            "Train_MaxReturn : 98.0\n",
            "Train_MinReturn : 54.0\n",
            "Train_AverageEpLen : 68.26666666666667\n",
            "Train_EnvstepsSoFar : 33113\n",
            "TimeSinceStart : 49.56735968589783\n",
            "Training Loss : 27221.8984375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.33333587646484\n",
            "Eval_StdReturn : 10.40299129486084\n",
            "Eval_MaxReturn : 82.0\n",
            "Eval_MinReturn : 53.0\n",
            "Eval_AverageEpLen : 67.33333333333333\n",
            "Train_AverageReturn : 64.75\n",
            "Train_StdReturn : 20.59581184387207\n",
            "Train_MaxReturn : 122.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 64.75\n",
            "Train_EnvstepsSoFar : 34149\n",
            "TimeSinceStart : 51.02840280532837\n",
            "Training Loss : 27548.029296875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 78.66666412353516\n",
            "Eval_StdReturn : 13.936363220214844\n",
            "Eval_MaxReturn : 102.0\n",
            "Eval_MinReturn : 56.0\n",
            "Eval_AverageEpLen : 78.66666666666667\n",
            "Train_AverageReturn : 84.38461303710938\n",
            "Train_StdReturn : 35.855560302734375\n",
            "Train_MaxReturn : 177.0\n",
            "Train_MinReturn : 51.0\n",
            "Train_AverageEpLen : 84.38461538461539\n",
            "Train_EnvstepsSoFar : 35246\n",
            "TimeSinceStart : 52.60971760749817\n",
            "Training Loss : 36655.1328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 88.80000305175781\n",
            "Eval_StdReturn : 38.3009147644043\n",
            "Eval_MaxReturn : 163.0\n",
            "Eval_MinReturn : 56.0\n",
            "Eval_AverageEpLen : 88.8\n",
            "Train_AverageReturn : 83.58333587646484\n",
            "Train_StdReturn : 38.45226287841797\n",
            "Train_MaxReturn : 199.0\n",
            "Train_MinReturn : 50.0\n",
            "Train_AverageEpLen : 83.58333333333333\n",
            "Train_EnvstepsSoFar : 36249\n",
            "TimeSinceStart : 54.0604465007782\n",
            "Training Loss : 34689.671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 92.0\n",
            "Eval_StdReturn : 20.57182502746582\n",
            "Eval_MaxReturn : 124.0\n",
            "Eval_MinReturn : 67.0\n",
            "Eval_AverageEpLen : 92.0\n",
            "Train_AverageReturn : 104.5999984741211\n",
            "Train_StdReturn : 42.46221923828125\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 50.0\n",
            "Train_AverageEpLen : 104.6\n",
            "Train_EnvstepsSoFar : 37295\n",
            "TimeSinceStart : 55.59060025215149\n",
            "Training Loss : 44356.3046875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 100.0\n",
            "Eval_StdReturn : 25.01999282836914\n",
            "Eval_MaxReturn : 134.0\n",
            "Eval_MinReturn : 72.0\n",
            "Eval_AverageEpLen : 100.0\n",
            "Train_AverageReturn : 93.58333587646484\n",
            "Train_StdReturn : 26.750259399414062\n",
            "Train_MaxReturn : 152.0\n",
            "Train_MinReturn : 60.0\n",
            "Train_AverageEpLen : 93.58333333333333\n",
            "Train_EnvstepsSoFar : 38418\n",
            "TimeSinceStart : 57.20842623710632\n",
            "Training Loss : 35860.89453125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 137.6666717529297\n",
            "Eval_StdReturn : 29.46561050415039\n",
            "Eval_MaxReturn : 159.0\n",
            "Eval_MinReturn : 96.0\n",
            "Eval_AverageEpLen : 137.66666666666666\n",
            "Train_AverageReturn : 111.11111450195312\n",
            "Train_StdReturn : 41.90583419799805\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 60.0\n",
            "Train_AverageEpLen : 111.11111111111111\n",
            "Train_EnvstepsSoFar : 39418\n",
            "TimeSinceStart : 58.63809156417847\n",
            "Training Loss : 43313.19140625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 114.80000305175781\n",
            "Eval_StdReturn : 43.90626525878906\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 75.0\n",
            "Eval_AverageEpLen : 114.8\n",
            "Train_AverageReturn : 116.55555725097656\n",
            "Train_StdReturn : 40.0890998840332\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 116.55555555555556\n",
            "Train_EnvstepsSoFar : 40467\n",
            "TimeSinceStart : 60.263720750808716\n",
            "Training Loss : 44359.29296875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 113.5\n",
            "Eval_StdReturn : 15.913830757141113\n",
            "Eval_MaxReturn : 136.0\n",
            "Eval_MinReturn : 93.0\n",
            "Eval_AverageEpLen : 113.5\n",
            "Train_AverageReturn : 101.5999984741211\n",
            "Train_StdReturn : 20.943735122680664\n",
            "Train_MaxReturn : 138.0\n",
            "Train_MinReturn : 67.0\n",
            "Train_AverageEpLen : 101.6\n",
            "Train_EnvstepsSoFar : 41483\n",
            "TimeSinceStart : 61.71479845046997\n",
            "Training Loss : 34236.078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 111.5\n",
            "Eval_StdReturn : 7.697402000427246\n",
            "Eval_MaxReturn : 120.0\n",
            "Eval_MinReturn : 99.0\n",
            "Eval_AverageEpLen : 111.5\n",
            "Train_AverageReturn : 118.77777862548828\n",
            "Train_StdReturn : 27.45006561279297\n",
            "Train_MaxReturn : 173.0\n",
            "Train_MinReturn : 80.0\n",
            "Train_AverageEpLen : 118.77777777777777\n",
            "Train_EnvstepsSoFar : 42552\n",
            "TimeSinceStart : 63.221877336502075\n",
            "Training Loss : 40729.2890625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 83.5999984741211\n",
            "Eval_StdReturn : 15.094369888305664\n",
            "Eval_MaxReturn : 109.0\n",
            "Eval_MinReturn : 63.0\n",
            "Eval_AverageEpLen : 83.6\n",
            "Train_AverageReturn : 113.11111450195312\n",
            "Train_StdReturn : 24.73613929748535\n",
            "Train_MaxReturn : 160.0\n",
            "Train_MinReturn : 70.0\n",
            "Train_AverageEpLen : 113.11111111111111\n",
            "Train_EnvstepsSoFar : 43570\n",
            "TimeSinceStart : 64.67710185050964\n",
            "Training Loss : 38506.0859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 123.75\n",
            "Eval_StdReturn : 37.432437896728516\n",
            "Eval_MaxReturn : 184.0\n",
            "Eval_MinReturn : 85.0\n",
            "Eval_AverageEpLen : 123.75\n",
            "Train_AverageReturn : 118.55555725097656\n",
            "Train_StdReturn : 45.57804489135742\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 76.0\n",
            "Train_AverageEpLen : 118.55555555555556\n",
            "Train_EnvstepsSoFar : 44637\n",
            "TimeSinceStart : 66.26321625709534\n",
            "Training Loss : 44039.6171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 97.80000305175781\n",
            "Eval_StdReturn : 18.861600875854492\n",
            "Eval_MaxReturn : 115.0\n",
            "Eval_MinReturn : 68.0\n",
            "Eval_AverageEpLen : 97.8\n",
            "Train_AverageReturn : 108.45454406738281\n",
            "Train_StdReturn : 32.02942657470703\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 79.0\n",
            "Train_AverageEpLen : 108.45454545454545\n",
            "Train_EnvstepsSoFar : 45830\n",
            "TimeSinceStart : 67.93895435333252\n",
            "Training Loss : 42177.4453125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 133.6666717529297\n",
            "Eval_StdReturn : 48.21018981933594\n",
            "Eval_MaxReturn : 194.0\n",
            "Eval_MinReturn : 76.0\n",
            "Eval_AverageEpLen : 133.66666666666666\n",
            "Train_AverageReturn : 101.69999694824219\n",
            "Train_StdReturn : 26.05014419555664\n",
            "Train_MaxReturn : 148.0\n",
            "Train_MinReturn : 68.0\n",
            "Train_AverageEpLen : 101.7\n",
            "Train_EnvstepsSoFar : 46847\n",
            "TimeSinceStart : 69.3866810798645\n",
            "Training Loss : 31440.57421875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 138.6666717529297\n",
            "Eval_StdReturn : 39.36439514160156\n",
            "Eval_MaxReturn : 193.0\n",
            "Eval_MinReturn : 101.0\n",
            "Eval_AverageEpLen : 138.66666666666666\n",
            "Train_AverageReturn : 116.69999694824219\n",
            "Train_StdReturn : 29.80956268310547\n",
            "Train_MaxReturn : 171.0\n",
            "Train_MinReturn : 63.0\n",
            "Train_AverageEpLen : 116.7\n",
            "Train_EnvstepsSoFar : 48014\n",
            "TimeSinceStart : 71.06908535957336\n",
            "Training Loss : 38445.9375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 183.3333282470703\n",
            "Eval_StdReturn : 12.036980628967285\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 172.0\n",
            "Eval_AverageEpLen : 183.33333333333334\n",
            "Train_AverageReturn : 143.14285278320312\n",
            "Train_StdReturn : 30.130329132080078\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 90.0\n",
            "Train_AverageEpLen : 143.14285714285714\n",
            "Train_EnvstepsSoFar : 49016\n",
            "TimeSinceStart : 72.61893677711487\n",
            "Training Loss : 42117.9765625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 197.6666717529297\n",
            "Eval_StdReturn : 3.2998316287994385\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 193.0\n",
            "Eval_AverageEpLen : 197.66666666666666\n",
            "Train_AverageReturn : 169.5\n",
            "Train_StdReturn : 15.924300193786621\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 149.0\n",
            "Train_AverageEpLen : 169.5\n",
            "Train_EnvstepsSoFar : 50033\n",
            "TimeSinceStart : 74.25000953674316\n",
            "Training Loss : 53257.625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 140.6666717529297\n",
            "Eval_StdReturn : 10.624918937683105\n",
            "Eval_MaxReturn : 154.0\n",
            "Eval_MinReturn : 128.0\n",
            "Eval_AverageEpLen : 140.66666666666666\n",
            "Train_AverageReturn : 167.3333282470703\n",
            "Train_StdReturn : 23.371397018432617\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 140.0\n",
            "Train_AverageEpLen : 167.33333333333334\n",
            "Train_EnvstepsSoFar : 51037\n",
            "TimeSinceStart : 75.7053234577179\n",
            "Training Loss : 52565.45703125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 168.0\n",
            "Train_StdReturn : 22.24110221862793\n",
            "Train_MaxReturn : 195.0\n",
            "Train_MinReturn : 125.0\n",
            "Train_AverageEpLen : 168.0\n",
            "Train_EnvstepsSoFar : 52045\n",
            "TimeSinceStart : 77.13764023780823\n",
            "Training Loss : 50781.578125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 166.6666717529297\n",
            "Eval_StdReturn : 20.677417755126953\n",
            "Eval_MaxReturn : 186.0\n",
            "Eval_MinReturn : 138.0\n",
            "Eval_AverageEpLen : 166.66666666666666\n",
            "Train_AverageReturn : 159.85714721679688\n",
            "Train_StdReturn : 28.772153854370117\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 121.0\n",
            "Train_AverageEpLen : 159.85714285714286\n",
            "Train_EnvstepsSoFar : 53164\n",
            "TimeSinceStart : 78.74933648109436\n",
            "Training Loss : 49121.06640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 172.3333282470703\n",
            "Eval_StdReturn : 32.96799850463867\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 126.0\n",
            "Eval_AverageEpLen : 172.33333333333334\n",
            "Train_AverageReturn : 147.75\n",
            "Train_StdReturn : 29.350255966186523\n",
            "Train_MaxReturn : 189.0\n",
            "Train_MinReturn : 83.0\n",
            "Train_AverageEpLen : 147.75\n",
            "Train_EnvstepsSoFar : 54346\n",
            "TimeSinceStart : 80.63382387161255\n",
            "Training Loss : 43719.0234375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 176.6666717529297\n",
            "Train_StdReturn : 21.00528907775879\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 136.0\n",
            "Train_AverageEpLen : 176.66666666666666\n",
            "Train_EnvstepsSoFar : 55406\n",
            "TimeSinceStart : 82.11880683898926\n",
            "Training Loss : 49435.5859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 151.3333282470703\n",
            "Eval_StdReturn : 34.499595642089844\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 124.0\n",
            "Eval_AverageEpLen : 151.33333333333334\n",
            "Train_AverageReturn : 139.0\n",
            "Train_StdReturn : 32.855743408203125\n",
            "Train_MaxReturn : 182.0\n",
            "Train_MinReturn : 92.0\n",
            "Train_AverageEpLen : 139.0\n",
            "Train_EnvstepsSoFar : 56518\n",
            "TimeSinceStart : 83.70603156089783\n",
            "Training Loss : 38642.8125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 160.6666717529297\n",
            "Eval_StdReturn : 37.606143951416016\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 110.0\n",
            "Eval_AverageEpLen : 160.66666666666666\n",
            "Train_AverageReturn : 133.125\n",
            "Train_StdReturn : 37.29087448120117\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 89.0\n",
            "Train_AverageEpLen : 133.125\n",
            "Train_EnvstepsSoFar : 57583\n",
            "TimeSinceStart : 85.33667492866516\n",
            "Training Loss : 38107.09765625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 134.3333282470703\n",
            "Eval_StdReturn : 24.904260635375977\n",
            "Eval_MaxReturn : 165.0\n",
            "Eval_MinReturn : 104.0\n",
            "Eval_AverageEpLen : 134.33333333333334\n",
            "Train_AverageReturn : 168.0\n",
            "Train_StdReturn : 33.63034439086914\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 116.0\n",
            "Train_AverageEpLen : 168.0\n",
            "Train_EnvstepsSoFar : 58591\n",
            "TimeSinceStart : 86.77319622039795\n",
            "Training Loss : 39980.37890625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 143.3333282470703\n",
            "Eval_StdReturn : 41.249916076660156\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 103.0\n",
            "Eval_AverageEpLen : 143.33333333333334\n",
            "Train_AverageReturn : 129.625\n",
            "Train_StdReturn : 48.197349548339844\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 78.0\n",
            "Train_AverageEpLen : 129.625\n",
            "Train_EnvstepsSoFar : 59628\n",
            "TimeSinceStart : 88.26236701011658\n",
            "Training Loss : 36071.6328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 172.0\n",
            "Eval_StdReturn : 19.815818786621094\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 157.0\n",
            "Eval_AverageEpLen : 172.0\n",
            "Train_AverageReturn : 138.5\n",
            "Train_StdReturn : 30.385028839111328\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 104.0\n",
            "Train_AverageEpLen : 138.5\n",
            "Train_EnvstepsSoFar : 60736\n",
            "TimeSinceStart : 89.95921540260315\n",
            "Training Loss : 42584.19921875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 151.6666717529297\n",
            "Eval_StdReturn : 4.189935207366943\n",
            "Eval_MaxReturn : 156.0\n",
            "Eval_MinReturn : 146.0\n",
            "Eval_AverageEpLen : 151.66666666666666\n",
            "Train_AverageReturn : 164.2857208251953\n",
            "Train_StdReturn : 22.249971389770508\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 132.0\n",
            "Train_AverageEpLen : 164.28571428571428\n",
            "Train_EnvstepsSoFar : 61886\n",
            "TimeSinceStart : 91.93483352661133\n",
            "Training Loss : 42280.13671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 118.5\n",
            "Eval_StdReturn : 26.253570556640625\n",
            "Eval_MaxReturn : 152.0\n",
            "Eval_MinReturn : 81.0\n",
            "Eval_AverageEpLen : 118.5\n",
            "Train_AverageReturn : 148.85714721679688\n",
            "Train_StdReturn : 35.65080261230469\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 100.0\n",
            "Train_AverageEpLen : 148.85714285714286\n",
            "Train_EnvstepsSoFar : 62928\n",
            "TimeSinceStart : 93.45658755302429\n",
            "Training Loss : 34493.97265625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 124.0\n",
            "Eval_StdReturn : 30.02499008178711\n",
            "Eval_MaxReturn : 169.0\n",
            "Eval_MinReturn : 86.0\n",
            "Eval_AverageEpLen : 124.0\n",
            "Train_AverageReturn : 119.66666412353516\n",
            "Train_StdReturn : 31.67192840576172\n",
            "Train_MaxReturn : 178.0\n",
            "Train_MinReturn : 72.0\n",
            "Train_AverageEpLen : 119.66666666666667\n",
            "Train_EnvstepsSoFar : 64005\n",
            "TimeSinceStart : 95.2488842010498\n",
            "Training Loss : 34092.26171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 97.80000305175781\n",
            "Eval_StdReturn : 11.68588924407959\n",
            "Eval_MaxReturn : 109.0\n",
            "Eval_MinReturn : 76.0\n",
            "Eval_AverageEpLen : 97.8\n",
            "Train_AverageReturn : 107.80000305175781\n",
            "Train_StdReturn : 33.2950439453125\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 75.0\n",
            "Train_AverageEpLen : 107.8\n",
            "Train_EnvstepsSoFar : 65083\n",
            "TimeSinceStart : 96.87178182601929\n",
            "Training Loss : 26004.28125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 104.75\n",
            "Eval_StdReturn : 33.063385009765625\n",
            "Eval_MaxReturn : 145.0\n",
            "Eval_MinReturn : 66.0\n",
            "Eval_AverageEpLen : 104.75\n",
            "Train_AverageReturn : 87.5\n",
            "Train_StdReturn : 11.842719078063965\n",
            "Train_MaxReturn : 111.0\n",
            "Train_MinReturn : 71.0\n",
            "Train_AverageEpLen : 87.5\n",
            "Train_EnvstepsSoFar : 66133\n",
            "TimeSinceStart : 98.35782313346863\n",
            "Training Loss : 20421.919921875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 87.19999694824219\n",
            "Eval_StdReturn : 16.1666316986084\n",
            "Eval_MaxReturn : 117.0\n",
            "Eval_MinReturn : 72.0\n",
            "Eval_AverageEpLen : 87.2\n",
            "Train_AverageReturn : 114.88888549804688\n",
            "Train_StdReturn : 33.03346633911133\n",
            "Train_MaxReturn : 181.0\n",
            "Train_MinReturn : 66.0\n",
            "Train_AverageEpLen : 114.88888888888889\n",
            "Train_EnvstepsSoFar : 67167\n",
            "TimeSinceStart : 99.84456014633179\n",
            "Training Loss : 28963.173828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 72.33333587646484\n",
            "Eval_StdReturn : 11.64283275604248\n",
            "Eval_MaxReturn : 88.0\n",
            "Eval_MinReturn : 54.0\n",
            "Eval_AverageEpLen : 72.33333333333333\n",
            "Train_AverageReturn : 92.2727279663086\n",
            "Train_StdReturn : 32.01729965209961\n",
            "Train_MaxReturn : 157.0\n",
            "Train_MinReturn : 57.0\n",
            "Train_AverageEpLen : 92.27272727272727\n",
            "Train_EnvstepsSoFar : 68182\n",
            "TimeSinceStart : 101.3262345790863\n",
            "Training Loss : 21878.671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 69.0\n",
            "Eval_StdReturn : 18.583147048950195\n",
            "Eval_MaxReturn : 108.0\n",
            "Eval_MinReturn : 54.0\n",
            "Eval_AverageEpLen : 69.0\n",
            "Train_AverageReturn : 79.53845977783203\n",
            "Train_StdReturn : 17.946996688842773\n",
            "Train_MaxReturn : 111.0\n",
            "Train_MinReturn : 48.0\n",
            "Train_AverageEpLen : 79.53846153846153\n",
            "Train_EnvstepsSoFar : 69216\n",
            "TimeSinceStart : 102.92574572563171\n",
            "Training Loss : 17207.140625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 73.0\n",
            "Eval_StdReturn : 16.031219482421875\n",
            "Eval_MaxReturn : 94.0\n",
            "Eval_MinReturn : 48.0\n",
            "Eval_AverageEpLen : 73.0\n",
            "Train_AverageReturn : 70.93333435058594\n",
            "Train_StdReturn : 15.804923057556152\n",
            "Train_MaxReturn : 114.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 70.93333333333334\n",
            "Train_EnvstepsSoFar : 70280\n",
            "TimeSinceStart : 104.5452151298523\n",
            "Training Loss : 14724.052734375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 61.42856979370117\n",
            "Eval_StdReturn : 7.027642250061035\n",
            "Eval_MaxReturn : 72.0\n",
            "Eval_MinReturn : 49.0\n",
            "Eval_AverageEpLen : 61.42857142857143\n",
            "Train_AverageReturn : 65.375\n",
            "Train_StdReturn : 16.859251022338867\n",
            "Train_MaxReturn : 109.0\n",
            "Train_MinReturn : 47.0\n",
            "Train_AverageEpLen : 65.375\n",
            "Train_EnvstepsSoFar : 71326\n",
            "TimeSinceStart : 106.08674025535583\n",
            "Training Loss : 15721.38671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 65.14286041259766\n",
            "Eval_StdReturn : 10.232003211975098\n",
            "Eval_MaxReturn : 84.0\n",
            "Eval_MinReturn : 49.0\n",
            "Eval_AverageEpLen : 65.14285714285714\n",
            "Train_AverageReturn : 65.5625\n",
            "Train_StdReturn : 17.237201690673828\n",
            "Train_MaxReturn : 97.0\n",
            "Train_MinReturn : 39.0\n",
            "Train_AverageEpLen : 65.5625\n",
            "Train_EnvstepsSoFar : 72375\n",
            "TimeSinceStart : 107.82336521148682\n",
            "Training Loss : 15436.0966796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 64.0\n",
            "Eval_StdReturn : 11.964232444763184\n",
            "Eval_MaxReturn : 84.0\n",
            "Eval_MinReturn : 48.0\n",
            "Eval_AverageEpLen : 64.0\n",
            "Train_AverageReturn : 65.0625\n",
            "Train_StdReturn : 13.658645629882812\n",
            "Train_MaxReturn : 97.0\n",
            "Train_MinReturn : 46.0\n",
            "Train_AverageEpLen : 65.0625\n",
            "Train_EnvstepsSoFar : 73416\n",
            "TimeSinceStart : 109.34491419792175\n",
            "Training Loss : 13700.861328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 62.71428680419922\n",
            "Eval_StdReturn : 10.38837718963623\n",
            "Eval_MaxReturn : 81.0\n",
            "Eval_MinReturn : 48.0\n",
            "Eval_AverageEpLen : 62.714285714285715\n",
            "Train_AverageReturn : 60.82352828979492\n",
            "Train_StdReturn : 13.844664573669434\n",
            "Train_MaxReturn : 97.0\n",
            "Train_MinReturn : 43.0\n",
            "Train_AverageEpLen : 60.8235294117647\n",
            "Train_EnvstepsSoFar : 74450\n",
            "TimeSinceStart : 110.81994819641113\n",
            "Training Loss : 16286.037109375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 64.28571319580078\n",
            "Eval_StdReturn : 17.67738151550293\n",
            "Eval_MaxReturn : 93.0\n",
            "Eval_MinReturn : 46.0\n",
            "Eval_AverageEpLen : 64.28571428571429\n",
            "Train_AverageReturn : 62.9375\n",
            "Train_StdReturn : 17.397804260253906\n",
            "Train_MaxReturn : 97.0\n",
            "Train_MinReturn : 39.0\n",
            "Train_AverageEpLen : 62.9375\n",
            "Train_EnvstepsSoFar : 75457\n",
            "TimeSinceStart : 112.31887340545654\n",
            "Training Loss : 15720.544921875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 60.71428680419922\n",
            "Eval_StdReturn : 11.335333824157715\n",
            "Eval_MaxReturn : 81.0\n",
            "Eval_MinReturn : 50.0\n",
            "Eval_AverageEpLen : 60.714285714285715\n",
            "Train_AverageReturn : 62.764705657958984\n",
            "Train_StdReturn : 15.603655815124512\n",
            "Train_MaxReturn : 100.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 62.76470588235294\n",
            "Train_EnvstepsSoFar : 76524\n",
            "TimeSinceStart : 113.81249523162842\n",
            "Training Loss : 12523.8603515625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 69.42857360839844\n",
            "Eval_StdReturn : 18.204059600830078\n",
            "Eval_MaxReturn : 102.0\n",
            "Eval_MinReturn : 50.0\n",
            "Eval_AverageEpLen : 69.42857142857143\n",
            "Train_AverageReturn : 63.29411697387695\n",
            "Train_StdReturn : 12.759147644042969\n",
            "Train_MaxReturn : 91.0\n",
            "Train_MinReturn : 47.0\n",
            "Train_AverageEpLen : 63.294117647058826\n",
            "Train_EnvstepsSoFar : 77600\n",
            "TimeSinceStart : 115.39635848999023\n",
            "Training Loss : 13618.05078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 78.16666412353516\n",
            "Eval_StdReturn : 23.989002227783203\n",
            "Eval_MaxReturn : 114.0\n",
            "Eval_MinReturn : 50.0\n",
            "Eval_AverageEpLen : 78.16666666666667\n",
            "Train_AverageReturn : 68.0\n",
            "Train_StdReturn : 11.7189302444458\n",
            "Train_MaxReturn : 96.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 68.0\n",
            "Train_EnvstepsSoFar : 78620\n",
            "TimeSinceStart : 116.94183158874512\n",
            "Training Loss : 16177.3896484375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 71.16666412353516\n",
            "Eval_StdReturn : 20.061710357666016\n",
            "Eval_MaxReturn : 99.0\n",
            "Eval_MinReturn : 50.0\n",
            "Eval_AverageEpLen : 71.16666666666667\n",
            "Train_AverageReturn : 71.57142639160156\n",
            "Train_StdReturn : 19.249170303344727\n",
            "Train_MaxReturn : 106.0\n",
            "Train_MinReturn : 49.0\n",
            "Train_AverageEpLen : 71.57142857142857\n",
            "Train_EnvstepsSoFar : 79622\n",
            "TimeSinceStart : 118.39671683311462\n",
            "Training Loss : 15231.0576171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 66.57142639160156\n",
            "Eval_StdReturn : 10.55403995513916\n",
            "Eval_MaxReturn : 88.0\n",
            "Eval_MinReturn : 52.0\n",
            "Eval_AverageEpLen : 66.57142857142857\n",
            "Train_AverageReturn : 65.375\n",
            "Train_StdReturn : 15.579935073852539\n",
            "Train_MaxReturn : 98.0\n",
            "Train_MinReturn : 44.0\n",
            "Train_AverageEpLen : 65.375\n",
            "Train_EnvstepsSoFar : 80668\n",
            "TimeSinceStart : 119.93588781356812\n",
            "Training Loss : 15278.583984375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.0\n",
            "Eval_StdReturn : 13.089435577392578\n",
            "Eval_MaxReturn : 93.0\n",
            "Eval_MinReturn : 54.0\n",
            "Eval_AverageEpLen : 67.0\n",
            "Train_AverageReturn : 63.5625\n",
            "Train_StdReturn : 12.06735610961914\n",
            "Train_MaxReturn : 87.0\n",
            "Train_MinReturn : 49.0\n",
            "Train_AverageEpLen : 63.5625\n",
            "Train_EnvstepsSoFar : 81685\n",
            "TimeSinceStart : 121.38455200195312\n",
            "Training Loss : 13168.5029296875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 75.66666412353516\n",
            "Eval_StdReturn : 12.77584457397461\n",
            "Eval_MaxReturn : 93.0\n",
            "Eval_MinReturn : 60.0\n",
            "Eval_AverageEpLen : 75.66666666666667\n",
            "Train_AverageReturn : 68.19999694824219\n",
            "Train_StdReturn : 20.30500602722168\n",
            "Train_MaxReturn : 128.0\n",
            "Train_MinReturn : 45.0\n",
            "Train_AverageEpLen : 68.2\n",
            "Train_EnvstepsSoFar : 82708\n",
            "TimeSinceStart : 122.89111185073853\n",
            "Training Loss : 16208.5029296875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 90.0\n",
            "Eval_StdReturn : 24.62519073486328\n",
            "Eval_MaxReturn : 121.0\n",
            "Eval_MinReturn : 61.0\n",
            "Eval_AverageEpLen : 90.0\n",
            "Train_AverageReturn : 79.15384674072266\n",
            "Train_StdReturn : 23.585674285888672\n",
            "Train_MaxReturn : 129.0\n",
            "Train_MinReturn : 53.0\n",
            "Train_AverageEpLen : 79.15384615384616\n",
            "Train_EnvstepsSoFar : 83737\n",
            "TimeSinceStart : 124.41793417930603\n",
            "Training Loss : 19240.0546875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 86.80000305175781\n",
            "Eval_StdReturn : 21.37662124633789\n",
            "Eval_MaxReturn : 124.0\n",
            "Eval_MinReturn : 63.0\n",
            "Eval_AverageEpLen : 86.8\n",
            "Train_AverageReturn : 78.21428680419922\n",
            "Train_StdReturn : 17.101287841796875\n",
            "Train_MaxReturn : 111.0\n",
            "Train_MinReturn : 57.0\n",
            "Train_AverageEpLen : 78.21428571428571\n",
            "Train_EnvstepsSoFar : 84832\n",
            "TimeSinceStart : 125.96143245697021\n",
            "Training Loss : 16446.91015625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 82.0\n",
            "Eval_StdReturn : 17.729448318481445\n",
            "Eval_MaxReturn : 109.0\n",
            "Eval_MinReturn : 60.0\n",
            "Eval_AverageEpLen : 82.0\n",
            "Train_AverageReturn : 86.91666412353516\n",
            "Train_StdReturn : 25.02151870727539\n",
            "Train_MaxReturn : 135.0\n",
            "Train_MinReturn : 64.0\n",
            "Train_AverageEpLen : 86.91666666666667\n",
            "Train_EnvstepsSoFar : 85875\n",
            "TimeSinceStart : 127.52613997459412\n",
            "Training Loss : 19173.05078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 108.5\n",
            "Eval_StdReturn : 27.941904067993164\n",
            "Eval_MaxReturn : 146.0\n",
            "Eval_MinReturn : 76.0\n",
            "Eval_AverageEpLen : 108.5\n",
            "Train_AverageReturn : 85.25\n",
            "Train_StdReturn : 31.92210578918457\n",
            "Train_MaxReturn : 180.0\n",
            "Train_MinReturn : 58.0\n",
            "Train_AverageEpLen : 85.25\n",
            "Train_EnvstepsSoFar : 86898\n",
            "TimeSinceStart : 129.06602787971497\n",
            "Training Loss : 20401.25\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 107.25\n",
            "Eval_StdReturn : 45.9367790222168\n",
            "Eval_MaxReturn : 185.0\n",
            "Eval_MinReturn : 71.0\n",
            "Eval_AverageEpLen : 107.25\n",
            "Train_AverageReturn : 87.16666412353516\n",
            "Train_StdReturn : 23.995946884155273\n",
            "Train_MaxReturn : 144.0\n",
            "Train_MinReturn : 55.0\n",
            "Train_AverageEpLen : 87.16666666666667\n",
            "Train_EnvstepsSoFar : 87944\n",
            "TimeSinceStart : 130.55869841575623\n",
            "Training Loss : 21349.810546875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 112.5\n",
            "Eval_StdReturn : 23.900836944580078\n",
            "Eval_MaxReturn : 153.0\n",
            "Eval_MinReturn : 91.0\n",
            "Eval_AverageEpLen : 112.5\n",
            "Train_AverageReturn : 101.30000305175781\n",
            "Train_StdReturn : 31.59129524230957\n",
            "Train_MaxReturn : 189.0\n",
            "Train_MinReturn : 76.0\n",
            "Train_AverageEpLen : 101.3\n",
            "Train_EnvstepsSoFar : 88957\n",
            "TimeSinceStart : 132.21210718154907\n",
            "Training Loss : 24094.08203125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 158.0\n",
            "Eval_StdReturn : 32.124755859375\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 122.0\n",
            "Eval_AverageEpLen : 158.0\n",
            "Train_AverageReturn : 102.7272720336914\n",
            "Train_StdReturn : 24.976682662963867\n",
            "Train_MaxReturn : 149.0\n",
            "Train_MinReturn : 70.0\n",
            "Train_AverageEpLen : 102.72727272727273\n",
            "Train_EnvstepsSoFar : 90087\n",
            "TimeSinceStart : 134.14071774482727\n",
            "Training Loss : 27077.05859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 140.3333282470703\n",
            "Eval_StdReturn : 29.555971145629883\n",
            "Eval_MaxReturn : 170.0\n",
            "Eval_MinReturn : 100.0\n",
            "Eval_AverageEpLen : 140.33333333333334\n",
            "Train_AverageReturn : 131.125\n",
            "Train_StdReturn : 46.182891845703125\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 83.0\n",
            "Train_AverageEpLen : 131.125\n",
            "Train_EnvstepsSoFar : 91136\n",
            "TimeSinceStart : 135.6269416809082\n",
            "Training Loss : 37379.015625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 110.5\n",
            "Eval_StdReturn : 20.71834945678711\n",
            "Eval_MaxReturn : 138.0\n",
            "Eval_MinReturn : 89.0\n",
            "Eval_AverageEpLen : 110.5\n",
            "Train_AverageReturn : 117.0\n",
            "Train_StdReturn : 22.181072235107422\n",
            "Train_MaxReturn : 158.0\n",
            "Train_MinReturn : 90.0\n",
            "Train_AverageEpLen : 117.0\n",
            "Train_EnvstepsSoFar : 92189\n",
            "TimeSinceStart : 137.16696691513062\n",
            "Training Loss : 26915.986328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 163.3333282470703\n",
            "Eval_StdReturn : 28.015867233276367\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 132.0\n",
            "Eval_AverageEpLen : 163.33333333333334\n",
            "Train_AverageReturn : 126.125\n",
            "Train_StdReturn : 23.040386199951172\n",
            "Train_MaxReturn : 172.0\n",
            "Train_MinReturn : 99.0\n",
            "Train_AverageEpLen : 126.125\n",
            "Train_EnvstepsSoFar : 93198\n",
            "TimeSinceStart : 138.67359328269958\n",
            "Training Loss : 28523.3671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 183.6666717529297\n",
            "Eval_StdReturn : 23.09882164001465\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 151.0\n",
            "Eval_AverageEpLen : 183.66666666666666\n",
            "Train_AverageReturn : 154.0\n",
            "Train_StdReturn : 30.78032875061035\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 119.0\n",
            "Train_AverageEpLen : 154.0\n",
            "Train_EnvstepsSoFar : 94276\n",
            "TimeSinceStart : 140.3275785446167\n",
            "Training Loss : 41716.375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 168.0\n",
            "Eval_StdReturn : 28.390138626098633\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 131.0\n",
            "Eval_AverageEpLen : 168.0\n",
            "Train_AverageReturn : 160.0\n",
            "Train_StdReturn : 32.33750534057617\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 110.0\n",
            "Train_AverageEpLen : 160.0\n",
            "Train_EnvstepsSoFar : 95396\n",
            "TimeSinceStart : 142.02158284187317\n",
            "Training Loss : 42804.765625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 145.0\n",
            "Eval_StdReturn : 39.25132751464844\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 111.0\n",
            "Eval_AverageEpLen : 145.0\n",
            "Train_AverageReturn : 154.0\n",
            "Train_StdReturn : 37.205989837646484\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 104.0\n",
            "Train_AverageEpLen : 154.0\n",
            "Train_EnvstepsSoFar : 96474\n",
            "TimeSinceStart : 143.5800426006317\n",
            "Training Loss : 39781.8515625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 154.6666717529297\n",
            "Eval_StdReturn : 33.76717758178711\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 119.0\n",
            "Eval_AverageEpLen : 154.66666666666666\n",
            "Train_AverageReturn : 138.125\n",
            "Train_StdReturn : 42.48069381713867\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 138.125\n",
            "Train_EnvstepsSoFar : 97579\n",
            "TimeSinceStart : 145.46145868301392\n",
            "Training Loss : 42927.484375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 180.0\n",
            "Eval_StdReturn : 20.39607810974121\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 152.0\n",
            "Eval_AverageEpLen : 180.0\n",
            "Train_AverageReturn : 144.75\n",
            "Train_StdReturn : 33.703670501708984\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 111.0\n",
            "Train_AverageEpLen : 144.75\n",
            "Train_EnvstepsSoFar : 98737\n",
            "TimeSinceStart : 147.20490980148315\n",
            "Training Loss : 41273.3828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 164.0\n",
            "Eval_StdReturn : 28.994253158569336\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 129.0\n",
            "Eval_AverageEpLen : 164.0\n",
            "Train_AverageReturn : 162.0\n",
            "Train_StdReturn : 25.286924362182617\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 123.0\n",
            "Train_AverageEpLen : 162.0\n",
            "Train_EnvstepsSoFar : 99871\n",
            "TimeSinceStart : 148.83004879951477\n",
            "Training Loss : 49403.0859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 169.3333282470703\n",
            "Eval_StdReturn : 6.12825870513916\n",
            "Eval_MaxReturn : 177.0\n",
            "Eval_MinReturn : 162.0\n",
            "Eval_AverageEpLen : 169.33333333333334\n",
            "Train_AverageReturn : 170.0\n",
            "Train_StdReturn : 22.41279411315918\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 141.0\n",
            "Train_AverageEpLen : 170.0\n",
            "Train_EnvstepsSoFar : 100891\n",
            "TimeSinceStart : 150.36718320846558\n",
            "Training Loss : 43222.26171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 178.0\n",
            "Eval_StdReturn : 25.66450309753418\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 142.0\n",
            "Eval_AverageEpLen : 178.0\n",
            "Train_AverageReturn : 170.1666717529297\n",
            "Train_StdReturn : 29.655895233154297\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 120.0\n",
            "Train_AverageEpLen : 170.16666666666666\n",
            "Train_EnvstepsSoFar : 101912\n",
            "TimeSinceStart : 151.92362427711487\n",
            "Training Loss : 51011.51953125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 156.0\n",
            "Eval_StdReturn : 31.790985107421875\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 126.0\n",
            "Eval_AverageEpLen : 156.0\n",
            "Train_AverageReturn : 169.8333282470703\n",
            "Train_StdReturn : 22.952245712280273\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 137.0\n",
            "Train_AverageEpLen : 169.83333333333334\n",
            "Train_EnvstepsSoFar : 102931\n",
            "TimeSinceStart : 153.42232966423035\n",
            "Training Loss : 49788.3828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 192.0\n",
            "Eval_StdReturn : 11.313708305358887\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 176.0\n",
            "Eval_AverageEpLen : 192.0\n",
            "Train_AverageReturn : 175.8333282470703\n",
            "Train_StdReturn : 23.36248207092285\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 141.0\n",
            "Train_AverageEpLen : 175.83333333333334\n",
            "Train_EnvstepsSoFar : 103986\n",
            "TimeSinceStart : 155.0846095085144\n",
            "Training Loss : 53168.3359375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 181.0\n",
            "Eval_StdReturn : 16.391054153442383\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 160.0\n",
            "Eval_AverageEpLen : 181.0\n",
            "Train_AverageReturn : 185.1666717529297\n",
            "Train_StdReturn : 22.80655288696289\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 140.0\n",
            "Train_AverageEpLen : 185.16666666666666\n",
            "Train_EnvstepsSoFar : 105097\n",
            "TimeSinceStart : 156.7479808330536\n",
            "Training Loss : 57939.0\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 \\\n",
        "-rtg -dsa --exp_name q1_sb_rtg_dsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psW_aBlMmWfZ",
        "outputId": "6a4d19e5-8dea-49b1-aefd-4467f6782065"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q1_sb_rtg_dsa_CartPole-v0_04-02-2022_16-40-57\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 28.85714340209961\n",
            "Eval_StdReturn : 13.579696655273438\n",
            "Eval_MaxReturn : 59.0\n",
            "Eval_MinReturn : 12.0\n",
            "Eval_AverageEpLen : 28.857142857142858\n",
            "Train_AverageReturn : 27.648649215698242\n",
            "Train_StdReturn : 12.643160820007324\n",
            "Train_MaxReturn : 65.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 27.64864864864865\n",
            "Train_EnvstepsSoFar : 1023\n",
            "TimeSinceStart : 1.5118110179901123\n",
            "Training Loss : 22864.16796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 35.75\n",
            "Eval_StdReturn : 18.828723907470703\n",
            "Eval_MaxReturn : 84.0\n",
            "Eval_MinReturn : 17.0\n",
            "Eval_AverageEpLen : 35.75\n",
            "Train_AverageReturn : 34.517242431640625\n",
            "Train_StdReturn : 24.89650535583496\n",
            "Train_MaxReturn : 124.0\n",
            "Train_MinReturn : 10.0\n",
            "Train_AverageEpLen : 34.51724137931034\n",
            "Train_EnvstepsSoFar : 2024\n",
            "TimeSinceStart : 2.9677908420562744\n",
            "Training Loss : 34364.06640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 45.77777862548828\n",
            "Eval_StdReturn : 21.760196685791016\n",
            "Eval_MaxReturn : 86.0\n",
            "Eval_MinReturn : 17.0\n",
            "Eval_AverageEpLen : 45.77777777777778\n",
            "Train_AverageReturn : 36.60714340209961\n",
            "Train_StdReturn : 19.032564163208008\n",
            "Train_MaxReturn : 80.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 36.607142857142854\n",
            "Train_EnvstepsSoFar : 3049\n",
            "TimeSinceStart : 4.5085344314575195\n",
            "Training Loss : 30112.818359375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 50.375\n",
            "Eval_StdReturn : 21.885711669921875\n",
            "Eval_MaxReturn : 96.0\n",
            "Eval_MinReturn : 24.0\n",
            "Eval_AverageEpLen : 50.375\n",
            "Train_AverageReturn : 48.619049072265625\n",
            "Train_StdReturn : 26.29227066040039\n",
            "Train_MaxReturn : 121.0\n",
            "Train_MinReturn : 17.0\n",
            "Train_AverageEpLen : 48.61904761904762\n",
            "Train_EnvstepsSoFar : 4070\n",
            "TimeSinceStart : 6.022811412811279\n",
            "Training Loss : 39109.4453125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 72.71428680419922\n",
            "Eval_StdReturn : 34.927330017089844\n",
            "Eval_MaxReturn : 155.0\n",
            "Eval_MinReturn : 45.0\n",
            "Eval_AverageEpLen : 72.71428571428571\n",
            "Train_AverageReturn : 59.82352828979492\n",
            "Train_StdReturn : 25.2452335357666\n",
            "Train_MaxReturn : 104.0\n",
            "Train_MinReturn : 25.0\n",
            "Train_AverageEpLen : 59.8235294117647\n",
            "Train_EnvstepsSoFar : 5087\n",
            "TimeSinceStart : 7.630367279052734\n",
            "Training Loss : 43453.75390625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 58.42856979370117\n",
            "Eval_StdReturn : 24.10013198852539\n",
            "Eval_MaxReturn : 104.0\n",
            "Eval_MinReturn : 36.0\n",
            "Eval_AverageEpLen : 58.42857142857143\n",
            "Train_AverageReturn : 52.29999923706055\n",
            "Train_StdReturn : 16.392375946044922\n",
            "Train_MaxReturn : 100.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 52.3\n",
            "Train_EnvstepsSoFar : 6133\n",
            "TimeSinceStart : 9.150901079177856\n",
            "Training Loss : 35003.265625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 56.5\n",
            "Eval_StdReturn : 14.124446868896484\n",
            "Eval_MaxReturn : 86.0\n",
            "Eval_MinReturn : 37.0\n",
            "Eval_AverageEpLen : 56.5\n",
            "Train_AverageReturn : 81.30769348144531\n",
            "Train_StdReturn : 47.0178108215332\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 81.3076923076923\n",
            "Train_EnvstepsSoFar : 7190\n",
            "TimeSinceStart : 10.71634817123413\n",
            "Training Loss : 67057.5078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 63.57143020629883\n",
            "Eval_StdReturn : 20.638778686523438\n",
            "Eval_MaxReturn : 96.0\n",
            "Eval_MinReturn : 33.0\n",
            "Eval_AverageEpLen : 63.57142857142857\n",
            "Train_AverageReturn : 55.94444274902344\n",
            "Train_StdReturn : 17.19971466064453\n",
            "Train_MaxReturn : 97.0\n",
            "Train_MinReturn : 19.0\n",
            "Train_AverageEpLen : 55.94444444444444\n",
            "Train_EnvstepsSoFar : 8197\n",
            "TimeSinceStart : 12.262415885925293\n",
            "Training Loss : 34342.21875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 51.5\n",
            "Eval_StdReturn : 21.05944061279297\n",
            "Eval_MaxReturn : 86.0\n",
            "Eval_MinReturn : 29.0\n",
            "Eval_AverageEpLen : 51.5\n",
            "Train_AverageReturn : 71.80000305175781\n",
            "Train_StdReturn : 31.726329803466797\n",
            "Train_MaxReturn : 154.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 71.8\n",
            "Train_EnvstepsSoFar : 9274\n",
            "TimeSinceStart : 13.849619388580322\n",
            "Training Loss : 52076.828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 108.5999984741211\n",
            "Eval_StdReturn : 53.40262222290039\n",
            "Eval_MaxReturn : 182.0\n",
            "Eval_MinReturn : 60.0\n",
            "Eval_AverageEpLen : 108.6\n",
            "Train_AverageReturn : 77.76923370361328\n",
            "Train_StdReturn : 40.89226531982422\n",
            "Train_MaxReturn : 164.0\n",
            "Train_MinReturn : 28.0\n",
            "Train_AverageEpLen : 77.76923076923077\n",
            "Train_EnvstepsSoFar : 10285\n",
            "TimeSinceStart : 15.472671747207642\n",
            "Training Loss : 55494.92578125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 115.0\n",
            "Eval_StdReturn : 51.951900482177734\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 64.0\n",
            "Eval_AverageEpLen : 115.0\n",
            "Train_AverageReturn : 77.64286041259766\n",
            "Train_StdReturn : 33.064022064208984\n",
            "Train_MaxReturn : 183.0\n",
            "Train_MinReturn : 43.0\n",
            "Train_AverageEpLen : 77.64285714285714\n",
            "Train_EnvstepsSoFar : 11372\n",
            "TimeSinceStart : 17.051501750946045\n",
            "Training Loss : 52937.6484375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 69.66666412353516\n",
            "Eval_StdReturn : 33.34499740600586\n",
            "Eval_MaxReturn : 139.0\n",
            "Eval_MinReturn : 34.0\n",
            "Eval_AverageEpLen : 69.66666666666667\n",
            "Train_AverageReturn : 89.66666412353516\n",
            "Train_StdReturn : 35.96139907836914\n",
            "Train_MaxReturn : 172.0\n",
            "Train_MinReturn : 41.0\n",
            "Train_AverageEpLen : 89.66666666666667\n",
            "Train_EnvstepsSoFar : 12448\n",
            "TimeSinceStart : 18.61513066291809\n",
            "Training Loss : 59370.328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 56.625\n",
            "Eval_StdReturn : 28.77037239074707\n",
            "Eval_MaxReturn : 119.0\n",
            "Eval_MinReturn : 26.0\n",
            "Eval_AverageEpLen : 56.625\n",
            "Train_AverageReturn : 78.73332977294922\n",
            "Train_StdReturn : 44.299686431884766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 78.73333333333333\n",
            "Train_EnvstepsSoFar : 13629\n",
            "TimeSinceStart : 20.330328464508057\n",
            "Training Loss : 58097.31640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 82.33333587646484\n",
            "Eval_StdReturn : 19.473628997802734\n",
            "Eval_MaxReturn : 111.0\n",
            "Eval_MinReturn : 45.0\n",
            "Eval_AverageEpLen : 82.33333333333333\n",
            "Train_AverageReturn : 68.80000305175781\n",
            "Train_StdReturn : 37.54322052001953\n",
            "Train_MaxReturn : 185.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 68.8\n",
            "Train_EnvstepsSoFar : 14661\n",
            "TimeSinceStart : 21.91715359687805\n",
            "Training Loss : 48788.4609375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 86.0\n",
            "Eval_StdReturn : 31.83080291748047\n",
            "Eval_MaxReturn : 124.0\n",
            "Eval_MinReturn : 42.0\n",
            "Eval_AverageEpLen : 86.0\n",
            "Train_AverageReturn : 81.15384674072266\n",
            "Train_StdReturn : 26.749107360839844\n",
            "Train_MaxReturn : 129.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 81.15384615384616\n",
            "Train_EnvstepsSoFar : 15716\n",
            "TimeSinceStart : 23.482585906982422\n",
            "Training Loss : 48838.5703125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 129.5\n",
            "Eval_StdReturn : 28.674901962280273\n",
            "Eval_MaxReturn : 163.0\n",
            "Eval_MinReturn : 84.0\n",
            "Eval_AverageEpLen : 129.5\n",
            "Train_AverageReturn : 88.16666412353516\n",
            "Train_StdReturn : 20.18593978881836\n",
            "Train_MaxReturn : 116.0\n",
            "Train_MinReturn : 51.0\n",
            "Train_AverageEpLen : 88.16666666666667\n",
            "Train_EnvstepsSoFar : 16774\n",
            "TimeSinceStart : 25.11539626121521\n",
            "Training Loss : 48618.046875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 148.3333282470703\n",
            "Eval_StdReturn : 20.677417755126953\n",
            "Eval_MaxReturn : 177.0\n",
            "Eval_MinReturn : 129.0\n",
            "Eval_AverageEpLen : 148.33333333333334\n",
            "Train_AverageReturn : 113.4000015258789\n",
            "Train_StdReturn : 43.968624114990234\n",
            "Train_MaxReturn : 190.0\n",
            "Train_MinReturn : 44.0\n",
            "Train_AverageEpLen : 113.4\n",
            "Train_EnvstepsSoFar : 17908\n",
            "TimeSinceStart : 26.730021476745605\n",
            "Training Loss : 73504.796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 156.3333282470703\n",
            "Eval_StdReturn : 22.42518424987793\n",
            "Eval_MaxReturn : 187.0\n",
            "Eval_MinReturn : 134.0\n",
            "Eval_AverageEpLen : 156.33333333333334\n",
            "Train_AverageReturn : 147.57142639160156\n",
            "Train_StdReturn : 35.516021728515625\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 93.0\n",
            "Train_AverageEpLen : 147.57142857142858\n",
            "Train_EnvstepsSoFar : 18941\n",
            "TimeSinceStart : 28.30425715446472\n",
            "Training Loss : 76735.21875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 100.19999694824219\n",
            "Eval_StdReturn : 40.40494918823242\n",
            "Eval_MaxReturn : 173.0\n",
            "Eval_MinReturn : 60.0\n",
            "Eval_AverageEpLen : 100.2\n",
            "Train_AverageReturn : 127.625\n",
            "Train_StdReturn : 37.9306526184082\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 74.0\n",
            "Train_AverageEpLen : 127.625\n",
            "Train_EnvstepsSoFar : 19962\n",
            "TimeSinceStart : 29.96126389503479\n",
            "Training Loss : 69618.6875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 110.75\n",
            "Eval_StdReturn : 14.618053436279297\n",
            "Eval_MaxReturn : 125.0\n",
            "Eval_MinReturn : 90.0\n",
            "Eval_AverageEpLen : 110.75\n",
            "Train_AverageReturn : 110.19999694824219\n",
            "Train_StdReturn : 36.049407958984375\n",
            "Train_MaxReturn : 158.0\n",
            "Train_MinReturn : 41.0\n",
            "Train_AverageEpLen : 110.2\n",
            "Train_EnvstepsSoFar : 21064\n",
            "TimeSinceStart : 31.569462537765503\n",
            "Training Loss : 57578.00390625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 101.75\n",
            "Eval_StdReturn : 16.990806579589844\n",
            "Eval_MaxReturn : 125.0\n",
            "Eval_MinReturn : 85.0\n",
            "Eval_AverageEpLen : 101.75\n",
            "Train_AverageReturn : 98.0\n",
            "Train_StdReturn : 36.91020965576172\n",
            "Train_MaxReturn : 189.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 98.0\n",
            "Train_EnvstepsSoFar : 22142\n",
            "TimeSinceStart : 33.15805983543396\n",
            "Training Loss : 55245.87109375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 87.0\n",
            "Eval_StdReturn : 10.825894355773926\n",
            "Eval_MaxReturn : 104.0\n",
            "Eval_MinReturn : 70.0\n",
            "Eval_AverageEpLen : 87.0\n",
            "Train_AverageReturn : 79.61538696289062\n",
            "Train_StdReturn : 21.759307861328125\n",
            "Train_MaxReturn : 126.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 79.61538461538461\n",
            "Train_EnvstepsSoFar : 23177\n",
            "TimeSinceStart : 34.917468309402466\n",
            "Training Loss : 41048.734375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 87.80000305175781\n",
            "Eval_StdReturn : 46.60643768310547\n",
            "Eval_MaxReturn : 179.0\n",
            "Eval_MinReturn : 55.0\n",
            "Eval_AverageEpLen : 87.8\n",
            "Train_AverageReturn : 91.45454406738281\n",
            "Train_StdReturn : 26.081562042236328\n",
            "Train_MaxReturn : 146.0\n",
            "Train_MinReturn : 49.0\n",
            "Train_AverageEpLen : 91.45454545454545\n",
            "Train_EnvstepsSoFar : 24183\n",
            "TimeSinceStart : 36.382792711257935\n",
            "Training Loss : 45279.5234375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 111.0\n",
            "Eval_StdReturn : 38.190311431884766\n",
            "Eval_MaxReturn : 177.0\n",
            "Eval_MinReturn : 86.0\n",
            "Eval_AverageEpLen : 111.0\n",
            "Train_AverageReturn : 100.4000015258789\n",
            "Train_StdReturn : 37.21881103515625\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 53.0\n",
            "Train_AverageEpLen : 100.4\n",
            "Train_EnvstepsSoFar : 25187\n",
            "TimeSinceStart : 37.84831190109253\n",
            "Training Loss : 51114.140625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 101.4000015258789\n",
            "Eval_StdReturn : 16.632497787475586\n",
            "Eval_MaxReturn : 123.0\n",
            "Eval_MinReturn : 72.0\n",
            "Eval_AverageEpLen : 101.4\n",
            "Train_AverageReturn : 101.45454406738281\n",
            "Train_StdReturn : 43.34505081176758\n",
            "Train_MaxReturn : 194.0\n",
            "Train_MinReturn : 62.0\n",
            "Train_AverageEpLen : 101.45454545454545\n",
            "Train_EnvstepsSoFar : 26303\n",
            "TimeSinceStart : 39.52528500556946\n",
            "Training Loss : 53436.90625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 120.25\n",
            "Eval_StdReturn : 31.89337730407715\n",
            "Eval_MaxReturn : 164.0\n",
            "Eval_MinReturn : 74.0\n",
            "Eval_AverageEpLen : 120.25\n",
            "Train_AverageReturn : 98.45454406738281\n",
            "Train_StdReturn : 38.80572509765625\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 58.0\n",
            "Train_AverageEpLen : 98.45454545454545\n",
            "Train_EnvstepsSoFar : 27386\n",
            "TimeSinceStart : 41.15051198005676\n",
            "Training Loss : 52446.5625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 161.0\n",
            "Eval_StdReturn : 47.58851623535156\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 94.0\n",
            "Eval_AverageEpLen : 161.0\n",
            "Train_AverageReturn : 112.11111450195312\n",
            "Train_StdReturn : 44.920780181884766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 51.0\n",
            "Train_AverageEpLen : 112.11111111111111\n",
            "Train_EnvstepsSoFar : 28395\n",
            "TimeSinceStart : 42.813037157058716\n",
            "Training Loss : 52149.01953125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 134.6666717529297\n",
            "Eval_StdReturn : 35.70558547973633\n",
            "Eval_MaxReturn : 185.0\n",
            "Eval_MinReturn : 106.0\n",
            "Eval_AverageEpLen : 134.66666666666666\n",
            "Train_AverageReturn : 125.125\n",
            "Train_StdReturn : 38.49168014526367\n",
            "Train_MaxReturn : 189.0\n",
            "Train_MinReturn : 86.0\n",
            "Train_AverageEpLen : 125.125\n",
            "Train_EnvstepsSoFar : 29396\n",
            "TimeSinceStart : 44.307852029800415\n",
            "Training Loss : 59813.3671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 153.3333282470703\n",
            "Eval_StdReturn : 51.23366928100586\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 82.0\n",
            "Eval_AverageEpLen : 153.33333333333334\n",
            "Train_AverageReturn : 135.5\n",
            "Train_StdReturn : 41.596275329589844\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 87.0\n",
            "Train_AverageEpLen : 135.5\n",
            "Train_EnvstepsSoFar : 30480\n",
            "TimeSinceStart : 45.91549849510193\n",
            "Training Loss : 69103.984375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 115.75\n",
            "Eval_StdReturn : 21.878929138183594\n",
            "Eval_MaxReturn : 151.0\n",
            "Eval_MinReturn : 96.0\n",
            "Eval_AverageEpLen : 115.75\n",
            "Train_AverageReturn : 115.4000015258789\n",
            "Train_StdReturn : 44.340049743652344\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 68.0\n",
            "Train_AverageEpLen : 115.4\n",
            "Train_EnvstepsSoFar : 31634\n",
            "TimeSinceStart : 47.55087184906006\n",
            "Training Loss : 59027.1328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 109.25\n",
            "Eval_StdReturn : 21.545011520385742\n",
            "Eval_MaxReturn : 146.0\n",
            "Eval_MinReturn : 91.0\n",
            "Eval_AverageEpLen : 109.25\n",
            "Train_AverageReturn : 115.0\n",
            "Train_StdReturn : 31.162654876708984\n",
            "Train_MaxReturn : 187.0\n",
            "Train_MinReturn : 72.0\n",
            "Train_AverageEpLen : 115.0\n",
            "Train_EnvstepsSoFar : 32669\n",
            "TimeSinceStart : 49.04507517814636\n",
            "Training Loss : 54913.578125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 121.75\n",
            "Eval_StdReturn : 45.974857330322266\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 83.0\n",
            "Eval_AverageEpLen : 121.75\n",
            "Train_AverageReturn : 128.375\n",
            "Train_StdReturn : 28.1422176361084\n",
            "Train_MaxReturn : 169.0\n",
            "Train_MinReturn : 71.0\n",
            "Train_AverageEpLen : 128.375\n",
            "Train_EnvstepsSoFar : 33696\n",
            "TimeSinceStart : 50.60121965408325\n",
            "Training Loss : 53040.6953125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 117.25\n",
            "Eval_StdReturn : 32.26743698120117\n",
            "Eval_MaxReturn : 163.0\n",
            "Eval_MinReturn : 80.0\n",
            "Eval_AverageEpLen : 117.25\n",
            "Train_AverageReturn : 107.30000305175781\n",
            "Train_StdReturn : 20.13976287841797\n",
            "Train_MaxReturn : 145.0\n",
            "Train_MinReturn : 80.0\n",
            "Train_AverageEpLen : 107.3\n",
            "Train_EnvstepsSoFar : 34769\n",
            "TimeSinceStart : 52.197068214416504\n",
            "Training Loss : 46948.43359375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 111.4000015258789\n",
            "Eval_StdReturn : 36.02554702758789\n",
            "Eval_MaxReturn : 169.0\n",
            "Eval_MinReturn : 67.0\n",
            "Eval_AverageEpLen : 111.4\n",
            "Train_AverageReturn : 117.77777862548828\n",
            "Train_StdReturn : 38.5595588684082\n",
            "Train_MaxReturn : 199.0\n",
            "Train_MinReturn : 83.0\n",
            "Train_AverageEpLen : 117.77777777777777\n",
            "Train_EnvstepsSoFar : 35829\n",
            "TimeSinceStart : 53.91147494316101\n",
            "Training Loss : 56773.71875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 126.44444274902344\n",
            "Train_StdReturn : 54.60995101928711\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 57.0\n",
            "Train_AverageEpLen : 126.44444444444444\n",
            "Train_EnvstepsSoFar : 36967\n",
            "TimeSinceStart : 55.48844623565674\n",
            "Training Loss : 68398.28125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 134.6666717529297\n",
            "Eval_StdReturn : 18.803071975708008\n",
            "Eval_MaxReturn : 157.0\n",
            "Eval_MinReturn : 111.0\n",
            "Eval_AverageEpLen : 134.66666666666666\n",
            "Train_AverageReturn : 105.4000015258789\n",
            "Train_StdReturn : 29.88711929321289\n",
            "Train_MaxReturn : 159.0\n",
            "Train_MinReturn : 61.0\n",
            "Train_AverageEpLen : 105.4\n",
            "Train_EnvstepsSoFar : 38021\n",
            "TimeSinceStart : 56.9871461391449\n",
            "Training Loss : 47882.796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 99.4000015258789\n",
            "Eval_StdReturn : 7.7097344398498535\n",
            "Eval_MaxReturn : 107.0\n",
            "Eval_MinReturn : 90.0\n",
            "Eval_AverageEpLen : 99.4\n",
            "Train_AverageReturn : 145.0\n",
            "Train_StdReturn : 39.0164794921875\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 100.0\n",
            "Train_AverageEpLen : 145.0\n",
            "Train_EnvstepsSoFar : 39036\n",
            "TimeSinceStart : 58.904369831085205\n",
            "Training Loss : 63972.3515625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 109.0\n",
            "Eval_StdReturn : 17.50714111328125\n",
            "Eval_MaxReturn : 134.0\n",
            "Eval_MinReturn : 85.0\n",
            "Eval_AverageEpLen : 109.0\n",
            "Train_AverageReturn : 123.33333587646484\n",
            "Train_StdReturn : 46.15673828125\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 63.0\n",
            "Train_AverageEpLen : 123.33333333333333\n",
            "Train_EnvstepsSoFar : 40146\n",
            "TimeSinceStart : 60.4733567237854\n",
            "Training Loss : 56802.6953125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 126.25\n",
            "Eval_StdReturn : 43.819942474365234\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 86.0\n",
            "Eval_AverageEpLen : 126.25\n",
            "Train_AverageReturn : 104.9000015258789\n",
            "Train_StdReturn : 39.414337158203125\n",
            "Train_MaxReturn : 186.0\n",
            "Train_MinReturn : 65.0\n",
            "Train_AverageEpLen : 104.9\n",
            "Train_EnvstepsSoFar : 41195\n",
            "TimeSinceStart : 62.06711935997009\n",
            "Training Loss : 47673.984375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 156.0\n",
            "Eval_StdReturn : 31.496030807495117\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 128.0\n",
            "Eval_AverageEpLen : 156.0\n",
            "Train_AverageReturn : 114.33333587646484\n",
            "Train_StdReturn : 49.51543045043945\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 61.0\n",
            "Train_AverageEpLen : 114.33333333333333\n",
            "Train_EnvstepsSoFar : 42224\n",
            "TimeSinceStart : 63.63339972496033\n",
            "Training Loss : 51469.09765625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.0\n",
            "Eval_StdReturn : 15.620499610900879\n",
            "Eval_MaxReturn : 90.0\n",
            "Eval_MinReturn : 46.0\n",
            "Eval_AverageEpLen : 67.0\n",
            "Train_AverageReturn : 91.2727279663086\n",
            "Train_StdReturn : 27.001989364624023\n",
            "Train_MaxReturn : 141.0\n",
            "Train_MinReturn : 51.0\n",
            "Train_AverageEpLen : 91.27272727272727\n",
            "Train_EnvstepsSoFar : 43228\n",
            "TimeSinceStart : 65.15559530258179\n",
            "Training Loss : 38675.30078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 83.4000015258789\n",
            "Eval_StdReturn : 22.240503311157227\n",
            "Eval_MaxReturn : 105.0\n",
            "Eval_MinReturn : 52.0\n",
            "Eval_AverageEpLen : 83.4\n",
            "Train_AverageReturn : 85.75\n",
            "Train_StdReturn : 26.089988708496094\n",
            "Train_MaxReturn : 150.0\n",
            "Train_MinReturn : 53.0\n",
            "Train_AverageEpLen : 85.75\n",
            "Train_EnvstepsSoFar : 44257\n",
            "TimeSinceStart : 66.6373987197876\n",
            "Training Loss : 36064.9765625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 60.57143020629883\n",
            "Eval_StdReturn : 17.990928649902344\n",
            "Eval_MaxReturn : 103.0\n",
            "Eval_MinReturn : 47.0\n",
            "Eval_AverageEpLen : 60.57142857142857\n",
            "Train_AverageReturn : 85.41666412353516\n",
            "Train_StdReturn : 29.1732120513916\n",
            "Train_MaxReturn : 166.0\n",
            "Train_MinReturn : 48.0\n",
            "Train_AverageEpLen : 85.41666666666667\n",
            "Train_EnvstepsSoFar : 45282\n",
            "TimeSinceStart : 68.10505294799805\n",
            "Training Loss : 37104.0234375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 75.5\n",
            "Eval_StdReturn : 28.871843338012695\n",
            "Eval_MaxReturn : 139.0\n",
            "Eval_MinReturn : 59.0\n",
            "Eval_AverageEpLen : 75.5\n",
            "Train_AverageReturn : 77.0\n",
            "Train_StdReturn : 34.86457443237305\n",
            "Train_MaxReturn : 186.0\n",
            "Train_MinReturn : 43.0\n",
            "Train_AverageEpLen : 77.0\n",
            "Train_EnvstepsSoFar : 46283\n",
            "TimeSinceStart : 69.57561373710632\n",
            "Training Loss : 34063.05078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 61.0\n",
            "Eval_StdReturn : 11.364103317260742\n",
            "Eval_MaxReturn : 75.0\n",
            "Eval_MinReturn : 41.0\n",
            "Eval_AverageEpLen : 61.0\n",
            "Train_AverageReturn : 74.64286041259766\n",
            "Train_StdReturn : 24.426586151123047\n",
            "Train_MaxReturn : 127.0\n",
            "Train_MinReturn : 46.0\n",
            "Train_AverageEpLen : 74.64285714285714\n",
            "Train_EnvstepsSoFar : 47328\n",
            "TimeSinceStart : 71.07812976837158\n",
            "Training Loss : 29440.87109375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 63.57143020629883\n",
            "Eval_StdReturn : 15.728565216064453\n",
            "Eval_MaxReturn : 99.0\n",
            "Eval_MinReturn : 51.0\n",
            "Eval_AverageEpLen : 63.57142857142857\n",
            "Train_AverageReturn : 79.14286041259766\n",
            "Train_StdReturn : 20.914304733276367\n",
            "Train_MaxReturn : 133.0\n",
            "Train_MinReturn : 51.0\n",
            "Train_AverageEpLen : 79.14285714285714\n",
            "Train_EnvstepsSoFar : 48436\n",
            "TimeSinceStart : 72.6823194026947\n",
            "Training Loss : 29792.16796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 92.19999694824219\n",
            "Eval_StdReturn : 26.693819046020508\n",
            "Eval_MaxReturn : 128.0\n",
            "Eval_MinReturn : 67.0\n",
            "Eval_AverageEpLen : 92.2\n",
            "Train_AverageReturn : 83.41666412353516\n",
            "Train_StdReturn : 29.739028930664062\n",
            "Train_MaxReturn : 159.0\n",
            "Train_MinReturn : 47.0\n",
            "Train_AverageEpLen : 83.41666666666667\n",
            "Train_EnvstepsSoFar : 49437\n",
            "TimeSinceStart : 74.18772387504578\n",
            "Training Loss : 31763.5234375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 115.25\n",
            "Eval_StdReturn : 40.499229431152344\n",
            "Eval_MaxReturn : 159.0\n",
            "Eval_MinReturn : 59.0\n",
            "Eval_AverageEpLen : 115.25\n",
            "Train_AverageReturn : 76.0\n",
            "Train_StdReturn : 29.51512908935547\n",
            "Train_MaxReturn : 175.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 76.0\n",
            "Train_EnvstepsSoFar : 50501\n",
            "TimeSinceStart : 75.73212504386902\n",
            "Training Loss : 30211.6796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 114.75\n",
            "Eval_StdReturn : 32.321624755859375\n",
            "Eval_MaxReturn : 164.0\n",
            "Eval_MinReturn : 82.0\n",
            "Eval_AverageEpLen : 114.75\n",
            "Train_AverageReturn : 85.16666412353516\n",
            "Train_StdReturn : 20.260112762451172\n",
            "Train_MaxReturn : 129.0\n",
            "Train_MinReturn : 60.0\n",
            "Train_AverageEpLen : 85.16666666666667\n",
            "Train_EnvstepsSoFar : 51523\n",
            "TimeSinceStart : 77.30703377723694\n",
            "Training Loss : 29879.25\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 71.16666412353516\n",
            "Eval_StdReturn : 12.615026473999023\n",
            "Eval_MaxReturn : 92.0\n",
            "Eval_MinReturn : 55.0\n",
            "Eval_AverageEpLen : 71.16666666666667\n",
            "Train_AverageReturn : 82.69230651855469\n",
            "Train_StdReturn : 29.50719451904297\n",
            "Train_MaxReturn : 162.0\n",
            "Train_MinReturn : 57.0\n",
            "Train_AverageEpLen : 82.6923076923077\n",
            "Train_EnvstepsSoFar : 52598\n",
            "TimeSinceStart : 78.80981183052063\n",
            "Training Loss : 29206.333984375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 116.0\n",
            "Eval_StdReturn : 22.315914154052734\n",
            "Eval_MaxReturn : 142.0\n",
            "Eval_MinReturn : 82.0\n",
            "Eval_AverageEpLen : 116.0\n",
            "Train_AverageReturn : 100.81818389892578\n",
            "Train_StdReturn : 31.45796012878418\n",
            "Train_MaxReturn : 167.0\n",
            "Train_MinReturn : 65.0\n",
            "Train_AverageEpLen : 100.81818181818181\n",
            "Train_EnvstepsSoFar : 53707\n",
            "TimeSinceStart : 80.42752838134766\n",
            "Training Loss : 39200.76953125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 101.75\n",
            "Eval_StdReturn : 27.00347137451172\n",
            "Eval_MaxReturn : 130.0\n",
            "Eval_MinReturn : 58.0\n",
            "Eval_AverageEpLen : 101.75\n",
            "Train_AverageReturn : 125.75\n",
            "Train_StdReturn : 35.070465087890625\n",
            "Train_MaxReturn : 182.0\n",
            "Train_MinReturn : 80.0\n",
            "Train_AverageEpLen : 125.75\n",
            "Train_EnvstepsSoFar : 54713\n",
            "TimeSinceStart : 81.88776063919067\n",
            "Training Loss : 44263.92578125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 123.25\n",
            "Eval_StdReturn : 10.009370803833008\n",
            "Eval_MaxReturn : 132.0\n",
            "Eval_MinReturn : 107.0\n",
            "Eval_AverageEpLen : 123.25\n",
            "Train_AverageReturn : 100.0999984741211\n",
            "Train_StdReturn : 24.097509384155273\n",
            "Train_MaxReturn : 154.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 100.1\n",
            "Train_EnvstepsSoFar : 55714\n",
            "TimeSinceStart : 83.39798593521118\n",
            "Training Loss : 30505.16796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 109.5\n",
            "Eval_StdReturn : 22.028390884399414\n",
            "Eval_MaxReturn : 132.0\n",
            "Eval_MinReturn : 73.0\n",
            "Eval_AverageEpLen : 109.5\n",
            "Train_AverageReturn : 96.0\n",
            "Train_StdReturn : 17.135820388793945\n",
            "Train_MaxReturn : 125.0\n",
            "Train_MinReturn : 68.0\n",
            "Train_AverageEpLen : 96.0\n",
            "Train_EnvstepsSoFar : 56770\n",
            "TimeSinceStart : 84.96865701675415\n",
            "Training Loss : 30808.68359375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 92.4000015258789\n",
            "Eval_StdReturn : 8.685620307922363\n",
            "Eval_MaxReturn : 106.0\n",
            "Eval_MinReturn : 80.0\n",
            "Eval_AverageEpLen : 92.4\n",
            "Train_AverageReturn : 112.55555725097656\n",
            "Train_StdReturn : 39.72715377807617\n",
            "Train_MaxReturn : 179.0\n",
            "Train_MinReturn : 72.0\n",
            "Train_AverageEpLen : 112.55555555555556\n",
            "Train_EnvstepsSoFar : 57783\n",
            "TimeSinceStart : 86.5084936618805\n",
            "Training Loss : 40575.5\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 141.3333282470703\n",
            "Eval_StdReturn : 35.93821716308594\n",
            "Eval_MaxReturn : 186.0\n",
            "Eval_MinReturn : 98.0\n",
            "Eval_AverageEpLen : 141.33333333333334\n",
            "Train_AverageReturn : 111.44444274902344\n",
            "Train_StdReturn : 28.07177734375\n",
            "Train_MaxReturn : 150.0\n",
            "Train_MinReturn : 72.0\n",
            "Train_AverageEpLen : 111.44444444444444\n",
            "Train_EnvstepsSoFar : 58786\n",
            "TimeSinceStart : 87.97345995903015\n",
            "Training Loss : 34240.72265625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 143.0\n",
            "Eval_StdReturn : 13.490737915039062\n",
            "Eval_MaxReturn : 159.0\n",
            "Eval_MinReturn : 126.0\n",
            "Eval_AverageEpLen : 143.0\n",
            "Train_AverageReturn : 108.19999694824219\n",
            "Train_StdReturn : 21.479291915893555\n",
            "Train_MaxReturn : 145.0\n",
            "Train_MinReturn : 74.0\n",
            "Train_AverageEpLen : 108.2\n",
            "Train_EnvstepsSoFar : 59868\n",
            "TimeSinceStart : 89.52618455886841\n",
            "Training Loss : 33661.2421875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 100.25\n",
            "Eval_StdReturn : 28.19906997680664\n",
            "Eval_MaxReturn : 143.0\n",
            "Eval_MinReturn : 72.0\n",
            "Eval_AverageEpLen : 100.25\n",
            "Train_AverageReturn : 95.0\n",
            "Train_StdReturn : 31.614151000976562\n",
            "Train_MaxReturn : 167.0\n",
            "Train_MinReturn : 54.0\n",
            "Train_AverageEpLen : 95.0\n",
            "Train_EnvstepsSoFar : 60913\n",
            "TimeSinceStart : 91.00353026390076\n",
            "Training Loss : 33620.6640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 89.0\n",
            "Eval_StdReturn : 8.508818626403809\n",
            "Eval_MaxReturn : 103.0\n",
            "Eval_MinReturn : 79.0\n",
            "Eval_AverageEpLen : 89.0\n",
            "Train_AverageReturn : 89.5\n",
            "Train_StdReturn : 22.518510818481445\n",
            "Train_MaxReturn : 133.0\n",
            "Train_MinReturn : 61.0\n",
            "Train_AverageEpLen : 89.5\n",
            "Train_EnvstepsSoFar : 61987\n",
            "TimeSinceStart : 92.58937644958496\n",
            "Training Loss : 27648.66796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 80.16666412353516\n",
            "Eval_StdReturn : 9.990272521972656\n",
            "Eval_MaxReturn : 94.0\n",
            "Eval_MinReturn : 68.0\n",
            "Eval_AverageEpLen : 80.16666666666667\n",
            "Train_AverageReturn : 103.54545593261719\n",
            "Train_StdReturn : 34.989253997802734\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 63.0\n",
            "Train_AverageEpLen : 103.54545454545455\n",
            "Train_EnvstepsSoFar : 63126\n",
            "TimeSinceStart : 94.29084897041321\n",
            "Training Loss : 32528.248046875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 104.0\n",
            "Eval_StdReturn : 21.250883102416992\n",
            "Eval_MaxReturn : 135.0\n",
            "Eval_MinReturn : 78.0\n",
            "Eval_AverageEpLen : 104.0\n",
            "Train_AverageReturn : 100.9000015258789\n",
            "Train_StdReturn : 33.901180267333984\n",
            "Train_MaxReturn : 170.0\n",
            "Train_MinReturn : 60.0\n",
            "Train_AverageEpLen : 100.9\n",
            "Train_EnvstepsSoFar : 64135\n",
            "TimeSinceStart : 95.85725522041321\n",
            "Training Loss : 30457.171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 107.5\n",
            "Eval_StdReturn : 26.34862518310547\n",
            "Eval_MaxReturn : 137.0\n",
            "Eval_MinReturn : 66.0\n",
            "Eval_AverageEpLen : 107.5\n",
            "Train_AverageReturn : 109.4000015258789\n",
            "Train_StdReturn : 34.18537521362305\n",
            "Train_MaxReturn : 175.0\n",
            "Train_MinReturn : 68.0\n",
            "Train_AverageEpLen : 109.4\n",
            "Train_EnvstepsSoFar : 65229\n",
            "TimeSinceStart : 97.42751455307007\n",
            "Training Loss : 35601.66796875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 123.0\n",
            "Eval_StdReturn : 41.176448822021484\n",
            "Eval_MaxReturn : 189.0\n",
            "Eval_MinReturn : 80.0\n",
            "Eval_AverageEpLen : 123.0\n",
            "Train_AverageReturn : 108.5999984741211\n",
            "Train_StdReturn : 32.63801574707031\n",
            "Train_MaxReturn : 181.0\n",
            "Train_MinReturn : 74.0\n",
            "Train_AverageEpLen : 108.6\n",
            "Train_EnvstepsSoFar : 66315\n",
            "TimeSinceStart : 99.01613759994507\n",
            "Training Loss : 35532.703125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 114.25\n",
            "Eval_StdReturn : 11.211043357849121\n",
            "Eval_MaxReturn : 123.0\n",
            "Eval_MinReturn : 95.0\n",
            "Eval_AverageEpLen : 114.25\n",
            "Train_AverageReturn : 116.11111450195312\n",
            "Train_StdReturn : 24.98641586303711\n",
            "Train_MaxReturn : 173.0\n",
            "Train_MinReturn : 84.0\n",
            "Train_AverageEpLen : 116.11111111111111\n",
            "Train_EnvstepsSoFar : 67360\n",
            "TimeSinceStart : 100.56693720817566\n",
            "Training Loss : 40594.05078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 101.75\n",
            "Eval_StdReturn : 27.634897232055664\n",
            "Eval_MaxReturn : 148.0\n",
            "Eval_MinReturn : 77.0\n",
            "Eval_AverageEpLen : 101.75\n",
            "Train_AverageReturn : 109.5\n",
            "Train_StdReturn : 24.699190139770508\n",
            "Train_MaxReturn : 157.0\n",
            "Train_MinReturn : 77.0\n",
            "Train_AverageEpLen : 109.5\n",
            "Train_EnvstepsSoFar : 68455\n",
            "TimeSinceStart : 102.1241843700409\n",
            "Training Loss : 30045.8515625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 132.25\n",
            "Eval_StdReturn : 32.452850341796875\n",
            "Eval_MaxReturn : 182.0\n",
            "Eval_MinReturn : 91.0\n",
            "Eval_AverageEpLen : 132.25\n",
            "Train_AverageReturn : 122.11111450195312\n",
            "Train_StdReturn : 31.409284591674805\n",
            "Train_MaxReturn : 178.0\n",
            "Train_MinReturn : 85.0\n",
            "Train_AverageEpLen : 122.11111111111111\n",
            "Train_EnvstepsSoFar : 69554\n",
            "TimeSinceStart : 103.79762268066406\n",
            "Training Loss : 36681.58984375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 84.80000305175781\n",
            "Eval_StdReturn : 22.73675537109375\n",
            "Eval_MaxReturn : 117.0\n",
            "Eval_MinReturn : 59.0\n",
            "Eval_AverageEpLen : 84.8\n",
            "Train_AverageReturn : 131.625\n",
            "Train_StdReturn : 42.517459869384766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 81.0\n",
            "Train_AverageEpLen : 131.625\n",
            "Train_EnvstepsSoFar : 70607\n",
            "TimeSinceStart : 105.33971929550171\n",
            "Training Loss : 39372.9375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 116.0\n",
            "Eval_StdReturn : 29.52117919921875\n",
            "Eval_MaxReturn : 165.0\n",
            "Eval_MinReturn : 86.0\n",
            "Eval_AverageEpLen : 116.0\n",
            "Train_AverageReturn : 117.88888549804688\n",
            "Train_StdReturn : 38.9770393371582\n",
            "Train_MaxReturn : 194.0\n",
            "Train_MinReturn : 77.0\n",
            "Train_AverageEpLen : 117.88888888888889\n",
            "Train_EnvstepsSoFar : 71668\n",
            "TimeSinceStart : 106.91438317298889\n",
            "Training Loss : 37225.078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 116.25\n",
            "Eval_StdReturn : 13.348689079284668\n",
            "Eval_MaxReturn : 136.0\n",
            "Eval_MinReturn : 104.0\n",
            "Eval_AverageEpLen : 116.25\n",
            "Train_AverageReturn : 113.66666412353516\n",
            "Train_StdReturn : 37.34523391723633\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 81.0\n",
            "Train_AverageEpLen : 113.66666666666667\n",
            "Train_EnvstepsSoFar : 72691\n",
            "TimeSinceStart : 108.4263391494751\n",
            "Training Loss : 31246.77734375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 106.0\n",
            "Eval_StdReturn : 13.964240074157715\n",
            "Eval_MaxReturn : 129.0\n",
            "Eval_MinReturn : 93.0\n",
            "Eval_AverageEpLen : 106.0\n",
            "Train_AverageReturn : 109.0\n",
            "Train_StdReturn : 18.957847595214844\n",
            "Train_MaxReturn : 143.0\n",
            "Train_MinReturn : 87.0\n",
            "Train_AverageEpLen : 109.0\n",
            "Train_EnvstepsSoFar : 73781\n",
            "TimeSinceStart : 110.02022671699524\n",
            "Training Loss : 33570.17578125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 138.6666717529297\n",
            "Eval_StdReturn : 29.533409118652344\n",
            "Eval_MaxReturn : 162.0\n",
            "Eval_MinReturn : 97.0\n",
            "Eval_AverageEpLen : 138.66666666666666\n",
            "Train_AverageReturn : 129.875\n",
            "Train_StdReturn : 42.630496978759766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 80.0\n",
            "Train_AverageEpLen : 129.875\n",
            "Train_EnvstepsSoFar : 74820\n",
            "TimeSinceStart : 111.55630421638489\n",
            "Training Loss : 43896.31640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 115.75\n",
            "Eval_StdReturn : 4.710361003875732\n",
            "Eval_MaxReturn : 123.0\n",
            "Eval_MinReturn : 110.0\n",
            "Eval_AverageEpLen : 115.75\n",
            "Train_AverageReturn : 118.55555725097656\n",
            "Train_StdReturn : 33.552978515625\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 118.55555555555556\n",
            "Train_EnvstepsSoFar : 75887\n",
            "TimeSinceStart : 113.13336157798767\n",
            "Training Loss : 33990.9921875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 104.5\n",
            "Eval_StdReturn : 18.241436004638672\n",
            "Eval_MaxReturn : 131.0\n",
            "Eval_MinReturn : 81.0\n",
            "Eval_AverageEpLen : 104.5\n",
            "Train_AverageReturn : 119.0\n",
            "Train_StdReturn : 33.536048889160156\n",
            "Train_MaxReturn : 177.0\n",
            "Train_MinReturn : 62.0\n",
            "Train_AverageEpLen : 119.0\n",
            "Train_EnvstepsSoFar : 76958\n",
            "TimeSinceStart : 114.67201280593872\n",
            "Training Loss : 36444.30078125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 102.0\n",
            "Eval_StdReturn : 7.5828752517700195\n",
            "Eval_MaxReturn : 115.0\n",
            "Eval_MinReturn : 96.0\n",
            "Eval_AverageEpLen : 102.0\n",
            "Train_AverageReturn : 125.375\n",
            "Train_StdReturn : 29.042802810668945\n",
            "Train_MaxReturn : 187.0\n",
            "Train_MinReturn : 92.0\n",
            "Train_AverageEpLen : 125.375\n",
            "Train_EnvstepsSoFar : 77961\n",
            "TimeSinceStart : 116.12470746040344\n",
            "Training Loss : 32125.142578125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 102.0\n",
            "Eval_StdReturn : 14.230249404907227\n",
            "Eval_MaxReturn : 119.0\n",
            "Eval_MinReturn : 86.0\n",
            "Eval_AverageEpLen : 102.0\n",
            "Train_AverageReturn : 104.80000305175781\n",
            "Train_StdReturn : 13.658697128295898\n",
            "Train_MaxReturn : 134.0\n",
            "Train_MinReturn : 87.0\n",
            "Train_AverageEpLen : 104.8\n",
            "Train_EnvstepsSoFar : 79009\n",
            "TimeSinceStart : 117.63735699653625\n",
            "Training Loss : 27848.38671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 76.33333587646484\n",
            "Eval_StdReturn : 5.617433547973633\n",
            "Eval_MaxReturn : 83.0\n",
            "Eval_MinReturn : 69.0\n",
            "Eval_AverageEpLen : 76.33333333333333\n",
            "Train_AverageReturn : 98.18181610107422\n",
            "Train_StdReturn : 23.976572036743164\n",
            "Train_MaxReturn : 144.0\n",
            "Train_MinReturn : 64.0\n",
            "Train_AverageEpLen : 98.18181818181819\n",
            "Train_EnvstepsSoFar : 80089\n",
            "TimeSinceStart : 119.2329306602478\n",
            "Training Loss : 28167.673828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 86.0\n",
            "Eval_StdReturn : 13.145341873168945\n",
            "Eval_MaxReturn : 111.0\n",
            "Eval_MinReturn : 72.0\n",
            "Eval_AverageEpLen : 86.0\n",
            "Train_AverageReturn : 89.08333587646484\n",
            "Train_StdReturn : 15.702750205993652\n",
            "Train_MaxReturn : 111.0\n",
            "Train_MinReturn : 65.0\n",
            "Train_AverageEpLen : 89.08333333333333\n",
            "Train_EnvstepsSoFar : 81158\n",
            "TimeSinceStart : 120.758131980896\n",
            "Training Loss : 25720.181640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 81.16666412353516\n",
            "Eval_StdReturn : 11.393223762512207\n",
            "Eval_MaxReturn : 98.0\n",
            "Eval_MinReturn : 68.0\n",
            "Eval_AverageEpLen : 81.16666666666667\n",
            "Train_AverageReturn : 91.81818389892578\n",
            "Train_StdReturn : 13.415175437927246\n",
            "Train_MaxReturn : 114.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 91.81818181818181\n",
            "Train_EnvstepsSoFar : 82168\n",
            "TimeSinceStart : 122.30832147598267\n",
            "Training Loss : 24493.56640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 81.19999694824219\n",
            "Eval_StdReturn : 18.214279174804688\n",
            "Eval_MaxReturn : 111.0\n",
            "Eval_MinReturn : 56.0\n",
            "Eval_AverageEpLen : 81.2\n",
            "Train_AverageReturn : 89.08333587646484\n",
            "Train_StdReturn : 10.742892265319824\n",
            "Train_MaxReturn : 103.0\n",
            "Train_MinReturn : 67.0\n",
            "Train_AverageEpLen : 89.08333333333333\n",
            "Train_EnvstepsSoFar : 83237\n",
            "TimeSinceStart : 123.85911655426025\n",
            "Training Loss : 22153.42578125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 90.4000015258789\n",
            "Eval_StdReturn : 25.819372177124023\n",
            "Eval_MaxReturn : 116.0\n",
            "Eval_MinReturn : 50.0\n",
            "Eval_AverageEpLen : 90.4\n",
            "Train_AverageReturn : 97.45454406738281\n",
            "Train_StdReturn : 16.66440773010254\n",
            "Train_MaxReturn : 131.0\n",
            "Train_MinReturn : 74.0\n",
            "Train_AverageEpLen : 97.45454545454545\n",
            "Train_EnvstepsSoFar : 84309\n",
            "TimeSinceStart : 125.46178460121155\n",
            "Training Loss : 29586.88671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 111.0\n",
            "Eval_StdReturn : 21.610183715820312\n",
            "Eval_MaxReturn : 144.0\n",
            "Eval_MinReturn : 84.0\n",
            "Eval_AverageEpLen : 111.0\n",
            "Train_AverageReturn : 100.5999984741211\n",
            "Train_StdReturn : 19.226022720336914\n",
            "Train_MaxReturn : 138.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 100.6\n",
            "Train_EnvstepsSoFar : 85315\n",
            "TimeSinceStart : 126.94546270370483\n",
            "Training Loss : 27407.228515625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 124.25\n",
            "Eval_StdReturn : 17.397916793823242\n",
            "Eval_MaxReturn : 153.0\n",
            "Eval_MinReturn : 109.0\n",
            "Eval_AverageEpLen : 124.25\n",
            "Train_AverageReturn : 122.66666412353516\n",
            "Train_StdReturn : 11.105554580688477\n",
            "Train_MaxReturn : 139.0\n",
            "Train_MinReturn : 103.0\n",
            "Train_AverageEpLen : 122.66666666666667\n",
            "Train_EnvstepsSoFar : 86419\n",
            "TimeSinceStart : 128.63631129264832\n",
            "Training Loss : 35272.87109375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 118.75\n",
            "Eval_StdReturn : 13.479150772094727\n",
            "Eval_MaxReturn : 142.0\n",
            "Eval_MinReturn : 109.0\n",
            "Eval_AverageEpLen : 118.75\n",
            "Train_AverageReturn : 125.875\n",
            "Train_StdReturn : 11.645143508911133\n",
            "Train_MaxReturn : 146.0\n",
            "Train_MinReturn : 105.0\n",
            "Train_AverageEpLen : 125.875\n",
            "Train_EnvstepsSoFar : 87426\n",
            "TimeSinceStart : 130.18591380119324\n",
            "Training Loss : 33784.7109375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 133.0\n",
            "Eval_StdReturn : 2.5495097637176514\n",
            "Eval_MaxReturn : 136.0\n",
            "Eval_MinReturn : 129.0\n",
            "Eval_AverageEpLen : 133.0\n",
            "Train_AverageReturn : 124.44444274902344\n",
            "Train_StdReturn : 8.65526294708252\n",
            "Train_MaxReturn : 136.0\n",
            "Train_MinReturn : 109.0\n",
            "Train_AverageEpLen : 124.44444444444444\n",
            "Train_EnvstepsSoFar : 88546\n",
            "TimeSinceStart : 131.940571308136\n",
            "Training Loss : 37183.46875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 122.25\n",
            "Eval_StdReturn : 9.756407737731934\n",
            "Eval_MaxReturn : 132.0\n",
            "Eval_MinReturn : 112.0\n",
            "Eval_AverageEpLen : 122.25\n",
            "Train_AverageReturn : 125.22222137451172\n",
            "Train_StdReturn : 10.809232711791992\n",
            "Train_MaxReturn : 140.0\n",
            "Train_MinReturn : 113.0\n",
            "Train_AverageEpLen : 125.22222222222223\n",
            "Train_EnvstepsSoFar : 89673\n",
            "TimeSinceStart : 133.5872986316681\n",
            "Training Loss : 35397.828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 133.25\n",
            "Eval_StdReturn : 7.395099639892578\n",
            "Eval_MaxReturn : 145.0\n",
            "Eval_MinReturn : 126.0\n",
            "Eval_AverageEpLen : 133.25\n",
            "Train_AverageReturn : 138.625\n",
            "Train_StdReturn : 21.153825759887695\n",
            "Train_MaxReturn : 168.0\n",
            "Train_MinReturn : 114.0\n",
            "Train_AverageEpLen : 138.625\n",
            "Train_EnvstepsSoFar : 90782\n",
            "TimeSinceStart : 135.26157069206238\n",
            "Training Loss : 38330.4296875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 149.0\n",
            "Eval_StdReturn : 14.514360427856445\n",
            "Eval_MaxReturn : 169.0\n",
            "Eval_MinReturn : 135.0\n",
            "Eval_AverageEpLen : 149.0\n",
            "Train_AverageReturn : 138.5\n",
            "Train_StdReturn : 14.645818710327148\n",
            "Train_MaxReturn : 166.0\n",
            "Train_MinReturn : 117.0\n",
            "Train_AverageEpLen : 138.5\n",
            "Train_EnvstepsSoFar : 91890\n",
            "TimeSinceStart : 136.84650254249573\n",
            "Training Loss : 40728.203125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 147.0\n",
            "Eval_StdReturn : 11.860298156738281\n",
            "Eval_MaxReturn : 161.0\n",
            "Eval_MinReturn : 132.0\n",
            "Eval_AverageEpLen : 147.0\n",
            "Train_AverageReturn : 147.375\n",
            "Train_StdReturn : 16.40074348449707\n",
            "Train_MaxReturn : 182.0\n",
            "Train_MinReturn : 129.0\n",
            "Train_AverageEpLen : 147.375\n",
            "Train_EnvstepsSoFar : 93069\n",
            "TimeSinceStart : 138.47960758209229\n",
            "Training Loss : 39036.40234375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 156.3333282470703\n",
            "Eval_StdReturn : 6.599663257598877\n",
            "Eval_MaxReturn : 165.0\n",
            "Eval_MinReturn : 149.0\n",
            "Eval_AverageEpLen : 156.33333333333334\n",
            "Train_AverageReturn : 167.85714721679688\n",
            "Train_StdReturn : 18.349218368530273\n",
            "Train_MaxReturn : 189.0\n",
            "Train_MinReturn : 137.0\n",
            "Train_AverageEpLen : 167.85714285714286\n",
            "Train_EnvstepsSoFar : 94244\n",
            "TimeSinceStart : 140.18725490570068\n",
            "Training Loss : 41615.5\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 171.6666717529297\n",
            "Train_StdReturn : 21.36716079711914\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 137.0\n",
            "Train_AverageEpLen : 171.66666666666666\n",
            "Train_EnvstepsSoFar : 95274\n",
            "TimeSinceStart : 141.65063667297363\n",
            "Training Loss : 53977.8359375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 167.5\n",
            "Train_StdReturn : 14.4769926071167\n",
            "Train_MaxReturn : 193.0\n",
            "Train_MinReturn : 150.0\n",
            "Train_AverageEpLen : 167.5\n",
            "Train_EnvstepsSoFar : 96279\n",
            "TimeSinceStart : 143.10435318946838\n",
            "Training Loss : 46098.63671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 97279\n",
            "TimeSinceStart : 144.55838751792908\n",
            "Training Loss : 48698.1328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 98279\n",
            "TimeSinceStart : 146.01002740859985\n",
            "Training Loss : 46992.765625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 99279\n",
            "TimeSinceStart : 147.48020339012146\n",
            "Training Loss : 55944.07421875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 100279\n",
            "TimeSinceStart : 148.91721773147583\n",
            "Training Loss : 52373.6171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 101279\n",
            "TimeSinceStart : 150.36960744857788\n",
            "Training Loss : 54789.34765625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 102279\n",
            "TimeSinceStart : 151.80850791931152\n",
            "Training Loss : 52195.921875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 175.6666717529297\n",
            "Eval_StdReturn : 15.173075675964355\n",
            "Eval_MaxReturn : 197.0\n",
            "Eval_MinReturn : 163.0\n",
            "Eval_AverageEpLen : 175.66666666666666\n",
            "Train_AverageReturn : 195.8333282470703\n",
            "Train_StdReturn : 9.316949844360352\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 175.0\n",
            "Train_AverageEpLen : 195.83333333333334\n",
            "Train_EnvstepsSoFar : 103454\n",
            "TimeSinceStart : 153.56246280670166\n",
            "Training Loss : 67411.0859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 133.3333282470703\n",
            "Eval_StdReturn : 8.653837203979492\n",
            "Eval_MaxReturn : 143.0\n",
            "Eval_MinReturn : 122.0\n",
            "Eval_AverageEpLen : 133.33333333333334\n",
            "Train_AverageReturn : 154.57142639160156\n",
            "Train_StdReturn : 9.7666654586792\n",
            "Train_MaxReturn : 170.0\n",
            "Train_MinReturn : 141.0\n",
            "Train_AverageEpLen : 154.57142857142858\n",
            "Train_EnvstepsSoFar : 104536\n",
            "TimeSinceStart : 155.1153702735901\n",
            "Training Loss : 47959.953125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 120.75\n",
            "Eval_StdReturn : 3.3447721004486084\n",
            "Eval_MaxReturn : 123.0\n",
            "Eval_MinReturn : 115.0\n",
            "Eval_AverageEpLen : 120.75\n",
            "Train_AverageReturn : 125.25\n",
            "Train_StdReturn : 8.584142684936523\n",
            "Train_MaxReturn : 140.0\n",
            "Train_MinReturn : 114.0\n",
            "Train_AverageEpLen : 125.25\n",
            "Train_EnvstepsSoFar : 105538\n",
            "TimeSinceStart : 156.61593580245972\n",
            "Training Loss : 34411.1171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 1000 \\\n",
        "-rtg --exp_name q1_sb_rtg_na"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nikg_nQmY9c",
        "outputId": "3d7a63bc-71eb-4010-faab-3080429a3f38"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q1_sb_rtg_na_CartPole-v0_04-02-2022_16-43-37\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 28.733333587646484\n",
            "Eval_StdReturn : 14.401233673095703\n",
            "Eval_MaxReturn : 67.0\n",
            "Eval_MinReturn : 13.0\n",
            "Eval_AverageEpLen : 28.733333333333334\n",
            "Train_AverageReturn : 27.648649215698242\n",
            "Train_StdReturn : 12.643160820007324\n",
            "Train_MaxReturn : 65.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 27.64864864864865\n",
            "Train_EnvstepsSoFar : 1023\n",
            "TimeSinceStart : 1.370870590209961\n",
            "Training Loss : -3.9271583557128906\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 37.181819915771484\n",
            "Eval_StdReturn : 16.878271102905273\n",
            "Eval_MaxReturn : 80.0\n",
            "Eval_MinReturn : 19.0\n",
            "Eval_AverageEpLen : 37.18181818181818\n",
            "Train_AverageReturn : 28.30555534362793\n",
            "Train_StdReturn : 15.314879417419434\n",
            "Train_MaxReturn : 79.0\n",
            "Train_MinReturn : 10.0\n",
            "Train_AverageEpLen : 28.305555555555557\n",
            "Train_EnvstepsSoFar : 2042\n",
            "TimeSinceStart : 2.787846326828003\n",
            "Training Loss : -1.3636932373046875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 50.11111068725586\n",
            "Eval_StdReturn : 25.23861312866211\n",
            "Eval_MaxReturn : 91.0\n",
            "Eval_MinReturn : 20.0\n",
            "Eval_AverageEpLen : 50.111111111111114\n",
            "Train_AverageReturn : 35.400001525878906\n",
            "Train_StdReturn : 19.6224365234375\n",
            "Train_MaxReturn : 80.0\n",
            "Train_MinReturn : 12.0\n",
            "Train_AverageEpLen : 35.4\n",
            "Train_EnvstepsSoFar : 3104\n",
            "TimeSinceStart : 4.264931678771973\n",
            "Training Loss : -17.9229793548584\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 36.08333206176758\n",
            "Eval_StdReturn : 14.619953155517578\n",
            "Eval_MaxReturn : 66.0\n",
            "Eval_MinReturn : 13.0\n",
            "Eval_AverageEpLen : 36.083333333333336\n",
            "Train_AverageReturn : 39.846153259277344\n",
            "Train_StdReturn : 9.606456756591797\n",
            "Train_MaxReturn : 60.0\n",
            "Train_MinReturn : 24.0\n",
            "Train_AverageEpLen : 39.84615384615385\n",
            "Train_EnvstepsSoFar : 4140\n",
            "TimeSinceStart : 5.708150863647461\n",
            "Training Loss : 3.777252197265625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 40.900001525878906\n",
            "Eval_StdReturn : 34.209503173828125\n",
            "Eval_MaxReturn : 140.0\n",
            "Eval_MinReturn : 16.0\n",
            "Eval_AverageEpLen : 40.9\n",
            "Train_AverageReturn : 41.70833206176758\n",
            "Train_StdReturn : 24.51271629333496\n",
            "Train_MaxReturn : 137.0\n",
            "Train_MinReturn : 18.0\n",
            "Train_AverageEpLen : 41.708333333333336\n",
            "Train_EnvstepsSoFar : 5141\n",
            "TimeSinceStart : 7.1086413860321045\n",
            "Training Loss : 6.898922920227051\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 41.29999923706055\n",
            "Eval_StdReturn : 14.227087020874023\n",
            "Eval_MaxReturn : 76.0\n",
            "Eval_MinReturn : 28.0\n",
            "Eval_AverageEpLen : 41.3\n",
            "Train_AverageReturn : 43.04166793823242\n",
            "Train_StdReturn : 22.17633819580078\n",
            "Train_MaxReturn : 105.0\n",
            "Train_MinReturn : 18.0\n",
            "Train_AverageEpLen : 43.041666666666664\n",
            "Train_EnvstepsSoFar : 6174\n",
            "TimeSinceStart : 8.553456783294678\n",
            "Training Loss : 0.5188722610473633\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 59.125\n",
            "Eval_StdReturn : 24.962158203125\n",
            "Eval_MaxReturn : 105.0\n",
            "Eval_MinReturn : 21.0\n",
            "Eval_AverageEpLen : 59.125\n",
            "Train_AverageReturn : 39.57692337036133\n",
            "Train_StdReturn : 19.29561996459961\n",
            "Train_MaxReturn : 104.0\n",
            "Train_MinReturn : 21.0\n",
            "Train_AverageEpLen : 39.57692307692308\n",
            "Train_EnvstepsSoFar : 7203\n",
            "TimeSinceStart : 10.031347274780273\n",
            "Training Loss : 3.894865036010742\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 66.57142639160156\n",
            "Eval_StdReturn : 11.043513298034668\n",
            "Eval_MaxReturn : 90.0\n",
            "Eval_MinReturn : 54.0\n",
            "Eval_AverageEpLen : 66.57142857142857\n",
            "Train_AverageReturn : 65.3125\n",
            "Train_StdReturn : 22.05027961730957\n",
            "Train_MaxReturn : 99.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 65.3125\n",
            "Train_EnvstepsSoFar : 8248\n",
            "TimeSinceStart : 11.567851781845093\n",
            "Training Loss : -0.26949119567871094\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 56.25\n",
            "Eval_StdReturn : 20.698732376098633\n",
            "Eval_MaxReturn : 97.0\n",
            "Eval_MinReturn : 34.0\n",
            "Eval_AverageEpLen : 56.25\n",
            "Train_AverageReturn : 69.4000015258789\n",
            "Train_StdReturn : 23.767765045166016\n",
            "Train_MaxReturn : 116.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 69.4\n",
            "Train_EnvstepsSoFar : 9289\n",
            "TimeSinceStart : 13.017677545547485\n",
            "Training Loss : -13.054889678955078\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 47.33333206176758\n",
            "Eval_StdReturn : 12.771495819091797\n",
            "Eval_MaxReturn : 73.0\n",
            "Eval_MinReturn : 32.0\n",
            "Eval_AverageEpLen : 47.333333333333336\n",
            "Train_AverageReturn : 75.07142639160156\n",
            "Train_StdReturn : 26.143733978271484\n",
            "Train_MaxReturn : 127.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 75.07142857142857\n",
            "Train_EnvstepsSoFar : 10340\n",
            "TimeSinceStart : 14.48916482925415\n",
            "Training Loss : 3.9574317932128906\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 51.0\n",
            "Eval_StdReturn : 4.9749369621276855\n",
            "Eval_MaxReturn : 58.0\n",
            "Eval_MinReturn : 41.0\n",
            "Eval_AverageEpLen : 51.0\n",
            "Train_AverageReturn : 51.45000076293945\n",
            "Train_StdReturn : 16.098058700561523\n",
            "Train_MaxReturn : 97.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 51.45\n",
            "Train_EnvstepsSoFar : 11369\n",
            "TimeSinceStart : 15.919778823852539\n",
            "Training Loss : -9.672157287597656\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 51.75\n",
            "Eval_StdReturn : 17.311485290527344\n",
            "Eval_MaxReturn : 78.0\n",
            "Eval_MinReturn : 28.0\n",
            "Eval_AverageEpLen : 51.75\n",
            "Train_AverageReturn : 46.6363639831543\n",
            "Train_StdReturn : 18.675567626953125\n",
            "Train_MaxReturn : 106.0\n",
            "Train_MinReturn : 27.0\n",
            "Train_AverageEpLen : 46.63636363636363\n",
            "Train_EnvstepsSoFar : 12395\n",
            "TimeSinceStart : 17.32117509841919\n",
            "Training Loss : -6.167761325836182\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 74.83333587646484\n",
            "Eval_StdReturn : 39.426795959472656\n",
            "Eval_MaxReturn : 160.0\n",
            "Eval_MinReturn : 39.0\n",
            "Eval_AverageEpLen : 74.83333333333333\n",
            "Train_AverageReturn : 61.38888931274414\n",
            "Train_StdReturn : 27.998292922973633\n",
            "Train_MaxReturn : 126.0\n",
            "Train_MinReturn : 28.0\n",
            "Train_AverageEpLen : 61.388888888888886\n",
            "Train_EnvstepsSoFar : 13500\n",
            "TimeSinceStart : 18.811023235321045\n",
            "Training Loss : -1.9805831909179688\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 57.14285659790039\n",
            "Eval_StdReturn : 26.035297393798828\n",
            "Eval_MaxReturn : 104.0\n",
            "Eval_MinReturn : 30.0\n",
            "Eval_AverageEpLen : 57.142857142857146\n",
            "Train_AverageReturn : 69.53333282470703\n",
            "Train_StdReturn : 39.93639373779297\n",
            "Train_MaxReturn : 183.0\n",
            "Train_MinReturn : 24.0\n",
            "Train_AverageEpLen : 69.53333333333333\n",
            "Train_EnvstepsSoFar : 14543\n",
            "TimeSinceStart : 20.237784147262573\n",
            "Training Loss : 15.10614013671875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 80.19999694824219\n",
            "Eval_StdReturn : 9.195651054382324\n",
            "Eval_MaxReturn : 90.0\n",
            "Eval_MinReturn : 65.0\n",
            "Eval_AverageEpLen : 80.2\n",
            "Train_AverageReturn : 82.53845977783203\n",
            "Train_StdReturn : 44.26774215698242\n",
            "Train_MaxReturn : 188.0\n",
            "Train_MinReturn : 35.0\n",
            "Train_AverageEpLen : 82.53846153846153\n",
            "Train_EnvstepsSoFar : 15616\n",
            "TimeSinceStart : 21.68943190574646\n",
            "Training Loss : -4.1357421875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 76.83333587646484\n",
            "Eval_StdReturn : 25.082639694213867\n",
            "Eval_MaxReturn : 123.0\n",
            "Eval_MinReturn : 47.0\n",
            "Eval_AverageEpLen : 76.83333333333333\n",
            "Train_AverageReturn : 95.54545593261719\n",
            "Train_StdReturn : 27.460264205932617\n",
            "Train_MaxReturn : 143.0\n",
            "Train_MinReturn : 60.0\n",
            "Train_AverageEpLen : 95.54545454545455\n",
            "Train_EnvstepsSoFar : 16667\n",
            "TimeSinceStart : 23.171478509902954\n",
            "Training Loss : -3.827620506286621\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 74.0\n",
            "Eval_StdReturn : 27.300792694091797\n",
            "Eval_MaxReturn : 114.0\n",
            "Eval_MinReturn : 40.0\n",
            "Eval_AverageEpLen : 74.0\n",
            "Train_AverageReturn : 72.64286041259766\n",
            "Train_StdReturn : 18.896739959716797\n",
            "Train_MaxReturn : 110.0\n",
            "Train_MinReturn : 47.0\n",
            "Train_AverageEpLen : 72.64285714285714\n",
            "Train_EnvstepsSoFar : 17684\n",
            "TimeSinceStart : 24.59265899658203\n",
            "Training Loss : -8.80021858215332\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 67.57142639160156\n",
            "Eval_StdReturn : 19.338024139404297\n",
            "Eval_MaxReturn : 100.0\n",
            "Eval_MinReturn : 40.0\n",
            "Eval_AverageEpLen : 67.57142857142857\n",
            "Train_AverageReturn : 71.85713958740234\n",
            "Train_StdReturn : 18.75731086730957\n",
            "Train_MaxReturn : 104.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 71.85714285714286\n",
            "Train_EnvstepsSoFar : 18690\n",
            "TimeSinceStart : 26.01059865951538\n",
            "Training Loss : -8.92135238647461\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 62.42856979370117\n",
            "Eval_StdReturn : 12.860318183898926\n",
            "Eval_MaxReturn : 84.0\n",
            "Eval_MinReturn : 46.0\n",
            "Eval_AverageEpLen : 62.42857142857143\n",
            "Train_AverageReturn : 86.83333587646484\n",
            "Train_StdReturn : 41.18825149536133\n",
            "Train_MaxReturn : 183.0\n",
            "Train_MinReturn : 46.0\n",
            "Train_AverageEpLen : 86.83333333333333\n",
            "Train_EnvstepsSoFar : 19732\n",
            "TimeSinceStart : 27.472663164138794\n",
            "Training Loss : -3.276508331298828\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 100.25\n",
            "Eval_StdReturn : 58.640323638916016\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 57.0\n",
            "Eval_AverageEpLen : 100.25\n",
            "Train_AverageReturn : 100.0\n",
            "Train_StdReturn : 23.74868392944336\n",
            "Train_MaxReturn : 135.0\n",
            "Train_MinReturn : 58.0\n",
            "Train_AverageEpLen : 100.0\n",
            "Train_EnvstepsSoFar : 20732\n",
            "TimeSinceStart : 28.816946268081665\n",
            "Training Loss : -7.134197235107422\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 136.0\n",
            "Eval_StdReturn : 40.82278823852539\n",
            "Eval_MaxReturn : 189.0\n",
            "Eval_MinReturn : 78.0\n",
            "Eval_AverageEpLen : 136.0\n",
            "Train_AverageReturn : 97.0\n",
            "Train_StdReturn : 34.28490447998047\n",
            "Train_MaxReturn : 197.0\n",
            "Train_MinReturn : 56.0\n",
            "Train_AverageEpLen : 97.0\n",
            "Train_EnvstepsSoFar : 21799\n",
            "TimeSinceStart : 30.43562960624695\n",
            "Training Loss : 2.775402069091797\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 113.75\n",
            "Eval_StdReturn : 35.08115768432617\n",
            "Eval_MaxReturn : 173.0\n",
            "Eval_MinReturn : 83.0\n",
            "Eval_AverageEpLen : 113.75\n",
            "Train_AverageReturn : 113.0\n",
            "Train_StdReturn : 43.553287506103516\n",
            "Train_MaxReturn : 193.0\n",
            "Train_MinReturn : 66.0\n",
            "Train_AverageEpLen : 113.0\n",
            "Train_EnvstepsSoFar : 22816\n",
            "TimeSinceStart : 31.82199192047119\n",
            "Training Loss : -1.7644195556640625\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 169.3333282470703\n",
            "Eval_StdReturn : 21.699975967407227\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 153.0\n",
            "Eval_AverageEpLen : 169.33333333333334\n",
            "Train_AverageReturn : 156.0\n",
            "Train_StdReturn : 41.06788635253906\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 82.0\n",
            "Train_AverageEpLen : 156.0\n",
            "Train_EnvstepsSoFar : 23908\n",
            "TimeSinceStart : 33.34596395492554\n",
            "Training Loss : 10.612316131591797\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 143.6666717529297\n",
            "Eval_StdReturn : 47.86323165893555\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 83.0\n",
            "Eval_AverageEpLen : 143.66666666666666\n",
            "Train_AverageReturn : 121.44444274902344\n",
            "Train_StdReturn : 24.85563087463379\n",
            "Train_MaxReturn : 154.0\n",
            "Train_MinReturn : 79.0\n",
            "Train_AverageEpLen : 121.44444444444444\n",
            "Train_EnvstepsSoFar : 25001\n",
            "TimeSinceStart : 34.78277945518494\n",
            "Training Loss : -9.6474609375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 134.0\n",
            "Eval_StdReturn : 9.416297912597656\n",
            "Eval_MaxReturn : 146.0\n",
            "Eval_MinReturn : 123.0\n",
            "Eval_AverageEpLen : 134.0\n",
            "Train_AverageReturn : 118.33333587646484\n",
            "Train_StdReturn : 35.238868713378906\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 69.0\n",
            "Train_AverageEpLen : 118.33333333333333\n",
            "Train_EnvstepsSoFar : 26066\n",
            "TimeSinceStart : 36.19151544570923\n",
            "Training Loss : 11.47061538696289\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 156.6666717529297\n",
            "Eval_StdReturn : 55.72153091430664\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 78.0\n",
            "Eval_AverageEpLen : 156.66666666666666\n",
            "Train_AverageReturn : 156.0\n",
            "Train_StdReturn : 29.237939834594727\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 117.0\n",
            "Train_AverageEpLen : 156.0\n",
            "Train_EnvstepsSoFar : 27158\n",
            "TimeSinceStart : 37.69582676887512\n",
            "Training Loss : 30.609756469726562\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 184.3333282470703\n",
            "Eval_StdReturn : 22.1560115814209\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 153.0\n",
            "Eval_AverageEpLen : 184.33333333333334\n",
            "Train_AverageReturn : 164.2857208251953\n",
            "Train_StdReturn : 33.238162994384766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 109.0\n",
            "Train_AverageEpLen : 164.28571428571428\n",
            "Train_EnvstepsSoFar : 28308\n",
            "TimeSinceStart : 39.31538414955139\n",
            "Training Loss : -14.871077537536621\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 177.0\n",
            "Train_StdReturn : 14.537307739257812\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 161.0\n",
            "Train_AverageEpLen : 177.0\n",
            "Train_EnvstepsSoFar : 29370\n",
            "TimeSinceStart : 40.76681661605835\n",
            "Training Loss : 10.636945724487305\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 191.1666717529297\n",
            "Train_StdReturn : 12.889488220214844\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 168.0\n",
            "Train_AverageEpLen : 191.16666666666666\n",
            "Train_EnvstepsSoFar : 30517\n",
            "TimeSinceStart : 42.25154447555542\n",
            "Training Loss : 2.558868408203125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 31517\n",
            "TimeSinceStart : 43.59251666069031\n",
            "Training Loss : 16.148000717163086\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 32517\n",
            "TimeSinceStart : 44.999074935913086\n",
            "Training Loss : 8.456890106201172\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 33517\n",
            "TimeSinceStart : 46.377644062042236\n",
            "Training Loss : 15.688060760498047\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 34517\n",
            "TimeSinceStart : 47.73702692985535\n",
            "Training Loss : 18.180641174316406\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 35517\n",
            "TimeSinceStart : 49.10505771636963\n",
            "Training Loss : 15.112428665161133\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 36517\n",
            "TimeSinceStart : 50.46058750152588\n",
            "Training Loss : 14.316056251525879\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 37517\n",
            "TimeSinceStart : 51.819793701171875\n",
            "Training Loss : 18.91094398498535\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 38517\n",
            "TimeSinceStart : 53.14222025871277\n",
            "Training Loss : 7.143811225891113\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 39517\n",
            "TimeSinceStart : 54.48259782791138\n",
            "Training Loss : 4.811718940734863\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 40517\n",
            "TimeSinceStart : 55.829440116882324\n",
            "Training Loss : 2.940330982208252\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 41517\n",
            "TimeSinceStart : 57.1932418346405\n",
            "Training Loss : 15.669596672058105\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 42517\n",
            "TimeSinceStart : 58.53454923629761\n",
            "Training Loss : 8.01899242401123\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 43517\n",
            "TimeSinceStart : 59.95645880699158\n",
            "Training Loss : 19.402379989624023\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 44517\n",
            "TimeSinceStart : 61.343953132629395\n",
            "Training Loss : -6.252616882324219\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 193.0\n",
            "Train_StdReturn : 15.65247631072998\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 158.0\n",
            "Train_AverageEpLen : 193.0\n",
            "Train_EnvstepsSoFar : 45675\n",
            "TimeSinceStart : 62.88024854660034\n",
            "Training Loss : -8.911588668823242\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 46675\n",
            "TimeSinceStart : 64.25604605674744\n",
            "Training Loss : -2.0607643127441406\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 47675\n",
            "TimeSinceStart : 65.60697841644287\n",
            "Training Loss : 15.652759552001953\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 48675\n",
            "TimeSinceStart : 66.94408869743347\n",
            "Training Loss : 7.253491401672363\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 49675\n",
            "TimeSinceStart : 68.30272626876831\n",
            "Training Loss : -8.817203521728516\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 50675\n",
            "TimeSinceStart : 69.63359951972961\n",
            "Training Loss : 4.465995788574219\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 51675\n",
            "TimeSinceStart : 71.00460743904114\n",
            "Training Loss : 21.93307876586914\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 52675\n",
            "TimeSinceStart : 72.35399150848389\n",
            "Training Loss : -12.459051132202148\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 53675\n",
            "TimeSinceStart : 73.72001028060913\n",
            "Training Loss : -20.37255859375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 54675\n",
            "TimeSinceStart : 75.07551622390747\n",
            "Training Loss : -24.978042602539062\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 55675\n",
            "TimeSinceStart : 76.47079730033875\n",
            "Training Loss : -21.07861328125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 56675\n",
            "TimeSinceStart : 77.84233808517456\n",
            "Training Loss : -10.389971733093262\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 57675\n",
            "TimeSinceStart : 79.19046425819397\n",
            "Training Loss : -29.14322280883789\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 58675\n",
            "TimeSinceStart : 80.58941102027893\n",
            "Training Loss : -1.1417598724365234\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 189.0\n",
            "Eval_StdReturn : 15.55634880065918\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 167.0\n",
            "Eval_AverageEpLen : 189.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 59675\n",
            "TimeSinceStart : 82.09799098968506\n",
            "Training Loss : -30.906396865844727\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 159.42857360839844\n",
            "Train_StdReturn : 65.87186431884766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 159.42857142857142\n",
            "Train_EnvstepsSoFar : 60791\n",
            "TimeSinceStart : 83.62574458122253\n",
            "Training Loss : -5.67949104309082\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 187.8333282470703\n",
            "Train_StdReturn : 27.205493927001953\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 127.0\n",
            "Train_AverageEpLen : 187.83333333333334\n",
            "Train_EnvstepsSoFar : 61918\n",
            "TimeSinceStart : 85.09844207763672\n",
            "Training Loss : -3.7501237392425537\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 62918\n",
            "TimeSinceStart : 86.49109435081482\n",
            "Training Loss : 0.25782203674316406\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 63918\n",
            "TimeSinceStart : 87.848637342453\n",
            "Training Loss : 2.6721878051757812\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 64918\n",
            "TimeSinceStart : 89.21353936195374\n",
            "Training Loss : -4.798525810241699\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 65918\n",
            "TimeSinceStart : 90.5969021320343\n",
            "Training Loss : -30.57380485534668\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 66918\n",
            "TimeSinceStart : 92.0123119354248\n",
            "Training Loss : -29.145492553710938\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 67918\n",
            "TimeSinceStart : 93.54047155380249\n",
            "Training Loss : -4.739597320556641\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 68918\n",
            "TimeSinceStart : 94.93987584114075\n",
            "Training Loss : -5.456677436828613\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 69918\n",
            "TimeSinceStart : 96.29913425445557\n",
            "Training Loss : -1.3785734176635742\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 70918\n",
            "TimeSinceStart : 97.67863440513611\n",
            "Training Loss : 12.63626480102539\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 71918\n",
            "TimeSinceStart : 99.03550553321838\n",
            "Training Loss : -24.133703231811523\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 72918\n",
            "TimeSinceStart : 100.36845064163208\n",
            "Training Loss : 20.29106903076172\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 73918\n",
            "TimeSinceStart : 101.92555618286133\n",
            "Training Loss : -15.401583671569824\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 74918\n",
            "TimeSinceStart : 103.28639268875122\n",
            "Training Loss : -9.949955940246582\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 75918\n",
            "TimeSinceStart : 104.64576506614685\n",
            "Training Loss : -4.911197662353516\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 76918\n",
            "TimeSinceStart : 106.03958868980408\n",
            "Training Loss : 5.315553665161133\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 77918\n",
            "TimeSinceStart : 107.39360761642456\n",
            "Training Loss : 18.755027770996094\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 78918\n",
            "TimeSinceStart : 108.75749158859253\n",
            "Training Loss : -4.883920192718506\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 79918\n",
            "TimeSinceStart : 110.11620998382568\n",
            "Training Loss : -12.833853721618652\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 80918\n",
            "TimeSinceStart : 111.54616451263428\n",
            "Training Loss : -22.19642448425293\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 81918\n",
            "TimeSinceStart : 112.86733961105347\n",
            "Training Loss : 10.31665325164795\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 82918\n",
            "TimeSinceStart : 114.28779530525208\n",
            "Training Loss : 27.85149383544922\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 83918\n",
            "TimeSinceStart : 115.68877458572388\n",
            "Training Loss : -23.977787017822266\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 84918\n",
            "TimeSinceStart : 117.01939010620117\n",
            "Training Loss : -24.35320281982422\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 85918\n",
            "TimeSinceStart : 118.36765789985657\n",
            "Training Loss : 2.0872135162353516\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 86918\n",
            "TimeSinceStart : 119.75475692749023\n",
            "Training Loss : -24.982194900512695\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 87918\n",
            "TimeSinceStart : 121.10181760787964\n",
            "Training Loss : -26.975908279418945\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 88918\n",
            "TimeSinceStart : 122.46347165107727\n",
            "Training Loss : -27.68096923828125\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 89918\n",
            "TimeSinceStart : 123.80997109413147\n",
            "Training Loss : -30.377281188964844\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.0\n",
            "Train_StdReturn : 2.2360680103302\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 194.0\n",
            "Train_AverageEpLen : 199.0\n",
            "Train_EnvstepsSoFar : 91112\n",
            "TimeSinceStart : 125.59118151664734\n",
            "Training Loss : -32.95043182373047\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 197.0\n",
            "Eval_StdReturn : 2.4494898319244385\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 194.0\n",
            "Eval_AverageEpLen : 197.0\n",
            "Train_AverageReturn : 196.6666717529297\n",
            "Train_StdReturn : 4.85340690612793\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 188.0\n",
            "Train_AverageEpLen : 196.66666666666666\n",
            "Train_EnvstepsSoFar : 92292\n",
            "TimeSinceStart : 127.54367852210999\n",
            "Training Loss : 7.8101043701171875\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 93292\n",
            "TimeSinceStart : 128.917870759964\n",
            "Training Loss : -32.55415344238281\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 94292\n",
            "TimeSinceStart : 130.30689811706543\n",
            "Training Loss : -30.243179321289062\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 95292\n",
            "TimeSinceStart : 131.7023687362671\n",
            "Training Loss : -31.502120971679688\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 96292\n",
            "TimeSinceStart : 133.06228685379028\n",
            "Training Loss : -30.745277404785156\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 97292\n",
            "TimeSinceStart : 134.44439435005188\n",
            "Training Loss : -31.59128189086914\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 98292\n",
            "TimeSinceStart : 135.83944058418274\n",
            "Training Loss : -32.123138427734375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 99292\n",
            "TimeSinceStart : 137.19722390174866\n",
            "Training Loss : -32.346214294433594\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 100292\n",
            "TimeSinceStart : 138.573810338974\n",
            "Training Loss : -33.85414505004883\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 101292\n",
            "TimeSinceStart : 139.94185376167297\n",
            "Training Loss : -32.21722412109375\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 102292\n",
            "TimeSinceStart : 141.29269218444824\n",
            "Training Loss : -32.11923599243164\n",
            "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 5000 \\\n",
        "-dsa --exp_name q1_lb_no_rtg_dsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qoaxQYqmcuu",
        "outputId": "1196a201-a0a9-4874-9e26-3cea06e9cf6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q1_lb_no_rtg_dsa_CartPole-v0_04-02-2022_16-46-01\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 33.33333206176758\n",
            "Eval_StdReturn : 20.159090042114258\n",
            "Eval_MaxReturn : 82.0\n",
            "Eval_MinReturn : 13.0\n",
            "Eval_AverageEpLen : 33.333333333333336\n",
            "Train_AverageReturn : 25.65816307067871\n",
            "Train_StdReturn : 14.328168869018555\n",
            "Train_MaxReturn : 132.0\n",
            "Train_MinReturn : 8.0\n",
            "Train_AverageEpLen : 25.658163265306122\n",
            "Train_EnvstepsSoFar : 5029\n",
            "TimeSinceStart : 5.3799378871917725\n",
            "Training Loss : 116112.53125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 31.538461685180664\n",
            "Eval_StdReturn : 13.624725341796875\n",
            "Eval_MaxReturn : 58.0\n",
            "Eval_MinReturn : 14.0\n",
            "Eval_AverageEpLen : 31.53846153846154\n",
            "Train_AverageReturn : 31.97452163696289\n",
            "Train_StdReturn : 17.77529525756836\n",
            "Train_MaxReturn : 105.0\n",
            "Train_MinReturn : 9.0\n",
            "Train_AverageEpLen : 31.97452229299363\n",
            "Train_EnvstepsSoFar : 10049\n",
            "TimeSinceStart : 10.843060970306396\n",
            "Training Loss : 140149.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 52.5\n",
            "Eval_StdReturn : 14.396180152893066\n",
            "Eval_MaxReturn : 78.0\n",
            "Eval_MinReturn : 31.0\n",
            "Eval_AverageEpLen : 52.5\n",
            "Train_AverageReturn : 42.092437744140625\n",
            "Train_StdReturn : 21.217161178588867\n",
            "Train_MaxReturn : 121.0\n",
            "Train_MinReturn : 12.0\n",
            "Train_AverageEpLen : 42.09243697478992\n",
            "Train_EnvstepsSoFar : 15058\n",
            "TimeSinceStart : 16.205942630767822\n",
            "Training Loss : 170702.78125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 60.85714340209961\n",
            "Eval_StdReturn : 30.870365142822266\n",
            "Eval_MaxReturn : 127.0\n",
            "Eval_MinReturn : 30.0\n",
            "Eval_AverageEpLen : 60.857142857142854\n",
            "Train_AverageReturn : 45.490909576416016\n",
            "Train_StdReturn : 19.291893005371094\n",
            "Train_MaxReturn : 108.0\n",
            "Train_MinReturn : 13.0\n",
            "Train_AverageEpLen : 45.49090909090909\n",
            "Train_EnvstepsSoFar : 20062\n",
            "TimeSinceStart : 21.627787590026855\n",
            "Training Loss : 168055.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 68.83333587646484\n",
            "Eval_StdReturn : 30.878345489501953\n",
            "Eval_MaxReturn : 108.0\n",
            "Eval_MinReturn : 30.0\n",
            "Eval_AverageEpLen : 68.83333333333333\n",
            "Train_AverageReturn : 51.20408248901367\n",
            "Train_StdReturn : 26.624736785888672\n",
            "Train_MaxReturn : 174.0\n",
            "Train_MinReturn : 18.0\n",
            "Train_AverageEpLen : 51.204081632653065\n",
            "Train_EnvstepsSoFar : 25080\n",
            "TimeSinceStart : 27.036601543426514\n",
            "Training Loss : 197715.359375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 77.5\n",
            "Eval_StdReturn : 24.472774505615234\n",
            "Eval_MaxReturn : 109.0\n",
            "Eval_MinReturn : 40.0\n",
            "Eval_AverageEpLen : 77.5\n",
            "Train_AverageReturn : 65.48052215576172\n",
            "Train_StdReturn : 23.568971633911133\n",
            "Train_MaxReturn : 139.0\n",
            "Train_MinReturn : 26.0\n",
            "Train_AverageEpLen : 65.48051948051948\n",
            "Train_EnvstepsSoFar : 30122\n",
            "TimeSinceStart : 32.592835664749146\n",
            "Training Loss : 219308.09375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 58.28571319580078\n",
            "Eval_StdReturn : 24.010202407836914\n",
            "Eval_MaxReturn : 109.0\n",
            "Eval_MinReturn : 25.0\n",
            "Eval_AverageEpLen : 58.285714285714285\n",
            "Train_AverageReturn : 70.53520965576172\n",
            "Train_StdReturn : 32.956581115722656\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 23.0\n",
            "Train_AverageEpLen : 70.53521126760563\n",
            "Train_EnvstepsSoFar : 35130\n",
            "TimeSinceStart : 37.937633752822876\n",
            "Training Loss : 246267.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 113.5\n",
            "Eval_StdReturn : 21.68524932861328\n",
            "Eval_MaxReturn : 150.0\n",
            "Eval_MinReturn : 93.0\n",
            "Eval_AverageEpLen : 113.5\n",
            "Train_AverageReturn : 77.01515197753906\n",
            "Train_StdReturn : 31.28049659729004\n",
            "Train_MaxReturn : 156.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 77.01515151515152\n",
            "Train_EnvstepsSoFar : 40213\n",
            "TimeSinceStart : 43.36684513092041\n",
            "Training Loss : 251837.375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 92.4000015258789\n",
            "Eval_StdReturn : 10.594338417053223\n",
            "Eval_MaxReturn : 105.0\n",
            "Eval_MinReturn : 80.0\n",
            "Eval_AverageEpLen : 92.4\n",
            "Train_AverageReturn : 78.59375\n",
            "Train_StdReturn : 34.328250885009766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 26.0\n",
            "Train_AverageEpLen : 78.59375\n",
            "Train_EnvstepsSoFar : 45243\n",
            "TimeSinceStart : 48.803605794906616\n",
            "Training Loss : 257722.203125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 97.0\n",
            "Eval_StdReturn : 39.613128662109375\n",
            "Eval_MaxReturn : 144.0\n",
            "Eval_MinReturn : 43.0\n",
            "Eval_AverageEpLen : 97.0\n",
            "Train_AverageReturn : 98.68627166748047\n",
            "Train_StdReturn : 33.887062072753906\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 47.0\n",
            "Train_AverageEpLen : 98.68627450980392\n",
            "Train_EnvstepsSoFar : 50276\n",
            "TimeSinceStart : 54.29566740989685\n",
            "Training Loss : 292526.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 102.5999984741211\n",
            "Eval_StdReturn : 37.07613754272461\n",
            "Eval_MaxReturn : 167.0\n",
            "Eval_MinReturn : 54.0\n",
            "Eval_AverageEpLen : 102.6\n",
            "Train_AverageReturn : 119.95237731933594\n",
            "Train_StdReturn : 43.38803482055664\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 119.95238095238095\n",
            "Train_EnvstepsSoFar : 55314\n",
            "TimeSinceStart : 59.841002464294434\n",
            "Training Loss : 351381.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 121.0\n",
            "Eval_StdReturn : 36.78994369506836\n",
            "Eval_MaxReturn : 170.0\n",
            "Eval_MinReturn : 67.0\n",
            "Eval_AverageEpLen : 121.0\n",
            "Train_AverageReturn : 141.02777099609375\n",
            "Train_StdReturn : 40.50959014892578\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 74.0\n",
            "Train_AverageEpLen : 141.02777777777777\n",
            "Train_EnvstepsSoFar : 60391\n",
            "TimeSinceStart : 65.32771420478821\n",
            "Training Loss : 399131.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 173.3333282470703\n",
            "Eval_StdReturn : 20.54804801940918\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 150.0\n",
            "Eval_AverageEpLen : 173.33333333333334\n",
            "Train_AverageReturn : 148.11764526367188\n",
            "Train_StdReturn : 35.426612854003906\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 52.0\n",
            "Train_AverageEpLen : 148.11764705882354\n",
            "Train_EnvstepsSoFar : 65427\n",
            "TimeSinceStart : 70.85170364379883\n",
            "Training Loss : 398702.6875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 164.3333282470703\n",
            "Eval_StdReturn : 28.158281326293945\n",
            "Eval_MaxReturn : 195.0\n",
            "Eval_MinReturn : 127.0\n",
            "Eval_AverageEpLen : 164.33333333333334\n",
            "Train_AverageReturn : 160.53125\n",
            "Train_StdReturn : 30.605335235595703\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 105.0\n",
            "Train_AverageEpLen : 160.53125\n",
            "Train_EnvstepsSoFar : 70564\n",
            "TimeSinceStart : 76.48421144485474\n",
            "Training Loss : 413748.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 160.0\n",
            "Eval_StdReturn : 29.223278045654297\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 131.0\n",
            "Eval_AverageEpLen : 160.0\n",
            "Train_AverageReturn : 161.1875\n",
            "Train_StdReturn : 28.548683166503906\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 113.0\n",
            "Train_AverageEpLen : 161.1875\n",
            "Train_EnvstepsSoFar : 75722\n",
            "TimeSinceStart : 82.05785584449768\n",
            "Training Loss : 413998.09375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 141.6666717529297\n",
            "Eval_StdReturn : 42.6953010559082\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 99.0\n",
            "Eval_AverageEpLen : 141.66666666666666\n",
            "Train_AverageReturn : 164.0\n",
            "Train_StdReturn : 31.81498908996582\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 98.0\n",
            "Train_AverageEpLen : 164.0\n",
            "Train_EnvstepsSoFar : 80806\n",
            "TimeSinceStart : 87.6561632156372\n",
            "Training Loss : 419875.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 157.3333282470703\n",
            "Eval_StdReturn : 30.70649528503418\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 129.0\n",
            "Eval_AverageEpLen : 157.33333333333334\n",
            "Train_AverageReturn : 160.375\n",
            "Train_StdReturn : 37.891910552978516\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 94.0\n",
            "Train_AverageEpLen : 160.375\n",
            "Train_EnvstepsSoFar : 85938\n",
            "TimeSinceStart : 98.60770750045776\n",
            "Training Loss : 409889.90625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 174.3333282470703\n",
            "Eval_StdReturn : 25.82419204711914\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 139.0\n",
            "Eval_AverageEpLen : 174.33333333333334\n",
            "Train_AverageReturn : 156.40625\n",
            "Train_StdReturn : 32.338890075683594\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 100.0\n",
            "Train_AverageEpLen : 156.40625\n",
            "Train_EnvstepsSoFar : 90943\n",
            "TimeSinceStart : 110.80741691589355\n",
            "Training Loss : 382624.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 167.5806427001953\n",
            "Train_StdReturn : 32.7904167175293\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 93.0\n",
            "Train_AverageEpLen : 167.58064516129033\n",
            "Train_EnvstepsSoFar : 96138\n",
            "TimeSinceStart : 116.39691209793091\n",
            "Training Loss : 416849.375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 186.0\n",
            "Eval_StdReturn : 19.79899024963379\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 158.0\n",
            "Eval_AverageEpLen : 186.0\n",
            "Train_AverageReturn : 179.85714721679688\n",
            "Train_StdReturn : 33.210060119628906\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 92.0\n",
            "Train_AverageEpLen : 179.85714285714286\n",
            "Train_EnvstepsSoFar : 101174\n",
            "TimeSinceStart : 124.22892260551453\n",
            "Training Loss : 434347.59375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 184.0\n",
            "Eval_StdReturn : 17.28197479248047\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 160.0\n",
            "Eval_AverageEpLen : 184.0\n",
            "Train_AverageReturn : 171.8000030517578\n",
            "Train_StdReturn : 35.56626892089844\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 102.0\n",
            "Train_AverageEpLen : 171.8\n",
            "Train_EnvstepsSoFar : 106328\n",
            "TimeSinceStart : 135.1439290046692\n",
            "Training Loss : 410455.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 155.0\n",
            "Eval_StdReturn : 43.59663391113281\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 96.0\n",
            "Eval_AverageEpLen : 155.0\n",
            "Train_AverageReturn : 173.03448486328125\n",
            "Train_StdReturn : 30.704914093017578\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 116.0\n",
            "Train_AverageEpLen : 173.0344827586207\n",
            "Train_EnvstepsSoFar : 111346\n",
            "TimeSinceStart : 143.814106464386\n",
            "Training Loss : 404383.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 180.7857208251953\n",
            "Train_StdReturn : 28.20379638671875\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 103.0\n",
            "Train_AverageEpLen : 180.78571428571428\n",
            "Train_EnvstepsSoFar : 116408\n",
            "TimeSinceStart : 149.33673572540283\n",
            "Training Loss : 403245.0625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 134.0\n",
            "Eval_StdReturn : 47.52543258666992\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 90.0\n",
            "Eval_AverageEpLen : 134.0\n",
            "Train_AverageReturn : 176.37930297851562\n",
            "Train_StdReturn : 30.304677963256836\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 98.0\n",
            "Train_AverageEpLen : 176.3793103448276\n",
            "Train_EnvstepsSoFar : 121523\n",
            "TimeSinceStart : 154.85067772865295\n",
            "Training Loss : 411726.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 138.6666717529297\n",
            "Eval_StdReturn : 6.944222450256348\n",
            "Eval_MaxReturn : 145.0\n",
            "Eval_MinReturn : 129.0\n",
            "Eval_AverageEpLen : 138.66666666666666\n",
            "Train_AverageReturn : 174.27586364746094\n",
            "Train_StdReturn : 32.411041259765625\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 96.0\n",
            "Train_AverageEpLen : 174.27586206896552\n",
            "Train_EnvstepsSoFar : 126577\n",
            "TimeSinceStart : 160.31446886062622\n",
            "Training Loss : 395265.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 119.5\n",
            "Eval_StdReturn : 8.558621406555176\n",
            "Eval_MaxReturn : 130.0\n",
            "Eval_MinReturn : 107.0\n",
            "Eval_AverageEpLen : 119.5\n",
            "Train_AverageReturn : 154.18182373046875\n",
            "Train_StdReturn : 36.57242965698242\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 85.0\n",
            "Train_AverageEpLen : 154.1818181818182\n",
            "Train_EnvstepsSoFar : 131665\n",
            "TimeSinceStart : 165.88440465927124\n",
            "Training Loss : 354966.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 125.25\n",
            "Eval_StdReturn : 11.540688514709473\n",
            "Eval_MaxReturn : 140.0\n",
            "Eval_MinReturn : 113.0\n",
            "Eval_AverageEpLen : 125.25\n",
            "Train_AverageReturn : 141.13888549804688\n",
            "Train_StdReturn : 30.533363342285156\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 90.0\n",
            "Train_AverageEpLen : 141.13888888888889\n",
            "Train_EnvstepsSoFar : 136746\n",
            "TimeSinceStart : 171.53481435775757\n",
            "Training Loss : 327546.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 120.75\n",
            "Eval_StdReturn : 9.756407737731934\n",
            "Eval_MaxReturn : 137.0\n",
            "Eval_MinReturn : 111.0\n",
            "Eval_AverageEpLen : 120.75\n",
            "Train_AverageReturn : 130.25640869140625\n",
            "Train_StdReturn : 26.31926155090332\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 95.0\n",
            "Train_AverageEpLen : 130.25641025641025\n",
            "Train_EnvstepsSoFar : 141826\n",
            "TimeSinceStart : 177.045814037323\n",
            "Training Loss : 291582.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 104.25\n",
            "Eval_StdReturn : 26.30945587158203\n",
            "Eval_MaxReturn : 132.0\n",
            "Eval_MinReturn : 65.0\n",
            "Eval_AverageEpLen : 104.25\n",
            "Train_AverageReturn : 121.23809814453125\n",
            "Train_StdReturn : 16.985721588134766\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 97.0\n",
            "Train_AverageEpLen : 121.23809523809524\n",
            "Train_EnvstepsSoFar : 146918\n",
            "TimeSinceStart : 182.58742904663086\n",
            "Training Loss : 269644.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 111.75\n",
            "Eval_StdReturn : 9.575359344482422\n",
            "Eval_MaxReturn : 126.0\n",
            "Eval_MinReturn : 103.0\n",
            "Eval_AverageEpLen : 111.75\n",
            "Train_AverageReturn : 119.26190185546875\n",
            "Train_StdReturn : 11.602666854858398\n",
            "Train_MaxReturn : 149.0\n",
            "Train_MinReturn : 97.0\n",
            "Train_AverageEpLen : 119.26190476190476\n",
            "Train_EnvstepsSoFar : 151927\n",
            "TimeSinceStart : 187.99977922439575\n",
            "Training Loss : 248315.3125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 113.5\n",
            "Eval_StdReturn : 2.872281312942505\n",
            "Eval_MaxReturn : 118.0\n",
            "Eval_MinReturn : 110.0\n",
            "Eval_AverageEpLen : 113.5\n",
            "Train_AverageReturn : 113.95454406738281\n",
            "Train_StdReturn : 19.035797119140625\n",
            "Train_MaxReturn : 138.0\n",
            "Train_MinReturn : 36.0\n",
            "Train_AverageEpLen : 113.95454545454545\n",
            "Train_EnvstepsSoFar : 156941\n",
            "TimeSinceStart : 193.435711145401\n",
            "Training Loss : 247662.40625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 127.5\n",
            "Eval_StdReturn : 1.658312439918518\n",
            "Eval_MaxReturn : 129.0\n",
            "Eval_MinReturn : 125.0\n",
            "Eval_AverageEpLen : 127.5\n",
            "Train_AverageReturn : 114.88636016845703\n",
            "Train_StdReturn : 12.3641996383667\n",
            "Train_MaxReturn : 148.0\n",
            "Train_MinReturn : 66.0\n",
            "Train_AverageEpLen : 114.88636363636364\n",
            "Train_EnvstepsSoFar : 161996\n",
            "TimeSinceStart : 198.98275923728943\n",
            "Training Loss : 245538.015625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 120.0\n",
            "Eval_StdReturn : 6.363961219787598\n",
            "Eval_MaxReturn : 130.0\n",
            "Eval_MinReturn : 114.0\n",
            "Eval_AverageEpLen : 120.0\n",
            "Train_AverageReturn : 115.84091186523438\n",
            "Train_StdReturn : 11.387679100036621\n",
            "Train_MaxReturn : 143.0\n",
            "Train_MinReturn : 98.0\n",
            "Train_AverageEpLen : 115.8409090909091\n",
            "Train_EnvstepsSoFar : 167093\n",
            "TimeSinceStart : 204.52099895477295\n",
            "Training Loss : 249715.875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 114.25\n",
            "Eval_StdReturn : 10.917303085327148\n",
            "Eval_MaxReturn : 130.0\n",
            "Eval_MinReturn : 101.0\n",
            "Eval_AverageEpLen : 114.25\n",
            "Train_AverageReturn : 111.4000015258789\n",
            "Train_StdReturn : 19.310791015625\n",
            "Train_MaxReturn : 134.0\n",
            "Train_MinReturn : 37.0\n",
            "Train_AverageEpLen : 111.4\n",
            "Train_EnvstepsSoFar : 172106\n",
            "TimeSinceStart : 210.2962188720703\n",
            "Training Loss : 233428.125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 121.75\n",
            "Eval_StdReturn : 5.117372512817383\n",
            "Eval_MaxReturn : 130.0\n",
            "Eval_MinReturn : 116.0\n",
            "Eval_AverageEpLen : 121.75\n",
            "Train_AverageReturn : 108.86956787109375\n",
            "Train_StdReturn : 19.107940673828125\n",
            "Train_MaxReturn : 135.0\n",
            "Train_MinReturn : 54.0\n",
            "Train_AverageEpLen : 108.8695652173913\n",
            "Train_EnvstepsSoFar : 177114\n",
            "TimeSinceStart : 216.83194136619568\n",
            "Training Loss : 223074.875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 119.5\n",
            "Eval_StdReturn : 1.5\n",
            "Eval_MaxReturn : 121.0\n",
            "Eval_MinReturn : 117.0\n",
            "Eval_AverageEpLen : 119.5\n",
            "Train_AverageReturn : 117.39534759521484\n",
            "Train_StdReturn : 14.719294548034668\n",
            "Train_MaxReturn : 145.0\n",
            "Train_MinReturn : 47.0\n",
            "Train_AverageEpLen : 117.3953488372093\n",
            "Train_EnvstepsSoFar : 182162\n",
            "TimeSinceStart : 222.3479118347168\n",
            "Training Loss : 240662.265625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 140.6666717529297\n",
            "Eval_StdReturn : 4.027681827545166\n",
            "Eval_MaxReturn : 144.0\n",
            "Eval_MinReturn : 135.0\n",
            "Eval_AverageEpLen : 140.66666666666666\n",
            "Train_AverageReturn : 125.43902587890625\n",
            "Train_StdReturn : 11.382590293884277\n",
            "Train_MaxReturn : 146.0\n",
            "Train_MinReturn : 102.0\n",
            "Train_AverageEpLen : 125.4390243902439\n",
            "Train_EnvstepsSoFar : 187305\n",
            "TimeSinceStart : 227.99393916130066\n",
            "Training Loss : 249142.53125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 140.3333282470703\n",
            "Eval_StdReturn : 7.363574028015137\n",
            "Eval_MaxReturn : 149.0\n",
            "Eval_MinReturn : 131.0\n",
            "Eval_AverageEpLen : 140.33333333333334\n",
            "Train_AverageReturn : 138.2432403564453\n",
            "Train_StdReturn : 15.078901290893555\n",
            "Train_MaxReturn : 170.0\n",
            "Train_MinReturn : 113.0\n",
            "Train_AverageEpLen : 138.24324324324326\n",
            "Train_EnvstepsSoFar : 192420\n",
            "TimeSinceStart : 233.37755250930786\n",
            "Training Loss : 282440.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 180.0\n",
            "Eval_StdReturn : 13.063945770263672\n",
            "Eval_MaxReturn : 196.0\n",
            "Eval_MinReturn : 164.0\n",
            "Eval_AverageEpLen : 180.0\n",
            "Train_AverageReturn : 156.71875\n",
            "Train_StdReturn : 21.916366577148438\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 125.0\n",
            "Train_AverageEpLen : 156.71875\n",
            "Train_EnvstepsSoFar : 197435\n",
            "TimeSinceStart : 238.89752531051636\n",
            "Training Loss : 313250.40625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 185.0\n",
            "Eval_StdReturn : 21.21320343017578\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 155.0\n",
            "Eval_AverageEpLen : 185.0\n",
            "Train_AverageReturn : 177.86207580566406\n",
            "Train_StdReturn : 25.308748245239258\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 134.0\n",
            "Train_AverageEpLen : 177.86206896551724\n",
            "Train_EnvstepsSoFar : 202593\n",
            "TimeSinceStart : 244.550146818161\n",
            "Training Loss : 375871.6875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 194.84616088867188\n",
            "Train_StdReturn : 13.363821029663086\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 146.0\n",
            "Train_AverageEpLen : 194.84615384615384\n",
            "Train_EnvstepsSoFar : 207659\n",
            "TimeSinceStart : 250.04895782470703\n",
            "Training Loss : 390603.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 212659\n",
            "TimeSinceStart : 255.44427919387817\n",
            "Training Loss : 379442.71875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 198.0\n",
            "Train_StdReturn : 10.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 148.0\n",
            "Train_AverageEpLen : 198.0\n",
            "Train_EnvstepsSoFar : 217807\n",
            "TimeSinceStart : 261.0561897754669\n",
            "Training Loss : 389023.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 222807\n",
            "TimeSinceStart : 266.3807225227356\n",
            "Training Loss : 377780.875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 227807\n",
            "TimeSinceStart : 271.75551891326904\n",
            "Training Loss : 371898.78125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 197.23077392578125\n",
            "Train_StdReturn : 13.846153259277344\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 128.0\n",
            "Train_AverageEpLen : 197.23076923076923\n",
            "Train_EnvstepsSoFar : 232935\n",
            "TimeSinceStart : 277.2248418331146\n",
            "Training Loss : 373889.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 237935\n",
            "TimeSinceStart : 282.5143554210663\n",
            "Training Loss : 377685.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 171.6666717529297\n",
            "Eval_StdReturn : 40.06938552856445\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 115.0\n",
            "Eval_AverageEpLen : 171.66666666666666\n",
            "Train_AverageReturn : 190.62962341308594\n",
            "Train_StdReturn : 26.512258529663086\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 96.0\n",
            "Train_AverageEpLen : 190.62962962962962\n",
            "Train_EnvstepsSoFar : 243082\n",
            "TimeSinceStart : 288.25617599487305\n",
            "Training Loss : 393394.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 188.25926208496094\n",
            "Train_StdReturn : 28.265399932861328\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 116.0\n",
            "Train_AverageEpLen : 188.25925925925927\n",
            "Train_EnvstepsSoFar : 248165\n",
            "TimeSinceStart : 294.6330029964447\n",
            "Training Loss : 387514.46875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 180.17857360839844\n",
            "Train_StdReturn : 34.78511047363281\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 109.0\n",
            "Train_AverageEpLen : 180.17857142857142\n",
            "Train_EnvstepsSoFar : 253210\n",
            "TimeSinceStart : 300.06418633461\n",
            "Training Loss : 370622.28125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 184.60714721679688\n",
            "Train_StdReturn : 32.240543365478516\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 107.0\n",
            "Train_AverageEpLen : 184.60714285714286\n",
            "Train_EnvstepsSoFar : 258379\n",
            "TimeSinceStart : 305.615754365921\n",
            "Training Loss : 388122.84375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 167.6666717529297\n",
            "Eval_StdReturn : 45.72623825073242\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 103.0\n",
            "Eval_AverageEpLen : 167.66666666666666\n",
            "Train_AverageReturn : 181.60714721679688\n",
            "Train_StdReturn : 41.38351821899414\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 40.0\n",
            "Train_AverageEpLen : 181.60714285714286\n",
            "Train_EnvstepsSoFar : 263464\n",
            "TimeSinceStart : 311.1469647884369\n",
            "Training Loss : 396450.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 184.42857360839844\n",
            "Train_StdReturn : 40.5713005065918\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 29.0\n",
            "Train_AverageEpLen : 184.42857142857142\n",
            "Train_EnvstepsSoFar : 268628\n",
            "TimeSinceStart : 316.78489351272583\n",
            "Training Loss : 410417.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 197.6666717529297\n",
            "Eval_StdReturn : 3.2998316287994385\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 193.0\n",
            "Eval_AverageEpLen : 197.66666666666666\n",
            "Train_AverageReturn : 186.1481475830078\n",
            "Train_StdReturn : 39.43649673461914\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 32.0\n",
            "Train_AverageEpLen : 186.14814814814815\n",
            "Train_EnvstepsSoFar : 273654\n",
            "TimeSinceStart : 322.4410991668701\n",
            "Training Loss : 407322.46875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 140.3333282470703\n",
            "Eval_StdReturn : 78.79227447509766\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 29.0\n",
            "Eval_AverageEpLen : 140.33333333333334\n",
            "Train_AverageReturn : 195.03846740722656\n",
            "Train_StdReturn : 19.28427505493164\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 100.0\n",
            "Train_AverageEpLen : 195.03846153846155\n",
            "Train_EnvstepsSoFar : 278725\n",
            "TimeSinceStart : 327.9428822994232\n",
            "Training Loss : 424819.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 141.25\n",
            "Eval_StdReturn : 64.7586898803711\n",
            "Eval_MaxReturn : 191.0\n",
            "Eval_MinReturn : 30.0\n",
            "Eval_AverageEpLen : 141.25\n",
            "Train_AverageReturn : 179.42857360839844\n",
            "Train_StdReturn : 46.40460968017578\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 27.0\n",
            "Train_AverageEpLen : 179.42857142857142\n",
            "Train_EnvstepsSoFar : 283749\n",
            "TimeSinceStart : 333.6098082065582\n",
            "Training Loss : 414224.5\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 131.5\n",
            "Eval_StdReturn : 60.4710693359375\n",
            "Eval_MaxReturn : 178.0\n",
            "Eval_MinReturn : 28.0\n",
            "Eval_AverageEpLen : 131.5\n",
            "Train_AverageReturn : 156.8125\n",
            "Train_StdReturn : 50.42409896850586\n",
            "Train_MaxReturn : 195.0\n",
            "Train_MinReturn : 27.0\n",
            "Train_AverageEpLen : 156.8125\n",
            "Train_EnvstepsSoFar : 288767\n",
            "TimeSinceStart : 339.12510871887207\n",
            "Training Loss : 369590.40625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 116.75\n",
            "Eval_StdReturn : 51.28535461425781\n",
            "Eval_MaxReturn : 153.0\n",
            "Eval_MinReturn : 29.0\n",
            "Eval_AverageEpLen : 116.75\n",
            "Train_AverageReturn : 131.35897827148438\n",
            "Train_StdReturn : 58.46782302856445\n",
            "Train_MaxReturn : 182.0\n",
            "Train_MinReturn : 21.0\n",
            "Train_AverageEpLen : 131.35897435897436\n",
            "Train_EnvstepsSoFar : 293890\n",
            "TimeSinceStart : 344.6672692298889\n",
            "Training Loss : 346385.03125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 144.3333282470703\n",
            "Eval_StdReturn : 7.039570331573486\n",
            "Eval_MaxReturn : 152.0\n",
            "Eval_MinReturn : 135.0\n",
            "Eval_AverageEpLen : 144.33333333333334\n",
            "Train_AverageReturn : 94.81481170654297\n",
            "Train_StdReturn : 60.904014587402344\n",
            "Train_MaxReturn : 177.0\n",
            "Train_MinReturn : 21.0\n",
            "Train_AverageEpLen : 94.81481481481481\n",
            "Train_EnvstepsSoFar : 299010\n",
            "TimeSinceStart : 350.28950929641724\n",
            "Training Loss : 289724.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 166.6666717529297\n",
            "Eval_StdReturn : 1.247219204902649\n",
            "Eval_MaxReturn : 168.0\n",
            "Eval_MinReturn : 165.0\n",
            "Eval_AverageEpLen : 166.66666666666666\n",
            "Train_AverageReturn : 131.73684692382812\n",
            "Train_StdReturn : 47.561973571777344\n",
            "Train_MaxReturn : 171.0\n",
            "Train_MinReturn : 19.0\n",
            "Train_AverageEpLen : 131.73684210526315\n",
            "Train_EnvstepsSoFar : 304016\n",
            "TimeSinceStart : 355.83886456489563\n",
            "Training Loss : 319468.125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 185.3333282470703\n",
            "Eval_StdReturn : 6.182412147521973\n",
            "Eval_MaxReturn : 194.0\n",
            "Eval_MinReturn : 180.0\n",
            "Eval_AverageEpLen : 185.33333333333334\n",
            "Train_AverageReturn : 162.25807189941406\n",
            "Train_StdReturn : 25.233064651489258\n",
            "Train_MaxReturn : 179.0\n",
            "Train_MinReturn : 30.0\n",
            "Train_AverageEpLen : 162.25806451612902\n",
            "Train_EnvstepsSoFar : 309046\n",
            "TimeSinceStart : 361.48642468452454\n",
            "Training Loss : 350774.34375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 172.96551513671875\n",
            "Train_StdReturn : 28.102622985839844\n",
            "Train_MaxReturn : 196.0\n",
            "Train_MinReturn : 31.0\n",
            "Train_AverageEpLen : 172.9655172413793\n",
            "Train_EnvstepsSoFar : 314062\n",
            "TimeSinceStart : 367.0217218399048\n",
            "Training Loss : 381044.84375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 190.55555725097656\n",
            "Train_StdReturn : 32.297611236572266\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 28.0\n",
            "Train_AverageEpLen : 190.55555555555554\n",
            "Train_EnvstepsSoFar : 319207\n",
            "TimeSinceStart : 372.4770441055298\n",
            "Training Loss : 433986.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 324207\n",
            "TimeSinceStart : 377.7648756504059\n",
            "Training Loss : 428559.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 329207\n",
            "TimeSinceStart : 383.15286231040955\n",
            "Training Loss : 415596.375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 334207\n",
            "TimeSinceStart : 388.57363867759705\n",
            "Training Loss : 418656.84375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 339207\n",
            "TimeSinceStart : 393.9255542755127\n",
            "Training Loss : 421751.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 344207\n",
            "TimeSinceStart : 399.2967097759247\n",
            "Training Loss : 411029.875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 349207\n",
            "TimeSinceStart : 404.7032473087311\n",
            "Training Loss : 420298.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 354207\n",
            "TimeSinceStart : 410.12287044525146\n",
            "Training Loss : 414711.15625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 359207\n",
            "TimeSinceStart : 415.56434440612793\n",
            "Training Loss : 402592.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 364207\n",
            "TimeSinceStart : 420.94606280326843\n",
            "Training Loss : 419067.28125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 369207\n",
            "TimeSinceStart : 426.6417233943939\n",
            "Training Loss : 406207.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 374207\n",
            "TimeSinceStart : 432.1702878475189\n",
            "Training Loss : 401541.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 379207\n",
            "TimeSinceStart : 437.587792634964\n",
            "Training Loss : 404763.6875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 384207\n",
            "TimeSinceStart : 443.00639152526855\n",
            "Training Loss : 413892.96875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 389207\n",
            "TimeSinceStart : 448.49598598480225\n",
            "Training Loss : 403418.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 394207\n",
            "TimeSinceStart : 453.96648693084717\n",
            "Training Loss : 394730.34375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 399207\n",
            "TimeSinceStart : 459.42319345474243\n",
            "Training Loss : 411613.71875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 404207\n",
            "TimeSinceStart : 464.8072500228882\n",
            "Training Loss : 408817.8125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 409207\n",
            "TimeSinceStart : 470.30159735679626\n",
            "Training Loss : 409165.40625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 414207\n",
            "TimeSinceStart : 475.8400287628174\n",
            "Training Loss : 415037.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 419207\n",
            "TimeSinceStart : 481.27052998542786\n",
            "Training Loss : 404705.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 424207\n",
            "TimeSinceStart : 486.691960811615\n",
            "Training Loss : 425162.5\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 429207\n",
            "TimeSinceStart : 492.1284523010254\n",
            "Training Loss : 413736.0625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 434207\n",
            "TimeSinceStart : 497.591744184494\n",
            "Training Loss : 416369.34375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 439207\n",
            "TimeSinceStart : 503.11459708213806\n",
            "Training Loss : 412931.90625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 444207\n",
            "TimeSinceStart : 508.46276116371155\n",
            "Training Loss : 426656.5\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 449207\n",
            "TimeSinceStart : 513.7622935771942\n",
            "Training Loss : 418070.0625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 454207\n",
            "TimeSinceStart : 519.0367045402527\n",
            "Training Loss : 415278.8125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 459207\n",
            "TimeSinceStart : 524.4409291744232\n",
            "Training Loss : 423176.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 464207\n",
            "TimeSinceStart : 529.7824699878693\n",
            "Training Loss : 432058.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 469207\n",
            "TimeSinceStart : 535.1155064105988\n",
            "Training Loss : 439134.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 474207\n",
            "TimeSinceStart : 540.6081211566925\n",
            "Training Loss : 436442.3125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 479207\n",
            "TimeSinceStart : 546.142767906189\n",
            "Training Loss : 446864.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 484207\n",
            "TimeSinceStart : 551.5984888076782\n",
            "Training Loss : 444687.90625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 489207\n",
            "TimeSinceStart : 556.9230661392212\n",
            "Training Loss : 452286.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 494207\n",
            "TimeSinceStart : 562.4723362922668\n",
            "Training Loss : 444262.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 499207\n",
            "TimeSinceStart : 567.9903430938721\n",
            "Training Loss : 444975.6875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 504207\n",
            "TimeSinceStart : 573.5460793972015\n",
            "Training Loss : 449954.09375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 5000 \\\n",
        "-rtg -dsa --exp_name q1_lb_rtg_dsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQJCEISsmnXx",
        "outputId": "00e0ff82-b3d5-46bc-f34a-a995057e2269"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q1_lb_rtg_dsa_CartPole-v0_04-02-2022_16-55-37\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 33.33333206176758\n",
            "Eval_StdReturn : 20.159090042114258\n",
            "Eval_MaxReturn : 82.0\n",
            "Eval_MinReturn : 13.0\n",
            "Eval_AverageEpLen : 33.333333333333336\n",
            "Train_AverageReturn : 25.65816307067871\n",
            "Train_StdReturn : 14.328168869018555\n",
            "Train_MaxReturn : 132.0\n",
            "Train_MinReturn : 8.0\n",
            "Train_AverageEpLen : 25.658163265306122\n",
            "Train_EnvstepsSoFar : 5029\n",
            "TimeSinceStart : 5.6480793952941895\n",
            "Training Loss : 112792.4453125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 39.83333206176758\n",
            "Eval_StdReturn : 23.39099884033203\n",
            "Eval_MaxReturn : 96.0\n",
            "Eval_MinReturn : 16.0\n",
            "Eval_AverageEpLen : 39.833333333333336\n",
            "Train_AverageReturn : 29.839284896850586\n",
            "Train_StdReturn : 16.807498931884766\n",
            "Train_MaxReturn : 85.0\n",
            "Train_MinReturn : 10.0\n",
            "Train_AverageEpLen : 29.839285714285715\n",
            "Train_EnvstepsSoFar : 10042\n",
            "TimeSinceStart : 11.372042655944824\n",
            "Training Loss : 128028.1640625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 48.11111068725586\n",
            "Eval_StdReturn : 17.978038787841797\n",
            "Eval_MaxReturn : 81.0\n",
            "Eval_MinReturn : 24.0\n",
            "Eval_AverageEpLen : 48.111111111111114\n",
            "Train_AverageReturn : 40.90243911743164\n",
            "Train_StdReturn : 23.87106704711914\n",
            "Train_MaxReturn : 166.0\n",
            "Train_MinReturn : 12.0\n",
            "Train_AverageEpLen : 40.90243902439025\n",
            "Train_EnvstepsSoFar : 15073\n",
            "TimeSinceStart : 17.045406341552734\n",
            "Training Loss : 174920.265625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 70.0\n",
            "Eval_StdReturn : 34.04898452758789\n",
            "Eval_MaxReturn : 134.0\n",
            "Eval_MinReturn : 39.0\n",
            "Eval_AverageEpLen : 70.0\n",
            "Train_AverageReturn : 52.19791793823242\n",
            "Train_StdReturn : 28.06377410888672\n",
            "Train_MaxReturn : 149.0\n",
            "Train_MinReturn : 15.0\n",
            "Train_AverageEpLen : 52.197916666666664\n",
            "Train_EnvstepsSoFar : 20084\n",
            "TimeSinceStart : 22.688806772232056\n",
            "Training Loss : 208941.6875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 76.71428680419922\n",
            "Eval_StdReturn : 46.6406135559082\n",
            "Eval_MaxReturn : 186.0\n",
            "Eval_MinReturn : 38.0\n",
            "Eval_AverageEpLen : 76.71428571428571\n",
            "Train_AverageReturn : 57.8505744934082\n",
            "Train_StdReturn : 27.447978973388672\n",
            "Train_MaxReturn : 136.0\n",
            "Train_MinReturn : 18.0\n",
            "Train_AverageEpLen : 57.85057471264368\n",
            "Train_EnvstepsSoFar : 25117\n",
            "TimeSinceStart : 28.47703456878662\n",
            "Training Loss : 215942.15625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 84.4000015258789\n",
            "Eval_StdReturn : 36.031097412109375\n",
            "Eval_MaxReturn : 149.0\n",
            "Eval_MinReturn : 51.0\n",
            "Eval_AverageEpLen : 84.4\n",
            "Train_AverageReturn : 63.658226013183594\n",
            "Train_StdReturn : 29.058597564697266\n",
            "Train_MaxReturn : 162.0\n",
            "Train_MinReturn : 15.0\n",
            "Train_AverageEpLen : 63.65822784810127\n",
            "Train_EnvstepsSoFar : 30146\n",
            "TimeSinceStart : 34.1542911529541\n",
            "Training Loss : 226563.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 86.0\n",
            "Eval_StdReturn : 10.788883209228516\n",
            "Eval_MaxReturn : 100.0\n",
            "Eval_MinReturn : 71.0\n",
            "Eval_AverageEpLen : 86.0\n",
            "Train_AverageReturn : 74.57353210449219\n",
            "Train_StdReturn : 32.81916427612305\n",
            "Train_MaxReturn : 181.0\n",
            "Train_MinReturn : 24.0\n",
            "Train_AverageEpLen : 74.57352941176471\n",
            "Train_EnvstepsSoFar : 35217\n",
            "TimeSinceStart : 39.84207582473755\n",
            "Training Loss : 258801.46875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 104.5999984741211\n",
            "Eval_StdReturn : 48.280845642089844\n",
            "Eval_MaxReturn : 167.0\n",
            "Eval_MinReturn : 44.0\n",
            "Eval_AverageEpLen : 104.6\n",
            "Train_AverageReturn : 82.42623138427734\n",
            "Train_StdReturn : 37.28246307373047\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 34.0\n",
            "Train_AverageEpLen : 82.42622950819673\n",
            "Train_EnvstepsSoFar : 40245\n",
            "TimeSinceStart : 45.65001726150513\n",
            "Training Loss : 278950.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 114.25\n",
            "Eval_StdReturn : 52.17458724975586\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 59.0\n",
            "Eval_AverageEpLen : 114.25\n",
            "Train_AverageReturn : 104.1875\n",
            "Train_StdReturn : 46.60456466674805\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 36.0\n",
            "Train_AverageEpLen : 104.1875\n",
            "Train_EnvstepsSoFar : 45246\n",
            "TimeSinceStart : 51.34543204307556\n",
            "Training Loss : 345737.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 160.6666717529297\n",
            "Eval_StdReturn : 31.84685516357422\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 122.0\n",
            "Eval_AverageEpLen : 160.66666666666666\n",
            "Train_AverageReturn : 118.86046600341797\n",
            "Train_StdReturn : 44.96747589111328\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 41.0\n",
            "Train_AverageEpLen : 118.86046511627907\n",
            "Train_EnvstepsSoFar : 50357\n",
            "TimeSinceStart : 57.224520683288574\n",
            "Training Loss : 377174.3125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 146.3333282470703\n",
            "Eval_StdReturn : 13.274871826171875\n",
            "Eval_MaxReturn : 159.0\n",
            "Eval_MinReturn : 128.0\n",
            "Eval_AverageEpLen : 146.33333333333334\n",
            "Train_AverageReturn : 148.88235473632812\n",
            "Train_StdReturn : 31.768245697021484\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 65.0\n",
            "Train_AverageEpLen : 148.88235294117646\n",
            "Train_EnvstepsSoFar : 55419\n",
            "TimeSinceStart : 62.929423332214355\n",
            "Training Loss : 414556.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 143.6666717529297\n",
            "Eval_StdReturn : 21.51485252380371\n",
            "Eval_MaxReturn : 173.0\n",
            "Eval_MinReturn : 122.0\n",
            "Eval_AverageEpLen : 143.66666666666666\n",
            "Train_AverageReturn : 148.55882263183594\n",
            "Train_StdReturn : 28.618371963500977\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 99.0\n",
            "Train_AverageEpLen : 148.55882352941177\n",
            "Train_EnvstepsSoFar : 60470\n",
            "TimeSinceStart : 68.66142988204956\n",
            "Training Loss : 405207.125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 152.3333282470703\n",
            "Eval_StdReturn : 62.52643966674805\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 64.0\n",
            "Eval_AverageEpLen : 152.33333333333334\n",
            "Train_AverageReturn : 159.75\n",
            "Train_StdReturn : 32.85859680175781\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 86.0\n",
            "Train_AverageEpLen : 159.75\n",
            "Train_EnvstepsSoFar : 65582\n",
            "TimeSinceStart : 74.43749284744263\n",
            "Training Loss : 436612.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 191.0\n",
            "Eval_StdReturn : 12.727922439575195\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 173.0\n",
            "Eval_AverageEpLen : 191.0\n",
            "Train_AverageReturn : 151.35293579101562\n",
            "Train_StdReturn : 37.470680236816406\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 51.0\n",
            "Train_AverageEpLen : 151.35294117647058\n",
            "Train_EnvstepsSoFar : 70728\n",
            "TimeSinceStart : 80.40636205673218\n",
            "Training Loss : 413838.21875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 166.0\n",
            "Eval_StdReturn : 24.711671829223633\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 142.0\n",
            "Eval_AverageEpLen : 166.0\n",
            "Train_AverageReturn : 161.3225860595703\n",
            "Train_StdReturn : 36.09788513183594\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 91.0\n",
            "Train_AverageEpLen : 161.32258064516128\n",
            "Train_EnvstepsSoFar : 75729\n",
            "TimeSinceStart : 86.1479332447052\n",
            "Training Loss : 425491.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 139.6666717529297\n",
            "Eval_StdReturn : 14.613540649414062\n",
            "Eval_MaxReturn : 155.0\n",
            "Eval_MinReturn : 120.0\n",
            "Eval_AverageEpLen : 139.66666666666666\n",
            "Train_AverageReturn : 155.36363220214844\n",
            "Train_StdReturn : 31.194229125976562\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 102.0\n",
            "Train_AverageEpLen : 155.36363636363637\n",
            "Train_EnvstepsSoFar : 80856\n",
            "TimeSinceStart : 91.85797882080078\n",
            "Training Loss : 406338.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 148.0\n",
            "Eval_StdReturn : 14.966629981994629\n",
            "Eval_MaxReturn : 168.0\n",
            "Eval_MinReturn : 132.0\n",
            "Eval_AverageEpLen : 148.0\n",
            "Train_AverageReturn : 154.4242401123047\n",
            "Train_StdReturn : 31.542207717895508\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 99.0\n",
            "Train_AverageEpLen : 154.42424242424244\n",
            "Train_EnvstepsSoFar : 85952\n",
            "TimeSinceStart : 97.59909772872925\n",
            "Training Loss : 395371.40625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 115.5\n",
            "Eval_StdReturn : 6.103277683258057\n",
            "Eval_MaxReturn : 120.0\n",
            "Eval_MinReturn : 105.0\n",
            "Eval_AverageEpLen : 115.5\n",
            "Train_AverageReturn : 136.45945739746094\n",
            "Train_StdReturn : 22.070642471313477\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 107.0\n",
            "Train_AverageEpLen : 136.45945945945945\n",
            "Train_EnvstepsSoFar : 91001\n",
            "TimeSinceStart : 103.2585940361023\n",
            "Training Loss : 341668.0625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 121.5\n",
            "Eval_StdReturn : 11.346805572509766\n",
            "Eval_MaxReturn : 138.0\n",
            "Eval_MinReturn : 106.0\n",
            "Eval_AverageEpLen : 121.5\n",
            "Train_AverageReturn : 127.2249984741211\n",
            "Train_StdReturn : 16.757814407348633\n",
            "Train_MaxReturn : 168.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 127.225\n",
            "Train_EnvstepsSoFar : 96090\n",
            "TimeSinceStart : 109.06887102127075\n",
            "Training Loss : 315951.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 139.3333282470703\n",
            "Eval_StdReturn : 24.72964859008789\n",
            "Eval_MaxReturn : 174.0\n",
            "Eval_MinReturn : 118.0\n",
            "Eval_AverageEpLen : 139.33333333333334\n",
            "Train_AverageReturn : 126.0\n",
            "Train_StdReturn : 21.907760620117188\n",
            "Train_MaxReturn : 166.0\n",
            "Train_MinReturn : 51.0\n",
            "Train_AverageEpLen : 126.0\n",
            "Train_EnvstepsSoFar : 101130\n",
            "TimeSinceStart : 114.74456191062927\n",
            "Training Loss : 312132.28125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 107.75\n",
            "Eval_StdReturn : 38.19276809692383\n",
            "Eval_MaxReturn : 141.0\n",
            "Eval_MinReturn : 44.0\n",
            "Eval_AverageEpLen : 107.75\n",
            "Train_AverageReturn : 117.6744155883789\n",
            "Train_StdReturn : 27.333189010620117\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 43.0\n",
            "Train_AverageEpLen : 117.67441860465117\n",
            "Train_EnvstepsSoFar : 106190\n",
            "TimeSinceStart : 120.37649631500244\n",
            "Training Loss : 290906.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 127.0\n",
            "Eval_StdReturn : 10.747092247009277\n",
            "Eval_MaxReturn : 141.0\n",
            "Eval_MinReturn : 111.0\n",
            "Eval_AverageEpLen : 127.0\n",
            "Train_AverageReturn : 123.82926940917969\n",
            "Train_StdReturn : 18.803089141845703\n",
            "Train_MaxReturn : 153.0\n",
            "Train_MinReturn : 43.0\n",
            "Train_AverageEpLen : 123.82926829268293\n",
            "Train_EnvstepsSoFar : 111267\n",
            "TimeSinceStart : 126.11976957321167\n",
            "Training Loss : 301995.59375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 131.75\n",
            "Eval_StdReturn : 17.9774169921875\n",
            "Eval_MaxReturn : 156.0\n",
            "Eval_MinReturn : 113.0\n",
            "Eval_AverageEpLen : 131.75\n",
            "Train_AverageReturn : 131.76315307617188\n",
            "Train_StdReturn : 20.348844528198242\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 96.0\n",
            "Train_AverageEpLen : 131.76315789473685\n",
            "Train_EnvstepsSoFar : 116274\n",
            "TimeSinceStart : 131.793123960495\n",
            "Training Loss : 310834.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 159.6666717529297\n",
            "Eval_StdReturn : 3.2998316287994385\n",
            "Eval_MaxReturn : 164.0\n",
            "Eval_MinReturn : 156.0\n",
            "Eval_AverageEpLen : 159.66666666666666\n",
            "Train_AverageReturn : 138.40541076660156\n",
            "Train_StdReturn : 22.3636531829834\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 100.0\n",
            "Train_AverageEpLen : 138.40540540540542\n",
            "Train_EnvstepsSoFar : 121395\n",
            "TimeSinceStart : 137.58761191368103\n",
            "Training Loss : 334652.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 166.3333282470703\n",
            "Eval_StdReturn : 28.193775177001953\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 131.0\n",
            "Eval_AverageEpLen : 166.33333333333334\n",
            "Train_AverageReturn : 159.1875\n",
            "Train_StdReturn : 25.223299026489258\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 119.0\n",
            "Train_AverageEpLen : 159.1875\n",
            "Train_EnvstepsSoFar : 126489\n",
            "TimeSinceStart : 143.3672206401825\n",
            "Training Loss : 376151.3125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 175.0\n",
            "Eval_StdReturn : 26.695817947387695\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 138.0\n",
            "Eval_AverageEpLen : 175.0\n",
            "Train_AverageReturn : 163.51612854003906\n",
            "Train_StdReturn : 28.013673782348633\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 113.0\n",
            "Train_AverageEpLen : 163.51612903225808\n",
            "Train_EnvstepsSoFar : 131558\n",
            "TimeSinceStart : 149.14195680618286\n",
            "Training Loss : 381818.375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 142.0\n",
            "Eval_StdReturn : 3.5590262413024902\n",
            "Eval_MaxReturn : 147.0\n",
            "Eval_MinReturn : 139.0\n",
            "Eval_AverageEpLen : 142.0\n",
            "Train_AverageReturn : 165.93548583984375\n",
            "Train_StdReturn : 23.862422943115234\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 126.0\n",
            "Train_AverageEpLen : 165.93548387096774\n",
            "Train_EnvstepsSoFar : 136702\n",
            "TimeSinceStart : 154.96738982200623\n",
            "Training Loss : 389400.40625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 139.0\n",
            "Eval_StdReturn : 18.055469512939453\n",
            "Eval_MaxReturn : 164.0\n",
            "Eval_MinReturn : 122.0\n",
            "Eval_AverageEpLen : 139.0\n",
            "Train_AverageReturn : 161.48387145996094\n",
            "Train_StdReturn : 23.722686767578125\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 118.0\n",
            "Train_AverageEpLen : 161.48387096774192\n",
            "Train_EnvstepsSoFar : 141708\n",
            "TimeSinceStart : 160.57278776168823\n",
            "Training Loss : 362593.3125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 159.3333282470703\n",
            "Eval_StdReturn : 29.318178176879883\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 132.0\n",
            "Eval_AverageEpLen : 159.33333333333334\n",
            "Train_AverageReturn : 167.06451416015625\n",
            "Train_StdReturn : 22.434040069580078\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 127.0\n",
            "Train_AverageEpLen : 167.06451612903226\n",
            "Train_EnvstepsSoFar : 146887\n",
            "TimeSinceStart : 166.5150842666626\n",
            "Training Loss : 381841.375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 178.3333282470703\n",
            "Eval_StdReturn : 18.873849868774414\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 154.0\n",
            "Eval_AverageEpLen : 178.33333333333334\n",
            "Train_AverageReturn : 166.19354248046875\n",
            "Train_StdReturn : 25.2834529876709\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 115.0\n",
            "Train_AverageEpLen : 166.19354838709677\n",
            "Train_EnvstepsSoFar : 152039\n",
            "TimeSinceStart : 172.41221475601196\n",
            "Training Loss : 385141.15625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 185.0\n",
            "Eval_StdReturn : 10.984837532043457\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 174.0\n",
            "Eval_AverageEpLen : 185.0\n",
            "Train_AverageReturn : 167.6774139404297\n",
            "Train_StdReturn : 22.804058074951172\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 118.0\n",
            "Train_AverageEpLen : 167.67741935483872\n",
            "Train_EnvstepsSoFar : 157237\n",
            "TimeSinceStart : 178.43793296813965\n",
            "Training Loss : 374073.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 168.0\n",
            "Eval_StdReturn : 19.200695037841797\n",
            "Eval_MaxReturn : 195.0\n",
            "Eval_MinReturn : 152.0\n",
            "Eval_AverageEpLen : 168.0\n",
            "Train_AverageReturn : 182.82142639160156\n",
            "Train_StdReturn : 23.47347068786621\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 119.0\n",
            "Train_AverageEpLen : 182.82142857142858\n",
            "Train_EnvstepsSoFar : 162356\n",
            "TimeSinceStart : 184.33181476593018\n",
            "Training Loss : 412521.5\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 188.37037658691406\n",
            "Train_StdReturn : 17.401891708374023\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 130.0\n",
            "Train_AverageEpLen : 188.37037037037038\n",
            "Train_EnvstepsSoFar : 167442\n",
            "TimeSinceStart : 189.99545431137085\n",
            "Training Loss : 413505.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 187.3333282470703\n",
            "Eval_StdReturn : 11.585432052612305\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 172.0\n",
            "Eval_AverageEpLen : 187.33333333333334\n",
            "Train_AverageReturn : 189.22222900390625\n",
            "Train_StdReturn : 15.788728713989258\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 151.0\n",
            "Train_AverageEpLen : 189.22222222222223\n",
            "Train_EnvstepsSoFar : 172551\n",
            "TimeSinceStart : 195.94099974632263\n",
            "Training Loss : 414704.28125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 172.6666717529297\n",
            "Eval_StdReturn : 23.795425415039062\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 142.0\n",
            "Eval_AverageEpLen : 172.66666666666666\n",
            "Train_AverageReturn : 191.11111450195312\n",
            "Train_StdReturn : 14.561067581176758\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 150.0\n",
            "Train_AverageEpLen : 191.11111111111111\n",
            "Train_EnvstepsSoFar : 177711\n",
            "TimeSinceStart : 201.81557726860046\n",
            "Training Loss : 422280.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 145.0\n",
            "Eval_StdReturn : 8.602325439453125\n",
            "Eval_MaxReturn : 156.0\n",
            "Eval_MinReturn : 135.0\n",
            "Eval_AverageEpLen : 145.0\n",
            "Train_AverageReturn : 173.13792419433594\n",
            "Train_StdReturn : 22.868022918701172\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 130.0\n",
            "Train_AverageEpLen : 173.13793103448276\n",
            "Train_EnvstepsSoFar : 182732\n",
            "TimeSinceStart : 207.41799664497375\n",
            "Training Loss : 379647.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 145.6666717529297\n",
            "Eval_StdReturn : 13.69509220123291\n",
            "Eval_MaxReturn : 165.0\n",
            "Eval_MinReturn : 135.0\n",
            "Eval_AverageEpLen : 145.66666666666666\n",
            "Train_AverageReturn : 151.3235321044922\n",
            "Train_StdReturn : 25.369874954223633\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 110.0\n",
            "Train_AverageEpLen : 151.3235294117647\n",
            "Train_EnvstepsSoFar : 187877\n",
            "TimeSinceStart : 213.25578594207764\n",
            "Training Loss : 337970.125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 142.6666717529297\n",
            "Eval_StdReturn : 19.871810913085938\n",
            "Eval_MaxReturn : 169.0\n",
            "Eval_MinReturn : 121.0\n",
            "Eval_AverageEpLen : 142.66666666666666\n",
            "Train_AverageReturn : 134.7894744873047\n",
            "Train_StdReturn : 19.354257583618164\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 94.0\n",
            "Train_AverageEpLen : 134.78947368421052\n",
            "Train_EnvstepsSoFar : 192999\n",
            "TimeSinceStart : 219.09997534751892\n",
            "Training Loss : 295208.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 137.5\n",
            "Eval_StdReturn : 21.289669036865234\n",
            "Eval_MaxReturn : 173.0\n",
            "Eval_MinReturn : 120.0\n",
            "Eval_AverageEpLen : 137.5\n",
            "Train_AverageReturn : 133.23684692382812\n",
            "Train_StdReturn : 18.744028091430664\n",
            "Train_MaxReturn : 186.0\n",
            "Train_MinReturn : 110.0\n",
            "Train_AverageEpLen : 133.23684210526315\n",
            "Train_EnvstepsSoFar : 198062\n",
            "TimeSinceStart : 224.87506937980652\n",
            "Training Loss : 289193.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 116.0\n",
            "Eval_StdReturn : 7.280109882354736\n",
            "Eval_MaxReturn : 125.0\n",
            "Eval_MinReturn : 105.0\n",
            "Eval_AverageEpLen : 116.0\n",
            "Train_AverageReturn : 129.41026306152344\n",
            "Train_StdReturn : 14.35934066772461\n",
            "Train_MaxReturn : 179.0\n",
            "Train_MinReturn : 104.0\n",
            "Train_AverageEpLen : 129.4102564102564\n",
            "Train_EnvstepsSoFar : 203109\n",
            "TimeSinceStart : 230.7105050086975\n",
            "Training Loss : 278747.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 139.3333282470703\n",
            "Eval_StdReturn : 7.586537837982178\n",
            "Eval_MaxReturn : 150.0\n",
            "Eval_MinReturn : 133.0\n",
            "Eval_AverageEpLen : 139.33333333333334\n",
            "Train_AverageReturn : 125.82499694824219\n",
            "Train_StdReturn : 13.850790023803711\n",
            "Train_MaxReturn : 160.0\n",
            "Train_MinReturn : 92.0\n",
            "Train_AverageEpLen : 125.825\n",
            "Train_EnvstepsSoFar : 208142\n",
            "TimeSinceStart : 236.51058340072632\n",
            "Training Loss : 270511.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 116.0\n",
            "Eval_StdReturn : 14.781745910644531\n",
            "Eval_MaxReturn : 141.0\n",
            "Eval_MinReturn : 103.0\n",
            "Eval_AverageEpLen : 116.0\n",
            "Train_AverageReturn : 127.07499694824219\n",
            "Train_StdReturn : 16.57767677307129\n",
            "Train_MaxReturn : 163.0\n",
            "Train_MinReturn : 81.0\n",
            "Train_AverageEpLen : 127.075\n",
            "Train_EnvstepsSoFar : 213225\n",
            "TimeSinceStart : 242.33598589897156\n",
            "Training Loss : 278945.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 100.0\n",
            "Eval_StdReturn : 13.725887298583984\n",
            "Eval_MaxReturn : 118.0\n",
            "Eval_MinReturn : 80.0\n",
            "Eval_AverageEpLen : 100.0\n",
            "Train_AverageReturn : 116.15908813476562\n",
            "Train_StdReturn : 16.038227081298828\n",
            "Train_MaxReturn : 149.0\n",
            "Train_MinReturn : 76.0\n",
            "Train_AverageEpLen : 116.1590909090909\n",
            "Train_EnvstepsSoFar : 218336\n",
            "TimeSinceStart : 248.12183475494385\n",
            "Training Loss : 257893.09375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 98.19999694824219\n",
            "Eval_StdReturn : 5.114684581756592\n",
            "Eval_MaxReturn : 105.0\n",
            "Eval_MinReturn : 90.0\n",
            "Eval_AverageEpLen : 98.2\n",
            "Train_AverageReturn : 112.4888916015625\n",
            "Train_StdReturn : 13.802773475646973\n",
            "Train_MaxReturn : 139.0\n",
            "Train_MinReturn : 77.0\n",
            "Train_AverageEpLen : 112.4888888888889\n",
            "Train_EnvstepsSoFar : 223398\n",
            "TimeSinceStart : 253.8227560520172\n",
            "Training Loss : 238750.078125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 104.75\n",
            "Eval_StdReturn : 6.684871196746826\n",
            "Eval_MaxReturn : 114.0\n",
            "Eval_MinReturn : 97.0\n",
            "Eval_AverageEpLen : 104.75\n",
            "Train_AverageReturn : 105.97916412353516\n",
            "Train_StdReturn : 14.00965404510498\n",
            "Train_MaxReturn : 139.0\n",
            "Train_MinReturn : 74.0\n",
            "Train_AverageEpLen : 105.97916666666667\n",
            "Train_EnvstepsSoFar : 228485\n",
            "TimeSinceStart : 259.6305122375488\n",
            "Training Loss : 229184.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 104.25\n",
            "Eval_StdReturn : 14.376630783081055\n",
            "Eval_MaxReturn : 125.0\n",
            "Eval_MinReturn : 85.0\n",
            "Eval_AverageEpLen : 104.25\n",
            "Train_AverageReturn : 101.87999725341797\n",
            "Train_StdReturn : 17.270366668701172\n",
            "Train_MaxReturn : 139.0\n",
            "Train_MinReturn : 59.0\n",
            "Train_AverageEpLen : 101.88\n",
            "Train_EnvstepsSoFar : 233579\n",
            "TimeSinceStart : 265.5639269351959\n",
            "Training Loss : 220727.359375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 106.75\n",
            "Eval_StdReturn : 16.207637786865234\n",
            "Eval_MaxReturn : 134.0\n",
            "Eval_MinReturn : 94.0\n",
            "Eval_AverageEpLen : 106.75\n",
            "Train_AverageReturn : 102.12000274658203\n",
            "Train_StdReturn : 12.717924118041992\n",
            "Train_MaxReturn : 130.0\n",
            "Train_MinReturn : 66.0\n",
            "Train_AverageEpLen : 102.12\n",
            "Train_EnvstepsSoFar : 238685\n",
            "TimeSinceStart : 271.267062664032\n",
            "Training Loss : 212065.03125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 83.5999984741211\n",
            "Eval_StdReturn : 13.836183547973633\n",
            "Eval_MaxReturn : 100.0\n",
            "Eval_MinReturn : 63.0\n",
            "Eval_AverageEpLen : 83.6\n",
            "Train_AverageReturn : 98.47058868408203\n",
            "Train_StdReturn : 15.464510917663574\n",
            "Train_MaxReturn : 131.0\n",
            "Train_MinReturn : 63.0\n",
            "Train_AverageEpLen : 98.47058823529412\n",
            "Train_EnvstepsSoFar : 243707\n",
            "TimeSinceStart : 276.9385437965393\n",
            "Training Loss : 207820.453125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 100.75\n",
            "Eval_StdReturn : 12.96871280670166\n",
            "Eval_MaxReturn : 113.0\n",
            "Eval_MinReturn : 79.0\n",
            "Eval_AverageEpLen : 100.75\n",
            "Train_AverageReturn : 103.32653045654297\n",
            "Train_StdReturn : 15.444948196411133\n",
            "Train_MaxReturn : 142.0\n",
            "Train_MinReturn : 80.0\n",
            "Train_AverageEpLen : 103.3265306122449\n",
            "Train_EnvstepsSoFar : 248770\n",
            "TimeSinceStart : 282.70689630508423\n",
            "Training Loss : 220156.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 120.25\n",
            "Eval_StdReturn : 13.367403984069824\n",
            "Eval_MaxReturn : 137.0\n",
            "Eval_MinReturn : 101.0\n",
            "Eval_AverageEpLen : 120.25\n",
            "Train_AverageReturn : 108.59574127197266\n",
            "Train_StdReturn : 16.453235626220703\n",
            "Train_MaxReturn : 139.0\n",
            "Train_MinReturn : 50.0\n",
            "Train_AverageEpLen : 108.59574468085107\n",
            "Train_EnvstepsSoFar : 253874\n",
            "TimeSinceStart : 288.57934045791626\n",
            "Training Loss : 230127.90625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 124.75\n",
            "Eval_StdReturn : 12.517487525939941\n",
            "Eval_MaxReturn : 138.0\n",
            "Eval_MinReturn : 109.0\n",
            "Eval_AverageEpLen : 124.75\n",
            "Train_AverageReturn : 107.85106658935547\n",
            "Train_StdReturn : 13.80869197845459\n",
            "Train_MaxReturn : 136.0\n",
            "Train_MinReturn : 82.0\n",
            "Train_AverageEpLen : 107.85106382978724\n",
            "Train_EnvstepsSoFar : 258943\n",
            "TimeSinceStart : 294.4232063293457\n",
            "Training Loss : 229449.171875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 118.75\n",
            "Eval_StdReturn : 6.684871196746826\n",
            "Eval_MaxReturn : 128.0\n",
            "Eval_MinReturn : 111.0\n",
            "Eval_AverageEpLen : 118.75\n",
            "Train_AverageReturn : 109.65217590332031\n",
            "Train_StdReturn : 10.603972434997559\n",
            "Train_MaxReturn : 137.0\n",
            "Train_MinReturn : 84.0\n",
            "Train_AverageEpLen : 109.65217391304348\n",
            "Train_EnvstepsSoFar : 263987\n",
            "TimeSinceStart : 300.2130661010742\n",
            "Training Loss : 226569.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 119.25\n",
            "Eval_StdReturn : 9.756407737731934\n",
            "Eval_MaxReturn : 129.0\n",
            "Eval_MinReturn : 103.0\n",
            "Eval_AverageEpLen : 119.25\n",
            "Train_AverageReturn : 114.5227279663086\n",
            "Train_StdReturn : 14.313834190368652\n",
            "Train_MaxReturn : 151.0\n",
            "Train_MinReturn : 87.0\n",
            "Train_AverageEpLen : 114.52272727272727\n",
            "Train_EnvstepsSoFar : 269026\n",
            "TimeSinceStart : 305.92770528793335\n",
            "Training Loss : 243719.203125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 112.0\n",
            "Eval_StdReturn : 15.41103458404541\n",
            "Eval_MaxReturn : 131.0\n",
            "Eval_MinReturn : 88.0\n",
            "Eval_AverageEpLen : 112.0\n",
            "Train_AverageReturn : 112.80000305175781\n",
            "Train_StdReturn : 12.337117195129395\n",
            "Train_MaxReturn : 136.0\n",
            "Train_MinReturn : 87.0\n",
            "Train_AverageEpLen : 112.8\n",
            "Train_EnvstepsSoFar : 274102\n",
            "TimeSinceStart : 311.7221894264221\n",
            "Training Loss : 232411.3125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 101.5999984741211\n",
            "Eval_StdReturn : 23.354656219482422\n",
            "Eval_MaxReturn : 119.0\n",
            "Eval_MinReturn : 57.0\n",
            "Eval_AverageEpLen : 101.6\n",
            "Train_AverageReturn : 103.51020050048828\n",
            "Train_StdReturn : 15.207573890686035\n",
            "Train_MaxReturn : 138.0\n",
            "Train_MinReturn : 58.0\n",
            "Train_AverageEpLen : 103.51020408163265\n",
            "Train_EnvstepsSoFar : 279174\n",
            "TimeSinceStart : 317.52294731140137\n",
            "Training Loss : 218370.78125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 105.5\n",
            "Eval_StdReturn : 7.632168769836426\n",
            "Eval_MaxReturn : 115.0\n",
            "Eval_MinReturn : 95.0\n",
            "Eval_AverageEpLen : 105.5\n",
            "Train_AverageReturn : 105.83333587646484\n",
            "Train_StdReturn : 12.315866470336914\n",
            "Train_MaxReturn : 129.0\n",
            "Train_MinReturn : 68.0\n",
            "Train_AverageEpLen : 105.83333333333333\n",
            "Train_EnvstepsSoFar : 284254\n",
            "TimeSinceStart : 323.23088574409485\n",
            "Training Loss : 224940.328125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 110.25\n",
            "Eval_StdReturn : 11.734031677246094\n",
            "Eval_MaxReturn : 124.0\n",
            "Eval_MinReturn : 95.0\n",
            "Eval_AverageEpLen : 110.25\n",
            "Train_AverageReturn : 101.4800033569336\n",
            "Train_StdReturn : 15.590047836303711\n",
            "Train_MaxReturn : 131.0\n",
            "Train_MinReturn : 54.0\n",
            "Train_AverageEpLen : 101.48\n",
            "Train_EnvstepsSoFar : 289328\n",
            "TimeSinceStart : 328.98166131973267\n",
            "Training Loss : 211756.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 102.75\n",
            "Eval_StdReturn : 10.80219841003418\n",
            "Eval_MaxReturn : 119.0\n",
            "Eval_MinReturn : 92.0\n",
            "Eval_AverageEpLen : 102.75\n",
            "Train_AverageReturn : 106.7872314453125\n",
            "Train_StdReturn : 14.926679611206055\n",
            "Train_MaxReturn : 142.0\n",
            "Train_MinReturn : 74.0\n",
            "Train_AverageEpLen : 106.7872340425532\n",
            "Train_EnvstepsSoFar : 294347\n",
            "TimeSinceStart : 334.56231236457825\n",
            "Training Loss : 223823.328125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 120.5\n",
            "Eval_StdReturn : 17.909494400024414\n",
            "Eval_MaxReturn : 141.0\n",
            "Eval_MinReturn : 95.0\n",
            "Eval_AverageEpLen : 120.5\n",
            "Train_AverageReturn : 111.3043441772461\n",
            "Train_StdReturn : 11.24213695526123\n",
            "Train_MaxReturn : 135.0\n",
            "Train_MinReturn : 89.0\n",
            "Train_AverageEpLen : 111.30434782608695\n",
            "Train_EnvstepsSoFar : 299467\n",
            "TimeSinceStart : 340.3971383571625\n",
            "Training Loss : 231163.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 123.0\n",
            "Eval_StdReturn : 12.267844200134277\n",
            "Eval_MaxReturn : 134.0\n",
            "Eval_MinReturn : 103.0\n",
            "Eval_AverageEpLen : 123.0\n",
            "Train_AverageReturn : 116.15908813476562\n",
            "Train_StdReturn : 14.414234161376953\n",
            "Train_MaxReturn : 168.0\n",
            "Train_MinReturn : 92.0\n",
            "Train_AverageEpLen : 116.1590909090909\n",
            "Train_EnvstepsSoFar : 304578\n",
            "TimeSinceStart : 346.2129371166229\n",
            "Training Loss : 243898.046875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 136.0\n",
            "Eval_StdReturn : 7.874007701873779\n",
            "Eval_MaxReturn : 143.0\n",
            "Eval_MinReturn : 125.0\n",
            "Eval_AverageEpLen : 136.0\n",
            "Train_AverageReturn : 133.5\n",
            "Train_StdReturn : 17.103015899658203\n",
            "Train_MaxReturn : 183.0\n",
            "Train_MinReturn : 108.0\n",
            "Train_AverageEpLen : 133.5\n",
            "Train_EnvstepsSoFar : 309651\n",
            "TimeSinceStart : 351.93991565704346\n",
            "Training Loss : 280247.03125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 180.0\n",
            "Eval_StdReturn : 24.041629791259766\n",
            "Eval_MaxReturn : 197.0\n",
            "Eval_MinReturn : 146.0\n",
            "Eval_AverageEpLen : 180.0\n",
            "Train_AverageReturn : 145.8000030517578\n",
            "Train_StdReturn : 19.04100799560547\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 120.0\n",
            "Train_AverageEpLen : 145.8\n",
            "Train_EnvstepsSoFar : 314754\n",
            "TimeSinceStart : 357.76950335502625\n",
            "Training Loss : 309466.5\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 179.6666717529297\n",
            "Eval_StdReturn : 11.440669059753418\n",
            "Eval_MaxReturn : 194.0\n",
            "Eval_MinReturn : 166.0\n",
            "Eval_AverageEpLen : 179.66666666666666\n",
            "Train_AverageReturn : 162.4193572998047\n",
            "Train_StdReturn : 20.963523864746094\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 126.0\n",
            "Train_AverageEpLen : 162.41935483870967\n",
            "Train_EnvstepsSoFar : 319789\n",
            "TimeSinceStart : 363.5308666229248\n",
            "Training Loss : 339506.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 185.1851806640625\n",
            "Train_StdReturn : 17.699207305908203\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 149.0\n",
            "Train_AverageEpLen : 185.1851851851852\n",
            "Train_EnvstepsSoFar : 324789\n",
            "TimeSinceStart : 369.23532700538635\n",
            "Training Loss : 378466.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 198.23077392578125\n",
            "Train_StdReturn : 6.283725738525391\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 172.0\n",
            "Train_AverageEpLen : 198.23076923076923\n",
            "Train_EnvstepsSoFar : 329943\n",
            "TimeSinceStart : 375.0240080356598\n",
            "Training Loss : 409723.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.8076934814453\n",
            "Train_StdReturn : 0.9615384340286255\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 195.0\n",
            "Train_AverageEpLen : 199.80769230769232\n",
            "Train_EnvstepsSoFar : 335138\n",
            "TimeSinceStart : 380.9189531803131\n",
            "Training Loss : 425880.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 191.0\n",
            "Eval_StdReturn : 6.68331241607666\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 184.0\n",
            "Eval_AverageEpLen : 191.0\n",
            "Train_AverageReturn : 196.07691955566406\n",
            "Train_StdReturn : 8.74828815460205\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 164.0\n",
            "Train_AverageEpLen : 196.07692307692307\n",
            "Train_EnvstepsSoFar : 340236\n",
            "TimeSinceStart : 386.94339966773987\n",
            "Training Loss : 404376.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 195.3076934814453\n",
            "Train_StdReturn : 8.137137413024902\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 175.0\n",
            "Train_AverageEpLen : 195.30769230769232\n",
            "Train_EnvstepsSoFar : 345314\n",
            "TimeSinceStart : 392.6163384914398\n",
            "Training Loss : 414743.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.38461303710938\n",
            "Train_StdReturn : 2.2027416229248047\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 190.0\n",
            "Train_AverageEpLen : 199.3846153846154\n",
            "Train_EnvstepsSoFar : 350498\n",
            "TimeSinceStart : 398.44240140914917\n",
            "Training Loss : 432121.40625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 355498\n",
            "TimeSinceStart : 404.1352162361145\n",
            "Training Loss : 425028.5625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 360498\n",
            "TimeSinceStart : 409.88750100135803\n",
            "Training Loss : 411156.1875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 365498\n",
            "TimeSinceStart : 415.5501732826233\n",
            "Training Loss : 426145.09375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 370498\n",
            "TimeSinceStart : 421.2365918159485\n",
            "Training Loss : 413436.625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 375498\n",
            "TimeSinceStart : 426.95928025245667\n",
            "Training Loss : 421310.125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 380498\n",
            "TimeSinceStart : 432.67796325683594\n",
            "Training Loss : 414823.5\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 385498\n",
            "TimeSinceStart : 438.3265678882599\n",
            "Training Loss : 417724.59375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 390498\n",
            "TimeSinceStart : 443.932053565979\n",
            "Training Loss : 410912.0\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 395498\n",
            "TimeSinceStart : 449.6547932624817\n",
            "Training Loss : 419509.84375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 196.3076934814453\n",
            "Train_StdReturn : 18.461536407470703\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 104.0\n",
            "Train_AverageEpLen : 196.30769230769232\n",
            "Train_EnvstepsSoFar : 400602\n",
            "TimeSinceStart : 455.45656657218933\n",
            "Training Loss : 435003.6875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 405602\n",
            "TimeSinceStart : 461.1580219268799\n",
            "Training Loss : 426989.375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 410602\n",
            "TimeSinceStart : 466.8514084815979\n",
            "Training Loss : 434927.0625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 195.1923065185547\n",
            "Train_StdReturn : 24.038461685180664\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 75.0\n",
            "Train_AverageEpLen : 195.19230769230768\n",
            "Train_EnvstepsSoFar : 415677\n",
            "TimeSinceStart : 472.6300218105316\n",
            "Training Loss : 438797.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 182.82142639160156\n",
            "Train_StdReturn : 42.09687042236328\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 75.0\n",
            "Train_AverageEpLen : 182.82142857142858\n",
            "Train_EnvstepsSoFar : 420796\n",
            "TimeSinceStart : 478.4417164325714\n",
            "Training Loss : 430376.5\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 190.29629516601562\n",
            "Train_StdReturn : 31.95345687866211\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 75.0\n",
            "Train_AverageEpLen : 190.2962962962963\n",
            "Train_EnvstepsSoFar : 425934\n",
            "TimeSinceStart : 484.2978072166443\n",
            "Training Loss : 438016.9375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 158.3333282470703\n",
            "Eval_StdReturn : 58.92556381225586\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 75.0\n",
            "Eval_AverageEpLen : 158.33333333333334\n",
            "Train_AverageReturn : 199.38461303710938\n",
            "Train_StdReturn : 3.076923370361328\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 184.0\n",
            "Train_AverageEpLen : 199.3846153846154\n",
            "Train_EnvstepsSoFar : 431118\n",
            "TimeSinceStart : 490.3033425807953\n",
            "Training Loss : 430846.4375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 177.0\n",
            "Eval_StdReturn : 32.526912689208984\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 131.0\n",
            "Eval_AverageEpLen : 177.0\n",
            "Train_AverageReturn : 188.9629669189453\n",
            "Train_StdReturn : 31.31971549987793\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 63.0\n",
            "Train_AverageEpLen : 188.96296296296296\n",
            "Train_EnvstepsSoFar : 436220\n",
            "TimeSinceStart : 496.2632782459259\n",
            "Training Loss : 420344.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 190.59259033203125\n",
            "Train_StdReturn : 28.178346633911133\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 76.0\n",
            "Train_AverageEpLen : 190.59259259259258\n",
            "Train_EnvstepsSoFar : 441366\n",
            "TimeSinceStart : 502.0864243507385\n",
            "Training Loss : 414711.59375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 446366\n",
            "TimeSinceStart : 507.84220480918884\n",
            "Training Loss : 401856.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 451366\n",
            "TimeSinceStart : 513.5669424533844\n",
            "Training Loss : 393646.75\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 181.3333282470703\n",
            "Eval_StdReturn : 26.398653030395508\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 144.0\n",
            "Eval_AverageEpLen : 181.33333333333334\n",
            "Train_AverageReturn : 199.8076934814453\n",
            "Train_StdReturn : 0.9615384340286255\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 195.0\n",
            "Train_AverageEpLen : 199.80769230769232\n",
            "Train_EnvstepsSoFar : 456561\n",
            "TimeSinceStart : 519.6572217941284\n",
            "Training Loss : 399340.65625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.11538696289062\n",
            "Train_StdReturn : 2.4857583045959473\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 191.0\n",
            "Train_AverageEpLen : 199.1153846153846\n",
            "Train_EnvstepsSoFar : 461738\n",
            "TimeSinceStart : 525.5146071910858\n",
            "Training Loss : 383732.8125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 466738\n",
            "TimeSinceStart : 531.1562459468842\n",
            "Training Loss : 366620.125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 471738\n",
            "TimeSinceStart : 536.8611261844635\n",
            "Training Loss : 351437.0625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 193.57691955566406\n",
            "Train_StdReturn : 24.00668716430664\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 84.0\n",
            "Train_AverageEpLen : 193.57692307692307\n",
            "Train_EnvstepsSoFar : 476771\n",
            "TimeSinceStart : 542.6671636104584\n",
            "Training Loss : 349870.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.5\n",
            "Train_StdReturn : 2.5\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 187.0\n",
            "Train_AverageEpLen : 199.5\n",
            "Train_EnvstepsSoFar : 481958\n",
            "TimeSinceStart : 548.5152900218964\n",
            "Training Loss : 358380.25\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.92308044433594\n",
            "Train_StdReturn : 0.38461536169052124\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 198.0\n",
            "Train_AverageEpLen : 199.92307692307693\n",
            "Train_EnvstepsSoFar : 487156\n",
            "TimeSinceStart : 554.2691974639893\n",
            "Training Loss : 354337.875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.88461303710938\n",
            "Train_StdReturn : 0.5769230723381042\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 197.0\n",
            "Train_AverageEpLen : 199.8846153846154\n",
            "Train_EnvstepsSoFar : 492353\n",
            "TimeSinceStart : 560.1853170394897\n",
            "Training Loss : 350363.375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 497353\n",
            "TimeSinceStart : 565.9419820308685\n",
            "Training Loss : 338669.3125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 502353\n",
            "TimeSinceStart : 571.6348440647125\n",
            "Training Loss : 348763.96875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 507353\n",
            "TimeSinceStart : 577.2592279911041\n",
            "Training Loss : 342645.875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name CartPole-v0 -n 100 -b 5000 \\\n",
        "-rtg --exp_name q1_lb_rtg_na"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_AK5O9kmq3m",
        "outputId": "3c407466-290b-40eb-94a7-de3c2d04b23c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q1_lb_rtg_na_CartPole-v0_04-02-2022_17-06-28\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 29.85714340209961\n",
            "Eval_StdReturn : 15.458964347839355\n",
            "Eval_MaxReturn : 67.0\n",
            "Eval_MinReturn : 13.0\n",
            "Eval_AverageEpLen : 29.857142857142858\n",
            "Train_AverageReturn : 25.65816307067871\n",
            "Train_StdReturn : 14.328168869018555\n",
            "Train_MaxReturn : 132.0\n",
            "Train_MinReturn : 8.0\n",
            "Train_AverageEpLen : 25.658163265306122\n",
            "Train_EnvstepsSoFar : 5029\n",
            "TimeSinceStart : 5.46908974647522\n",
            "Training Loss : -14.951866149902344\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 37.58333206176758\n",
            "Eval_StdReturn : 18.508819580078125\n",
            "Eval_MaxReturn : 76.0\n",
            "Eval_MinReturn : 19.0\n",
            "Eval_AverageEpLen : 37.583333333333336\n",
            "Train_AverageReturn : 29.61403465270996\n",
            "Train_StdReturn : 16.134037017822266\n",
            "Train_MaxReturn : 124.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 29.614035087719298\n",
            "Train_EnvstepsSoFar : 10093\n",
            "TimeSinceStart : 10.953059673309326\n",
            "Training Loss : -51.18506622314453\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 50.5\n",
            "Eval_StdReturn : 15.272523880004883\n",
            "Eval_MaxReturn : 68.0\n",
            "Eval_MinReturn : 23.0\n",
            "Eval_AverageEpLen : 50.5\n",
            "Train_AverageReturn : 40.879032135009766\n",
            "Train_StdReturn : 22.385042190551758\n",
            "Train_MaxReturn : 108.0\n",
            "Train_MinReturn : 12.0\n",
            "Train_AverageEpLen : 40.87903225806452\n",
            "Train_EnvstepsSoFar : 15162\n",
            "TimeSinceStart : 16.427634716033936\n",
            "Training Loss : -60.88793182373047\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 47.0\n",
            "Eval_StdReturn : 20.06987762451172\n",
            "Eval_MaxReturn : 84.0\n",
            "Eval_MinReturn : 18.0\n",
            "Eval_AverageEpLen : 47.0\n",
            "Train_AverageReturn : 48.07692337036133\n",
            "Train_StdReturn : 21.701068878173828\n",
            "Train_MaxReturn : 134.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 48.07692307692308\n",
            "Train_EnvstepsSoFar : 20162\n",
            "TimeSinceStart : 21.8602397441864\n",
            "Training Loss : -59.744422912597656\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 74.5\n",
            "Eval_StdReturn : 18.30072784423828\n",
            "Eval_MaxReturn : 100.0\n",
            "Eval_MinReturn : 54.0\n",
            "Eval_AverageEpLen : 74.5\n",
            "Train_AverageReturn : 54.10752868652344\n",
            "Train_StdReturn : 26.35835075378418\n",
            "Train_MaxReturn : 157.0\n",
            "Train_MinReturn : 18.0\n",
            "Train_AverageEpLen : 54.10752688172043\n",
            "Train_EnvstepsSoFar : 25194\n",
            "TimeSinceStart : 27.374410152435303\n",
            "Training Loss : -23.494461059570312\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 72.83333587646484\n",
            "Eval_StdReturn : 46.741905212402344\n",
            "Eval_MaxReturn : 171.0\n",
            "Eval_MinReturn : 37.0\n",
            "Eval_AverageEpLen : 72.83333333333333\n",
            "Train_AverageReturn : 65.90908813476562\n",
            "Train_StdReturn : 30.157670974731445\n",
            "Train_MaxReturn : 170.0\n",
            "Train_MinReturn : 21.0\n",
            "Train_AverageEpLen : 65.9090909090909\n",
            "Train_EnvstepsSoFar : 30269\n",
            "TimeSinceStart : 32.98580718040466\n",
            "Training Loss : -28.711397171020508\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 60.28571319580078\n",
            "Eval_StdReturn : 10.989790916442871\n",
            "Eval_MaxReturn : 82.0\n",
            "Eval_MinReturn : 48.0\n",
            "Eval_AverageEpLen : 60.285714285714285\n",
            "Train_AverageReturn : 70.8591537475586\n",
            "Train_StdReturn : 27.646303176879883\n",
            "Train_MaxReturn : 182.0\n",
            "Train_MinReturn : 32.0\n",
            "Train_AverageEpLen : 70.85915492957747\n",
            "Train_EnvstepsSoFar : 35300\n",
            "TimeSinceStart : 38.3994300365448\n",
            "Training Loss : -35.71420669555664\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 90.19999694824219\n",
            "Eval_StdReturn : 17.611360549926758\n",
            "Eval_MaxReturn : 111.0\n",
            "Eval_MinReturn : 59.0\n",
            "Eval_AverageEpLen : 90.2\n",
            "Train_AverageReturn : 77.64615631103516\n",
            "Train_StdReturn : 35.258358001708984\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 25.0\n",
            "Train_AverageEpLen : 77.64615384615385\n",
            "Train_EnvstepsSoFar : 40347\n",
            "TimeSinceStart : 43.92927956581116\n",
            "Training Loss : 16.14666748046875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 108.0\n",
            "Eval_StdReturn : 40.255435943603516\n",
            "Eval_MaxReturn : 154.0\n",
            "Eval_MinReturn : 53.0\n",
            "Eval_AverageEpLen : 108.0\n",
            "Train_AverageReturn : 81.54838562011719\n",
            "Train_StdReturn : 33.63642120361328\n",
            "Train_MaxReturn : 170.0\n",
            "Train_MinReturn : 32.0\n",
            "Train_AverageEpLen : 81.54838709677419\n",
            "Train_EnvstepsSoFar : 45403\n",
            "TimeSinceStart : 49.98302888870239\n",
            "Training Loss : 12.401607513427734\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 94.0\n",
            "Eval_StdReturn : 24.81128692626953\n",
            "Eval_MaxReturn : 140.0\n",
            "Eval_MinReturn : 67.0\n",
            "Eval_AverageEpLen : 94.0\n",
            "Train_AverageReturn : 77.73846435546875\n",
            "Train_StdReturn : 31.21798324584961\n",
            "Train_MaxReturn : 194.0\n",
            "Train_MinReturn : 37.0\n",
            "Train_AverageEpLen : 77.73846153846154\n",
            "Train_EnvstepsSoFar : 50456\n",
            "TimeSinceStart : 55.62457013130188\n",
            "Training Loss : 3.7004241943359375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 112.5\n",
            "Eval_StdReturn : 50.727210998535156\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 77.0\n",
            "Eval_AverageEpLen : 112.5\n",
            "Train_AverageReturn : 100.16000366210938\n",
            "Train_StdReturn : 38.058040618896484\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 42.0\n",
            "Train_AverageEpLen : 100.16\n",
            "Train_EnvstepsSoFar : 55464\n",
            "TimeSinceStart : 61.08631086349487\n",
            "Training Loss : -24.131423950195312\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 148.3333282470703\n",
            "Eval_StdReturn : 2.357022762298584\n",
            "Eval_MaxReturn : 150.0\n",
            "Eval_MinReturn : 145.0\n",
            "Eval_AverageEpLen : 148.33333333333334\n",
            "Train_AverageReturn : 121.26190185546875\n",
            "Train_StdReturn : 43.17906188964844\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 46.0\n",
            "Train_AverageEpLen : 121.26190476190476\n",
            "Train_EnvstepsSoFar : 60557\n",
            "TimeSinceStart : 66.62901878356934\n",
            "Training Loss : 6.629051208496094\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 155.6666717529297\n",
            "Eval_StdReturn : 37.187217712402344\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 109.0\n",
            "Eval_AverageEpLen : 155.66666666666666\n",
            "Train_AverageReturn : 143.22857666015625\n",
            "Train_StdReturn : 44.53190612792969\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 54.0\n",
            "Train_AverageEpLen : 143.22857142857143\n",
            "Train_EnvstepsSoFar : 65570\n",
            "TimeSinceStart : 72.15342307090759\n",
            "Training Loss : 0.7922592163085938\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 161.6666717529297\n",
            "Eval_StdReturn : 33.02860641479492\n",
            "Eval_MaxReturn : 193.0\n",
            "Eval_MinReturn : 116.0\n",
            "Eval_AverageEpLen : 161.66666666666666\n",
            "Train_AverageReturn : 162.125\n",
            "Train_StdReturn : 41.2187385559082\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 75.0\n",
            "Train_AverageEpLen : 162.125\n",
            "Train_EnvstepsSoFar : 70758\n",
            "TimeSinceStart : 77.85395169258118\n",
            "Training Loss : -19.573444366455078\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 172.3333282470703\n",
            "Eval_StdReturn : 26.836956024169922\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 136.0\n",
            "Eval_AverageEpLen : 172.33333333333334\n",
            "Train_AverageReturn : 171.53334045410156\n",
            "Train_StdReturn : 24.15054702758789\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 113.0\n",
            "Train_AverageEpLen : 171.53333333333333\n",
            "Train_EnvstepsSoFar : 75904\n",
            "TimeSinceStart : 83.52775192260742\n",
            "Training Loss : -34.19506072998047\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 137.25\n",
            "Eval_StdReturn : 21.92458724975586\n",
            "Eval_MaxReturn : 174.0\n",
            "Eval_MinReturn : 120.0\n",
            "Eval_AverageEpLen : 137.25\n",
            "Train_AverageReturn : 185.37037658691406\n",
            "Train_StdReturn : 23.579883575439453\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 121.0\n",
            "Train_AverageEpLen : 185.37037037037038\n",
            "Train_EnvstepsSoFar : 80909\n",
            "TimeSinceStart : 89.03923845291138\n",
            "Training Loss : -20.96901512145996\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 172.93333435058594\n",
            "Train_StdReturn : 26.991275787353516\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 119.0\n",
            "Train_AverageEpLen : 172.93333333333334\n",
            "Train_EnvstepsSoFar : 86097\n",
            "TimeSinceStart : 94.56283330917358\n",
            "Training Loss : -34.40095138549805\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 177.6666717529297\n",
            "Eval_StdReturn : 31.584104537963867\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 133.0\n",
            "Eval_AverageEpLen : 177.66666666666666\n",
            "Train_AverageReturn : 170.8000030517578\n",
            "Train_StdReturn : 27.10030746459961\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 118.0\n",
            "Train_AverageEpLen : 170.8\n",
            "Train_EnvstepsSoFar : 91221\n",
            "TimeSinceStart : 100.14763903617859\n",
            "Training Loss : -30.017578125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 182.2142791748047\n",
            "Train_StdReturn : 24.742034912109375\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 131.0\n",
            "Train_AverageEpLen : 182.21428571428572\n",
            "Train_EnvstepsSoFar : 96323\n",
            "TimeSinceStart : 105.76645922660828\n",
            "Training Loss : -50.36277770996094\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 193.1923065185547\n",
            "Train_StdReturn : 16.155540466308594\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 131.0\n",
            "Train_AverageEpLen : 193.19230769230768\n",
            "Train_EnvstepsSoFar : 101346\n",
            "TimeSinceStart : 111.16203331947327\n",
            "Training Loss : -6.338771820068359\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.73077392578125\n",
            "Train_StdReturn : 1.3461538553237915\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 193.0\n",
            "Train_AverageEpLen : 199.73076923076923\n",
            "Train_EnvstepsSoFar : 106539\n",
            "TimeSinceStart : 116.72649431228638\n",
            "Training Loss : 4.18438720703125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 111539\n",
            "TimeSinceStart : 122.12566018104553\n",
            "Training Loss : 84.88375854492188\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 116539\n",
            "TimeSinceStart : 127.60103297233582\n",
            "Training Loss : 88.40724182128906\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 121539\n",
            "TimeSinceStart : 132.9599642753601\n",
            "Training Loss : 93.07659149169922\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 126539\n",
            "TimeSinceStart : 138.35073709487915\n",
            "Training Loss : 71.71144104003906\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 131539\n",
            "TimeSinceStart : 143.81486988067627\n",
            "Training Loss : 58.14474868774414\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 136539\n",
            "TimeSinceStart : 149.2099883556366\n",
            "Training Loss : 83.42762756347656\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 141539\n",
            "TimeSinceStart : 154.73924565315247\n",
            "Training Loss : 80.88597869873047\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 146539\n",
            "TimeSinceStart : 160.12431812286377\n",
            "Training Loss : 83.86727905273438\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 151539\n",
            "TimeSinceStart : 165.48895835876465\n",
            "Training Loss : 77.91504669189453\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 156539\n",
            "TimeSinceStart : 170.86881756782532\n",
            "Training Loss : 111.4225845336914\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 161539\n",
            "TimeSinceStart : 176.29410886764526\n",
            "Training Loss : 65.5816421508789\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 166539\n",
            "TimeSinceStart : 181.62098503112793\n",
            "Training Loss : 89.54704284667969\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 171539\n",
            "TimeSinceStart : 187.03304076194763\n",
            "Training Loss : 105.29299926757812\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 176539\n",
            "TimeSinceStart : 192.42587065696716\n",
            "Training Loss : 69.64445495605469\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 181539\n",
            "TimeSinceStart : 197.74528217315674\n",
            "Training Loss : 83.30875396728516\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 186539\n",
            "TimeSinceStart : 203.09725689888\n",
            "Training Loss : 67.78364562988281\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 191539\n",
            "TimeSinceStart : 208.45903372764587\n",
            "Training Loss : 88.2174072265625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 196539\n",
            "TimeSinceStart : 213.80493140220642\n",
            "Training Loss : 101.18619537353516\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 201539\n",
            "TimeSinceStart : 219.29569101333618\n",
            "Training Loss : 125.2761001586914\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 206539\n",
            "TimeSinceStart : 224.6604082584381\n",
            "Training Loss : 81.4107437133789\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 211539\n",
            "TimeSinceStart : 230.00337982177734\n",
            "Training Loss : 71.56184387207031\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 216539\n",
            "TimeSinceStart : 235.31712937355042\n",
            "Training Loss : 68.91944885253906\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 221539\n",
            "TimeSinceStart : 240.66070771217346\n",
            "Training Loss : 79.56194305419922\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 226539\n",
            "TimeSinceStart : 245.94515657424927\n",
            "Training Loss : 92.4443359375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 231539\n",
            "TimeSinceStart : 251.3004322052002\n",
            "Training Loss : 82.57289123535156\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 236539\n",
            "TimeSinceStart : 256.6262423992157\n",
            "Training Loss : 113.08785247802734\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 241539\n",
            "TimeSinceStart : 261.91614174842834\n",
            "Training Loss : 53.462852478027344\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 246539\n",
            "TimeSinceStart : 267.2850253582001\n",
            "Training Loss : 75.41378784179688\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 251539\n",
            "TimeSinceStart : 272.5920078754425\n",
            "Training Loss : 45.98012924194336\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 256539\n",
            "TimeSinceStart : 277.86459255218506\n",
            "Training Loss : 59.8517951965332\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 261539\n",
            "TimeSinceStart : 283.2401192188263\n",
            "Training Loss : 85.79449462890625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 266539\n",
            "TimeSinceStart : 288.6438992023468\n",
            "Training Loss : 76.09854125976562\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 271539\n",
            "TimeSinceStart : 293.9539668560028\n",
            "Training Loss : 110.63101196289062\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 276539\n",
            "TimeSinceStart : 299.2934567928314\n",
            "Training Loss : 88.24662780761719\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 281539\n",
            "TimeSinceStart : 304.6849534511566\n",
            "Training Loss : 79.0805892944336\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 286539\n",
            "TimeSinceStart : 309.9893653392792\n",
            "Training Loss : 70.47989654541016\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 196.3333282470703\n",
            "Eval_StdReturn : 5.185449600219727\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 189.0\n",
            "Eval_AverageEpLen : 196.33333333333334\n",
            "Train_AverageReturn : 199.73077392578125\n",
            "Train_StdReturn : 1.3461538553237915\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 193.0\n",
            "Train_AverageEpLen : 199.73076923076923\n",
            "Train_EnvstepsSoFar : 291732\n",
            "TimeSinceStart : 315.6633970737457\n",
            "Training Loss : -16.344247817993164\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 198.88461303710938\n",
            "Train_StdReturn : 3.456620454788208\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 183.0\n",
            "Train_AverageEpLen : 198.8846153846154\n",
            "Train_EnvstepsSoFar : 296903\n",
            "TimeSinceStart : 321.17608189582825\n",
            "Training Loss : -8.454654693603516\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.5\n",
            "Train_StdReturn : 2.5\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 187.0\n",
            "Train_AverageEpLen : 199.5\n",
            "Train_EnvstepsSoFar : 302090\n",
            "TimeSinceStart : 326.6148066520691\n",
            "Training Loss : -31.187313079833984\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 307090\n",
            "TimeSinceStart : 332.03838658332825\n",
            "Training Loss : 50.569034576416016\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 312090\n",
            "TimeSinceStart : 337.3884425163269\n",
            "Training Loss : 88.63755798339844\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 317090\n",
            "TimeSinceStart : 342.8784601688385\n",
            "Training Loss : 6.187535762786865\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 322090\n",
            "TimeSinceStart : 348.13404846191406\n",
            "Training Loss : 38.35913848876953\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 327090\n",
            "TimeSinceStart : 353.534259557724\n",
            "Training Loss : 76.71174621582031\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 332090\n",
            "TimeSinceStart : 358.93510842323303\n",
            "Training Loss : 95.58694458007812\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 337090\n",
            "TimeSinceStart : 364.2942147254944\n",
            "Training Loss : 8.264545440673828\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 342090\n",
            "TimeSinceStart : 369.5724575519562\n",
            "Training Loss : 50.72811508178711\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 347090\n",
            "TimeSinceStart : 374.96034932136536\n",
            "Training Loss : 12.745805740356445\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 352090\n",
            "TimeSinceStart : 380.3579478263855\n",
            "Training Loss : 11.308767318725586\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 357090\n",
            "TimeSinceStart : 385.7170321941376\n",
            "Training Loss : -31.44483757019043\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 362090\n",
            "TimeSinceStart : 391.18378043174744\n",
            "Training Loss : -10.66861343383789\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 367090\n",
            "TimeSinceStart : 396.59934735298157\n",
            "Training Loss : -30.258068084716797\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 372090\n",
            "TimeSinceStart : 402.08894777297974\n",
            "Training Loss : -113.46687316894531\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 377090\n",
            "TimeSinceStart : 407.5940194129944\n",
            "Training Loss : -3.134505271911621\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 196.57691955566406\n",
            "Train_StdReturn : 17.11538314819336\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 111.0\n",
            "Train_AverageEpLen : 196.57692307692307\n",
            "Train_EnvstepsSoFar : 382201\n",
            "TimeSinceStart : 413.2504994869232\n",
            "Training Loss : -7.558603286743164\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 387201\n",
            "TimeSinceStart : 418.6848261356354\n",
            "Training Loss : -97.63068389892578\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 392201\n",
            "TimeSinceStart : 424.05610179901123\n",
            "Training Loss : -46.346405029296875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 397201\n",
            "TimeSinceStart : 429.4473707675934\n",
            "Training Loss : -35.83935546875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 402201\n",
            "TimeSinceStart : 434.8232231140137\n",
            "Training Loss : -146.14633178710938\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 407201\n",
            "TimeSinceStart : 440.26888632774353\n",
            "Training Loss : -145.96615600585938\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 159.3333282470703\n",
            "Eval_StdReturn : 57.5113525390625\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 78.0\n",
            "Eval_AverageEpLen : 159.33333333333334\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 412201\n",
            "TimeSinceStart : 445.73501420021057\n",
            "Training Loss : -147.23492431640625\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 417201\n",
            "TimeSinceStart : 451.1880955696106\n",
            "Training Loss : -150.4266357421875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 422201\n",
            "TimeSinceStart : 456.5962471961975\n",
            "Training Loss : -147.68875122070312\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 195.11538696289062\n",
            "Train_StdReturn : 24.423076629638672\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 73.0\n",
            "Train_AverageEpLen : 195.1153846153846\n",
            "Train_EnvstepsSoFar : 427274\n",
            "TimeSinceStart : 462.07252526283264\n",
            "Training Loss : -50.390846252441406\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 195.26922607421875\n",
            "Train_StdReturn : 23.653844833374023\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 77.0\n",
            "Train_AverageEpLen : 195.26923076923077\n",
            "Train_EnvstepsSoFar : 432351\n",
            "TimeSinceStart : 467.65238308906555\n",
            "Training Loss : -42.51190185546875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 437351\n",
            "TimeSinceStart : 473.0892279148102\n",
            "Training Loss : -150.28482055664062\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 442351\n",
            "TimeSinceStart : 478.5286588668823\n",
            "Training Loss : -147.77516174316406\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 447351\n",
            "TimeSinceStart : 483.93540024757385\n",
            "Training Loss : -82.59574890136719\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 452351\n",
            "TimeSinceStart : 489.27967643737793\n",
            "Training Loss : -91.76374816894531\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 457351\n",
            "TimeSinceStart : 494.7113404273987\n",
            "Training Loss : -95.92726135253906\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 462351\n",
            "TimeSinceStart : 500.0469856262207\n",
            "Training Loss : -138.46942138671875\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 467351\n",
            "TimeSinceStart : 505.5139660835266\n",
            "Training Loss : -135.07301330566406\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 472351\n",
            "TimeSinceStart : 510.8987271785736\n",
            "Training Loss : -133.8439483642578\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 477351\n",
            "TimeSinceStart : 516.3011445999146\n",
            "Training Loss : -134.78106689453125\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 482351\n",
            "TimeSinceStart : 521.7630593776703\n",
            "Training Loss : -75.20435333251953\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 487351\n",
            "TimeSinceStart : 527.1605072021484\n",
            "Training Loss : -132.0423583984375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 200.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 200.0\n",
            "Train_AverageEpLen : 200.0\n",
            "Train_EnvstepsSoFar : 492351\n",
            "TimeSinceStart : 532.4813299179077\n",
            "Training Loss : -131.2044677734375\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.88461303710938\n",
            "Train_StdReturn : 0.5769230723381042\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 197.0\n",
            "Train_AverageEpLen : 199.8846153846154\n",
            "Train_EnvstepsSoFar : 497548\n",
            "TimeSinceStart : 538.0454337596893\n",
            "Training Loss : -126.42959594726562\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 200.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 200.0\n",
            "Eval_MinReturn : 200.0\n",
            "Eval_AverageEpLen : 200.0\n",
            "Train_AverageReturn : 199.03846740722656\n",
            "Train_StdReturn : 2.4726314544677734\n",
            "Train_MaxReturn : 200.0\n",
            "Train_MinReturn : 191.0\n",
            "Train_AverageEpLen : 199.03846153846155\n",
            "Train_EnvstepsSoFar : 502723\n",
            "TimeSinceStart : 543.6402552127838\n",
            "Training Loss : 14.53679084777832\n",
            "Initial_DataCollection_AverageReturn : 25.65816307067871\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results"
      ],
      "metadata": {
        "id": "z0eIOrQ_oD1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "figsize=(5.7, 3)\n",
        "export_dir = os.path.join('solution', 'plots')\n",
        "\n",
        "\n",
        "sns.set_theme()\n",
        "sns.set_context(\"paper\")"
      ],
      "metadata": {
        "id": "PJdos2b-okYm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_section_results(file):\n",
        "    \"\"\"\n",
        "        requires tensorflow==1.12.0\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    Y = []\n",
        "    for e in tf.compat.v1.train.summary_iterator(file):\n",
        "        for v in e.summary.value:\n",
        "            if v.tag == 'Train_EnvstepsSoFar':\n",
        "                X.append(v.simple_value)\n",
        "            elif v.tag == 'Eval_AverageReturn':\n",
        "                Y.append(v.simple_value)\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "86ZDGxqZolz7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_q1_data(batch):\n",
        "    full_data = pd.DataFrame()\n",
        "\n",
        "    for folder in os.listdir('data'):\n",
        "        split = folder.split('_')\n",
        "        if 'CartPole-v0' in split and batch in split:\n",
        "            config_list = split[split.index(batch):split.index('CartPole-v0')]\n",
        "            # print('_'.join(config))\n",
        "            config = '_'.join(config_list)\n",
        "\n",
        "            logdir = os.path.join('data', folder, 'events*')\n",
        "            eventfile = glob.glob(logdir)[0]\n",
        "\n",
        "            X, Y = get_section_results(eventfile)\n",
        "            data = pd.DataFrame({'Iteration': range(len(X)), \n",
        "                                 'Config': np.repeat(config, len(X)), \n",
        "                                 'Train_EnvstepsSoFar': X, \n",
        "                                 'Eval_AverageReturn': Y})\n",
        "            data['Eval_AverageReturn_Smooth'] = data['Eval_AverageReturn'].ewm(alpha=0.6).mean()\n",
        "            full_data = pd.concat([full_data, data], axis=0, ignore_index=True)\n",
        "        \n",
        "    return full_data\n",
        "\n",
        "data_lb = read_q1_data('lb')\n",
        "data_lb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "HpKnw9wgoB1I",
        "outputId": "982b9651-4a97-49d3-f1e2-6db12c5afdfc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/summary/summary_iterator.py:31: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-446a00e1-a481-4637-ae96-51e6c837962b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iteration</th>\n",
              "      <th>Config</th>\n",
              "      <th>Train_EnvstepsSoFar</th>\n",
              "      <th>Eval_AverageReturn</th>\n",
              "      <th>Eval_AverageReturn_Smooth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>lb_no_rtg_dsa</td>\n",
              "      <td>5029.0</td>\n",
              "      <td>33.333332</td>\n",
              "      <td>33.333332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>lb_no_rtg_dsa</td>\n",
              "      <td>10049.0</td>\n",
              "      <td>31.538462</td>\n",
              "      <td>32.051282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>lb_no_rtg_dsa</td>\n",
              "      <td>15058.0</td>\n",
              "      <td>52.500000</td>\n",
              "      <td>45.159434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>lb_no_rtg_dsa</td>\n",
              "      <td>20062.0</td>\n",
              "      <td>60.857143</td>\n",
              "      <td>54.825511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>lb_no_rtg_dsa</td>\n",
              "      <td>25080.0</td>\n",
              "      <td>68.833336</td>\n",
              "      <td>63.317161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>95</td>\n",
              "      <td>lb_rtg_na</td>\n",
              "      <td>482351.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>199.999935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>96</td>\n",
              "      <td>lb_rtg_na</td>\n",
              "      <td>487351.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>199.999974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>97</td>\n",
              "      <td>lb_rtg_na</td>\n",
              "      <td>492351.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>199.999990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>98</td>\n",
              "      <td>lb_rtg_na</td>\n",
              "      <td>497548.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>199.999996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>99</td>\n",
              "      <td>lb_rtg_na</td>\n",
              "      <td>502723.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>199.999998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-446a00e1-a481-4637-ae96-51e6c837962b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-446a00e1-a481-4637-ae96-51e6c837962b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-446a00e1-a481-4637-ae96-51e6c837962b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Iteration         Config  ...  Eval_AverageReturn  Eval_AverageReturn_Smooth\n",
              "0            0  lb_no_rtg_dsa  ...           33.333332                  33.333332\n",
              "1            1  lb_no_rtg_dsa  ...           31.538462                  32.051282\n",
              "2            2  lb_no_rtg_dsa  ...           52.500000                  45.159434\n",
              "3            3  lb_no_rtg_dsa  ...           60.857143                  54.825511\n",
              "4            4  lb_no_rtg_dsa  ...           68.833336                  63.317161\n",
              "..         ...            ...  ...                 ...                        ...\n",
              "295         95      lb_rtg_na  ...          200.000000                 199.999935\n",
              "296         96      lb_rtg_na  ...          200.000000                 199.999974\n",
              "297         97      lb_rtg_na  ...          200.000000                 199.999990\n",
              "298         98      lb_rtg_na  ...          200.000000                 199.999996\n",
              "299         99      lb_rtg_na  ...          200.000000                 199.999998\n",
              "\n",
              "[300 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=figsize)\n",
        "sns.lineplot(data=data_lb, x='Iteration', y='Eval_AverageReturn_Smooth', hue='Config')\n",
        "\n",
        "#plt.savefig(os.path.join(export_dir, 'q1_lb.pdf'), bbox_inches='tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ivmNj4Xevv30",
        "outputId": "3074c12b-cbe3-4366-e662-35de3b4ff2da"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa51f9aee90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADQCAYAAAAXmZofAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eXQc5Zmo/1RV791qdWvfLEuyZcu7wQsG45hgDCYQwhJCQnDWC9kgQBbnkCHJzTaZzJCbmMkkkGGSXCAYuGGZgR8hBDC7jQ22MV5lWfveknrfu6p+f7QlW1ZLakktW5LrOUfnqGv56q3q6re+eldBVVUVDQ0NDY1pj3i2BdDQ0NDQyAyaQtfQ0NCYIWgKXUNDQ2OGoCl0DQ0NjRmCptA1NDQ0ZgiaQtfQ0NCYIejOtgATxeXyj2s/h8OCxxPKsDRTD+08Zx7nyrmeK+cJYzvX/PysYddpM3QNDQ2NGYKm0DU0NDRmCJpC19DQ0JghTKoNfe/evfzLv/wLer0ei8XCfffdRyKRYMuWLQSDQS666CLuuOMOALZv384DDzyAIAh8//vfZ+nSpZMpmoaGhsaMY1IVeklJCX/+858xm81s27aNv/zlL/h8Pm644QauvPJKbrvtNurq6qisrGTr1q08+uijBINB7rrrLrZt2zaZomloaGjMOCZVoRcWFg78r9frkSSJPXv2cOeddwJwySWXsHv3bgRBoKKiApvNhs1mI5FIEI1GMRqNkyneOYOiKrQFOugKufBGfXhjPiKJCFE5RlyOk1BlFFVBVhVUVTmxT3/NNpXpUL1NpxNJJJRh18uKQjQmk5BVErKCrKgoioqqqqgqqICqqmTbjNjM+jMnOFBgzuMTc6/EbjgZvaCqKoF4kM5gF50hF76oj3AiQigRBkkhFI0iK8nvTUFFVRVUkufCiW9MBWJxmbiskEgoJGQVRU2et6L2b5s81skDg0QCnRpHQkZEQURFQEEAhOSVQjh1h8m6LlGB9b0jW4UP6xZwSLfoFGlUZClAXOdDlsLIUghVjKMICVQhAYKCigKCysl7++S9PhwSCXTISGoCCQVh4Lr0X4/kvuleF1EVuKb6JhbPO3/E7cbKGQlbdLvdPPbYYzz00EM899xzmEwmAOx2O62trXi9Xux2+8D2drsdj8cz6IEwHA6HZVwySZI47n2nA6qq8m7bXt4+vIsjPccJxyPkW3NxmrJxmOxYDGaydFYMoh6dpEMSRMQTfwKAICD2356CcMqNOjURRAFVGfwjCkUSHGrso80VoNcbRi+JWM0GrCYdJqOEXiehkwQkUUQUBDr7Qog+gbU15SMeS1WhscNLbbOHi5aWkGUZ/wNABXa37ePnu37F55beyOKCGt5qfpftjTvoCHShF3UUZxWSY3JgNVjIslgw641INh06UUIURUREBEFI/gHdfWEaOny0dPoJhOOYDDpsZj3ZRgXRaEavF9FJyXMWRQFREDDG3GT5GzCHOhFUBVkyIeutyKIeVdKjCjpUQUIVRFRBBIQT2ks4RXVl7i7xCVEOGl2ss57PcHefzXuc1WoLeQsvpyfeQUPkEJ3RRgKKF6NgwSplYRNtGEQTOsGGTtAjISEA5nAPMXMhiNIJyfuPcfI/fdSLOdiOKdiOLhFCFfUkdBZknRlF1KGKuhPXRTjlmggD12Gk6yKKEtWVcwZ0UKb00aQr9HA4zJ133sm9995LTk4OZrN5YPbt9/vJzs4mOzsbv/9kPLnf78fhcKQ1/njjVGdyjGt3qIcnjj5Dva+JDZVruSB/FXMcFZh15rMt2qRx6vepqirvHuri6ZdqKcotZv38AhavzaE034ogDK903jvSzWMv17LmYxcMu837R108/cZx3H4jglCEaVYVa8pLJiT7BbmreKNtBw/t3UZcjlNgyWdtyWqW5i0i1+xEFAbPUoe7dxOywpOv1rF9bxtL51Rx/cI8ls3JI8skEn3nL8QPv4Z5053oypcP7CO724m8/hCKqxFd5Ur0y69DyqtAMNkmdE4TxR3x8ME7/0zJ+ivINtpTbhM/9g7B95+l17yXV1yvszRvEZ+YfTkLcqpxmlLrD8XTQXj7f6K46jGt/zL6+euGbKMqCtF3nyB+5CWk4hp0Cz+ObvZyBHP2iPfPeOj/HjMVhz6pCj2RSHD33XezefNmzj8/+WqxYsUKXn/9dS6//HLeeOMNvvWtbzF79mwaGxsJhUIEg0EkSdLMLePk/a4PePjwEyzMmc8PLvg2VUWlM/bBlYpoTOa/XjjM/roebvzoXD56filimj/C2UVZeAIxvIEo2bah99/eYy4e+O8DXHXhbDaumsXDLx6lxRWYsMyiIHJJ2VqW5S3CG/MxO2vWmBVHny/C7589gCcQ4/ubV1BZnFSCSshL+P/7D5RAL/qFlxJ+9Q9Yr//fiPYClEAv4Rf+DamwGvNl30C05U74XDJFttGOXtThCvcOq9B7jAb+KztBpHMv31x+K9XOOSOOGTu0neiOx9DNPg/RUUy84b0hCl2NhQm/+gCKqwHLJ+5FKhh5zKnGpCr0559/nvfee49gMMjDDz/M+vXrufXWW9myZQt/+tOfWLNmDdXV1QDcfvvtfPGLX0QQBO65557JFGvG4ol62Xb0KT5RtYlLyz9ytsU5K/zP2w00d/n5319aTVHO2F5h87JNWE06Gjv9LJs7WKG3ugL84blD3PjRuVy+ahYAZQU2Djf2DTteU6efV/e08oUra9JS0E6TY9iZ5UhE4zI/f+R9yvJt3HnjsgEfgBoNEnr2J4hZeViu+xGCKQs15CH8j99ivuIuwi/ch5hbjunSryCcMD1MFURBJN+aiyvcy1xH5ZD1iqrw5/bXccZlvrDwc1idZSOOJ7saiL79CKaP3op+7oUkOo4S/v/+DTUWRjAk31zVWJjQc/8Mqorluh9NqQdcukyqQr/22mu59tprhyx/6KGHhizbsGEDGzZsmExxZjxP1v43ZbYSLpl18dkW5azQ2Rfipd0t3HXjsjErcwBBECgvzKKpy8+yuXkDywPhOPf/dT+r5hewceVJxTEr38Y/dregquoQhd3U6ee+x/cSjCT45CVzyLIYxn9io/Dq+61IosAdNyxBJ500z8QOvIygM2C+6rsIYvKnbrrkywSf+THBJ+9Byi3HfNnXp5wy76fAmkdPuDfluh0du+mL+flSXxRjyAfO4cdRVYXIO39BP28t+rkXAiAVViMYLSSa9w0six/ejhqLYL3hJwNKfrqhJRbNEPZ2f8ih3iPcXHPDEJvrucLjrxxj6ZxcFlXmjHuM2UVZNHUOrg/0pxcOk20zsPmK+YMUd1mBlUA4jjcYG7R9vzJfVVMAgCcweH0mCUUSvLCziWvWVg5S5mosROzDv2M4/5oBZQ4gGCyYN96BruJ8zJvuQtBNXdNmoTU/pUIPJyI8d/zvXFW1EZstD8XXPeI4iWM7UPraMKz65MAyQRTRVawgUf8eAGoiRmz/3zEs+9i0VeagKfQZQSge4onaZ7iq8nIKLPlnW5yzwntHujjU6OamDdUTGqeiKDlD78cXjLGvrodbNs5Hrxv8c8m1mzAbJVq7T9rRA+E49z2+lzULi9h8xXysJh2eQHRCMo3ES7ubybIYuHDx4Iiw2IGXEcx2dFVDHbxSThnmS7+CYLROmlyZoNCWhyuFQv9746tYDVbWlaxBtBeg+l3DjqHGwkTffRLjimsRLdmD1umqVpFo+RA1HiVe+zYA+nlrM3sSZ5i0TS6qqrJz505aW1tRlJPxvjfddNOkCKaRPm+1v4tVb+XSWUM99ucCCVnhT88fYtMFsyhwTGx2Nbswiz5fFF8oht1iYM8xF3nZJsoLh0Z9CIJAab6NFleAxVVJe+uB+l50OpHPbKxGEAQcWUY8/slR6IFwnJd2t/CFK2uQxFNn52FiH/4d04U3I4jTd85WaM2nJzRYobtCvWxveZOvLP0CkighZOWj+IZX6LG9zyEYregXDzXnSsXzEXQGEs37iH3wAoalVyDoJs80diZIW6HffvvtyLLMwoULEafxTTITOdpXx/L8xUhT1BYK/ckrKsIkmINqWzz0eSNctaZiwmPlO82YjRJNnX6WVOXy/pFuVs4vGNapOSvfNmiG/mF9H0sqcwciaxw246TN0P+2s4l8h5mVJ0w7/cQOvYpgtKKbu2ZSjnumKLDmEUyECMXDWPTJB/Wb7TuoclSyMHc+AKI9n3jH0ZT7q6pK/OibGC/67CCzUz+CKKGrOI/oO4+hynH0Cz46eSdzhkhboTc1NfH8889Ppiwa4yCuJDjubeSKiql9M8b2PkfsgxfQlSxAKluEfs6ajMU617V6mVfuwGiY+ANNFATKC5J29MpiO4ebPFy/fvjQtbICG9v3tAHJ7NqDDb189vL5A+sdNsOk2NBVVeWdg53c9NG5g8IyVUUhvv9FjKtvnLLOznTJt+QgINAT7qVcn3RG13saWZa/eGAb0V6AMozJRfV2oUb8SCULhj2GrnIV8aNvJn0N09h23k/a06ULL7yQ999/fzJl0RgHjd5mVFQq7bPT3scXjBGNy5Mo1VDkjqPoypch2AuI7f87kdf/K2NjH2v1sKBi/I7Q0+l3jO6tdeHMMlBRNHwix6x8Gx29QRKyQktXAH84zsKKkyEXkzVDd/ujeAMxqssGhzkqvk7UiB9dRWZTys8GekmPw5g9YEePy3Ga/W3McVQMbCNmFUA0iBoNDtlf7qxFyC4aYjs/Fal0Ifqa9RgWX55x+c8Go87QL774ZAjcI488QnZ2Nnr9yVTnt956a3Ik00iLWncd+boS7rp/B1aTjiyLgdJ8K+uWFjO3NPWN/PtnD7BkTi4fW5P+Q2AiqKqK3NOI+aO3oitfjly1itD//AIl7EM0p04aSRdZUahr93HDhnkZkjbpGN1T6yKWUFgxgrkFoDTfiqyodPaF+LC+lzkl2VhNJ38fDpuRQyPEqo+Xhg4/doueHPvgKBXF1YiQlX/WMz0zRb7lZOhik78VAZiVdTJ0VMjKBQQUnwspf7CTN9F5DF3RyE5yQdJh+sgXMy32WWNUha4p7KnNUfdx9JECKoqyWL+8BH8oztFmN//62F4KnGZuvXYJFafc6Kqq0twdoDj3zNWxUf0uiAYR8yoAEAvmIGTlkTi+C8PiyyY0dmt3kFhcZn65k2g4M6aN2UVZ9HgjuP1RPr62YsRtzUYdedkmWrsDHKjvZXHV4DeF5Aw98yaXxk4fFcX2IQ8buacJKe/MPKjPBPnmnIEZer23kXJ7GfpTwzAlPYItB8XfjZRfMWhfubMW43lXn0lxzzppm1xuueWWtJZpnDmicoxGXzMxj5OacgerFxSyYUUZX79uCb/6xlrKC7P47zeOD9rHE4gRjiZwT1LkRSrknkYEqxPRkjQPCIKAvvoi4sfenvDYta0eyguyMBszlyNXmGPBaJCwWw1UlYz+BlGWb6O21Utdm48lVYOzCx1ZBryBGIqS2aqEDR2+gfT+U1F6GgcenDOBPHPuwAy93ttEVXbFkG3EFJEuSsiD6utCGmWGPtMYVaF7PB7q6+txu900NjbS0NBAQ0MDH374Ib29qbO4NM4Mxz0N6EUdvZ1GSvIGv27arQZWLyigod03qDxqW08yIsM9ibHRp6O4GpFOUzL66gtRXA3InvYJjX2s1Ut12fA20vEgCgKVRVmsmJ+fVh2YsgIb7xzowGLSMfs0e7vTZkRRVfyhzM3SFUWlscNPZfHgY6mqMuNm6HnmZPq/qqo0eJuoyh56bqI9H/W05CK58xiC2Y5gH71i60xi1GnN9u3befrpp2lvb+cHP/jBwHKbzcbdd989qcJpjEyt+zgV9gr2BhJDFDpAeUEWvmAMTyCGMytpa21zBRFg0mKjUyH3NCIVzR+0TLQXIBXNI1H7DtLqTw6z58ioqsqxVg83X5Y5+3k/X7lmUdpRM7MKbMTiCudXD30A2K3JuGZPIJay4Nd46OwNEoomqDhthq76uiEeQTzN9DCdyTfn4o36aA92EogHU87QBXsBcvvhQcvkzmPJ9P4MV0ec6oyq0K+77jquu+46Xn75ZS67bGL2To3MctRdR4WpBkkUKHAODbnKsRuxmvS0dPtPKvSeIOUnojgSsjIoXXwyUFUV2dWIYckVQ9bpqi8itud/MKy6flzx6S5vBG8gNqzzdyKMRfmWnfBRnG4/B9BJIlkWPe5AlNkMHy0zFo61esjLNmE/rT6M7GpEsOUimjJznKlAnjkXFZVdnXvIN+eSZRjq7BWz8on73hi0TO46hn7O9I7DHw9p/4ouvvhiHnjgAW677TZuu+02HnzwQSKRyGTKpnEarlAv/+f93/F8/d855q6nxd+GKVpAUa5lUKZgP4IgUFlip7nrZOJLe0+QxSdqnUxmSno/qt8FsVBKu66+ahVqxIc8TGLIaBxrSSq2/ofV2aLQaeHS80sHFfQ6lUyHLta1eofMzmHmOUQBzDoTNr2V3Z17U87OIfm2pwZ6UZUEAGo8gtLThFSc+Te3qU7aCv2ee+7B7/fz7W9/m29/+9sEAgGtzO0ZpsnfQnuwizpPA1v3PohVbyHosVCawtzST0WxneYTmYyqqtLWE2R+uQNJFPD4Mx99kWg/TPjl3w20spNd/Q7RobNowWhFV7aERNO+cR0raT8fe7nZTCOKArdcPn9QuOKpOGxGvBmMdKlr9Qyxn8PMc4j2k2/OxRvzMWcEhY6qoAaS4aFydz1IesTckTtPzUTSDg2or6/n17/+9cDn+fPn84lPfGLEfeLxOJs3b6auro6f/exnbNq0iX/913/lww8/BKC2tpZ//ud/ZsOGDVx22WUUFxcDSTPP9ddfP57zmdF4ol5KrEXcdf5X8ccChBNhHnmujeoRTA6VJXZ2H+4CoNcXIRqTKcu34bAZJsUxGj/4ComG90hUnI9+7hqUnqEO0VMRc8uRu48Pu34kjrV62HiiNvlUJpktmplrLSsKDe1ePnFRxaDlyVj/JgxLN2XkOFOJPHMuDb5mKlM4RAEwWkFvRvF1I9oLkDtrkQrnTPtM2fGQtkK32WyD7OivvfYaNtvIyQs6nY7777+fJ554YmDZli1bAJBlmY997GMDiUsmk4lHHnlkzCdwLuGJeHGakso7y2Ajy2CjvaeW9cuGb4FWUZxNd1+ISCxBe08Qq0lHttWAI8uI25dZk5kaj5Bo3o9Uuojo7qfQVa5EdjUgldQMu4/oLCV+9M0xH8sfitHRG5oSM/TRcNiMNHf5R98wDdpcQeIJZUg0zUimrelOnjkXs85MkbUg5XpBEBDt+SSa9pFo3Eu87h0Myz52hqWcGqRtcvnFL37Bk08+ybp161i3bh3btm3jF7/4xYj7CIJAQUHqL2Hnzp0sX758oNVc/2z+a1/7Gi0tLWM4hZmD4u8Z3IH9NDxR76B2XKFIMp68eASTS1mBDVEUaHUFaesJUpKX7KvpzDJlfIaeaP4AQW/EvPF2SMSIH96ejHAZaYbuLEEN9qHGwmM6Vm2LhyyLnpIzmCA1XhxZmUsuauz0U5pvGxJ3L/c0IlgcI6a5T1eW5S/mqsqNI9b5Fx0lxA9tRwn0Ylr3BQzLrjyDEk4d0p6hl5eX84c//CFjB37++ee56qqrBj5v27aNnJwcduzYwY9//OOUXY1SMd5O2Znqsp0plFiEtoe+h/OyL2FbvjHlNn7Zz2LHvAG5u5rdSKLA/MrcYaNVJEmkvDCLHn8UlzdKVakDh8NCUa4VTyCa0WvQ0/I+lvkX4CzIRb/uU3i2PwzxKM45C5CsqY+jZlUREkQsiT6MBekngdR3BlgyJw+nM/kwm2rf56mUFmbhDcUyIl9bb4h5s5xDxvL426F4zpS9BuOh/zt1OOayZNbcEbe1X/UVVPVWJPP0jPDJ1P2btkLv6+vj17/+NXv27AGSzZ7vuusucnLGXhQpFovx/vvv89Of/nRgWf84F154IT//+c/THmu8DZDH0mX7TCD3tYIi4371YWI51UlHz2n0BN0YlZNy1zb0UphjIeAf3nTicFgoybNwtLGPxk4/Fy8pxuMJYTFI1PYGM3YN1FiY8PG9mDfdjccTQi2/ACzPISRi+OMGGOE4YnYhvpZ69JbStI+3/5iLS88vHVfX9DONXgCvP0pvXyBlNNJYqG1yc9nq8iHnGmo9hlQ4d8peg/Ewtu9UACSITs/zH8u55ucP/9BK++7asmULCxYs4IknnuCJJ55gwYIFfPe7301390G89tprrF27Fp0u+TyJxWJEo8nX/9raWhyOqW8XzTSqz5XsMFO2iMhrD6Ge0kQEQFZkfDE/DuPJV+r23mBaJodZJ8rBdpwwuUAyJT2TJpdE8wcIBjNScTKBSBB1mNZ9Pi0nnegoQXa3pX0sXzB2IlpnhEaSUwiHzYgK+ILxCY0jKwptPUEqTytHoCpK0rSVP7SZssa5Rdoz9K6uLm6++eaBz5/5zGfYtm3bqPvdeeedHDhwAIvFwv79+9myZQvPP/88n/vc5wa2cbvdfOUrX8FqTSqbUzNSzxUUfzeCvQDjui8Q+n//RPzDFwc5dvzxAIqqDDhFIZkkVJUiHvl0ygtsPP5K0inXH+LotBlx+2MpGxyPh0T9LnSVKwdFFuhKFsAItaj7EXNKkV2NaR/raIsHu9VwRguMTQS7VY8gJOP+JxIz39kXJiErzC6yE4+efDgoPQ0Qiww8TDXOXdJW6EVFRfz5z3/mmmuuAeCFF16gsHD0Oglbt24dsuz+++8f9LmwsJBnn302XVFmJIrPhZiVj2i2Y1z3BSKv/B7d/HUDWX+eqBdRELEbTr5udfQEuXhJ8ahjzzrRPi3Loh9IRXdmGUnICsFIApt5aPy07G5DdBSnlcGpxsIkWvZjvvLbaZ3r6YiOkoGejulwpNlNTblj2qR1S6KI3WpIllsY/esalpZuP7l2E1azHs8pCj3R8iFS0dwZ0aBBY2KkbXL55S9/SVNTE1/60pf40pe+RH19Pb/85S8nU7ZzCsXvQrQnGzzrKs4DSY9yyqzVE/FiN2QNePrD0QS9vmjKGi6nYzXpybWbBiUgOU6ktqequqiEfYT++gPk1oNpyZ5oPYCgNw+p15IuYk5pMtMvnl4Y5ZEmNzXTxNzSTyayRVu7g8wqGBoqnGj5EGnWkgmNrTEzSHuGnpOTw49+9KPJlOWcRvW5ECtXAiAIIlJeOXJvE7oTP1R31DvIft7ZF0IUBAqd6ZkdKoqzyMkyDXw26CWsJh1uf3SIkkjU7wZVQe6uHzj+SMhddUhF1eNuSCxmF4EgoLjbkQqqRtzWG0zGn88vn15+FqfNiHuCoYst3YEh3ZPUSADFVY/u4s8Ns5fGuUTaCv3gwYP88Y9/pKOjA+UUh93jjz8+KYKdS6iqguJ3IZyIbInKMf6fXeDankb6La6e0xR6Y4eP4lwLel16SjTZGX6wicKZlXrWmKjfBXoTsqs+rbHl7uPoJ9DyTJD0CPZCFM/oCv1os5tsm4GinOlhP+8nE9miLd1+1i0dbLNJtB1EMNnPyTR3jaGkrdC/9a1v8b3vfY/q6mrECYZeaQxGDXlBjiNmJU0uB3oOsUvuY4XPTb8K90S9OE9R6Acb3SyoSN/skKrOiCPLSN9p2aJKoA+5oxbDyuuIH/jHqE5TVY6juBoRL/hU2rKkQnKWIve1kboaykn6zS3TxX7ej8NmpK7dO+79/aFkGeQhb1MtHyLNWjztrofG5JC2Zs7NzeXSSy9l1qxZlJaWDvxpTBzF7wJRh2BNmhH2dCdr3bTE/QN2ZU/Ui+NEhIusKBxu6huomjheclLM0BP1uxGdJejnr0ON+FEDPSPL3tMEqjpiNmg6iM4SlDSaXRxp9lAzzcwtcCJbdALF0Fq7Axj0IvmnlElWVRW59QC6Ms1+rpEk7Rn6d77zHb75zW+yevXqQU2ib7rppkkR7FxC9bkQs/IQBJGoHONg7xEKLfm0maIofa3JhJHISZNLQ7ufWFxh/qyJOQYdNiMNHYNrjMTr30U3ZzWi1YlgdSJ3Nwy8OaRC7qpDzCtH0BmG3SYdRGcJ8bqdI24TiSXo7AsxZxLqn082DpuRXl8ERVXT6oJ0Oi3dAcrybYP2VfpaUUNepLJFmRRVYxqT9gz9P/7jP1AUhb6+Plwu18CfxsRRfN0D9vODvUcwSHo+Outi2i0m5N5mVFUdZEM/0NBLdVl22h11hsOZZRwU5aL4XCjd9eirLgBAyq8a1Y4ud9UhFY6clp0OorMU1e9CjQ9vZ+7xJN9W8h3TLzyvqsROJJqgtTsw+sYpaOkOpDS3iPmVM6qhhcbESHuG3t3dzXPPPTeZspyzKP6egVnwvu4PWZy7CGMsl25JJexqIDp3NQlVHlDoBxv7WD5MM4WxcLpTNF6/CzF3NqKjCACxoBK5ef+w+6uqitxVh/HCz0xYloFIF0/HkO7t/bg8YbKtBoz66VcW1WbWU16UxeEmN+WFY1fALd0BPrJ8cFVNufXDtKKQNM4d0p6hX3rppbzwwgsEg0FisdjAn8bEUU/EoL93tIM9XQd55y144MlmRARavU14oklnWrbRTigSp77dx+LK3FFGHR2HzUggHCeekAFIHN+Fbs4FA+ul/CrknkZURU4td7APNeTJyAxd0BkQ7AUo7tZht3F5I+Q5TMOun+osrHByqNE95v0SskJ779AYdMXdjjjDOhRpTIy0Ffpzzz3Hfffdx8c//nE2bdrEpk2buPLKc7NEZaZRfN14BTsPvPYaIiJfv+wSqkud2JUsWiN9uMNusvQ29KKOw01urCb9QPbnROhPQ3cHYiiBXpTeJvSVJ8MPpfxKSMRR3KmdlXJXHYLViWCdmHO2H13JAuLH3x12fY8nTH729DO39LNwdg5HW9wkZGX0jU+hsy9EQlYpyz/5natyAjXsQ7Rl5tprzAzSNrm8+uqrkynHOYuaiKGGPOztAEdpL8tKlrK0Kp/O3ggvtTloM3Rh8rQMRLgcbOhjYYVzXI6107GZ9egkEY8/isO/HyG7MGn6OIFgMCM6i5Fd9Ui5QzsDyV11SAVzMhYyp1+0kdBf70X2tCM5hjbtcHnCzBqHuWKqUF2WjaJAfbuPebPSj9Rp6Q6Ql20aVANdDXkANbQemwcAACAASURBVGMPU42Zwagz9P379w9yfj711FN89atf5Sc/+Qlu99hfHzUGo/iTYYGv14eJWzo5Lz9pEz2/Og9vr402swm3rw2H0Y6qqhxo6GPRBMMV+xEEAYfNQJ8/gtyyH92spUO2EfOrULpTO0blruMZMbf0I+WUIpUuJH7g5ZTre7wR8rOnr8nFoJeoLsvmUGPfmPZrTeEQVYLuZKiraeJvahozh1EV+g9/+EMMhmRI2s6dO/nNb37Dddddh8Ph4J/+6Z8mXcCZjurvRtZb6ZG6MOgkanKSTR7yHGYKTcV06QS6g904jA46+0L0eCMsqsjcrKyy2M7RBheJtoPoypcNWS8VpI50UROxZGf1DCp0AMOSjcRr30KNBgcfT1VxecPkTcMIl1NZMHvsdvRUES5qoDdp7kqjeJrGucOod4OiKGRnJ1/3X3jhBW666SauuOIKvvnNb9LWln4Na43UKD4XXsGOc1Yv5xcsRSeefK1eOTuZBl+b8OIw2Pl/24+zsMJJjj1zs9RVNQV4jh8AhJTlV6X8KpS+tiHhhHJnLYhSxp1y0qylCFYn8SOvD1ruC8WJxRXyp7FTFGBhRQ717T7C0UTa+zR3B4ZExqhBt2Y/1xjCqApdVdWB5hPvvPPOQFNngERi5JsyHo/z6U9/mpUrV/Liiy8C8O///u9cddVVbN68mbvvvntg2+3bt3PTTTfx6U9/mv37hw+Vm2kkvN3URYwEDe2sKhpcD2XV/GL0YRNhQcXTk+BQYx+fuyKzNa+XzsllnthM0DEXQRqaeC/mloHBTKJp76Dl8dq30VWuSLnPRBAEEcOijcQOvjIouqbHE0YShUEFxqYjFUVZGA0SR1s8aW3vDUTxBWOUDzG59CFYp1fFSY3JZ1Sn6ObNm7nhhhuw2+2UlZWxfPlyAI4cOTJq+zmdTsf999/PE088MWj5HXfcwaZNJzvZyLLM1q1befTRRwkGg9x1111pNc+YCXg626i16ck2GqjKHjzbLcmzIsbzgFbKDj/Pl5deRUGa1RXTRa8TWWbu4FD8AopSrBdEHfr564gf3o5+7hoA1FiIRMP7mK+4M6OyDMg0/2Ki7z1Fom4H+nnJCYTLGybHbkQUp3fNElEUqCl3cCjNXILm7gBmo47c03wHatCdsk2hxrnNqAr9U5/6FOvXr6e3t5eampqB5Tk5OYPqobe3t1NSMjgyQRAECgqG3nS///3vefjhh7n55pu5+uqraWxspKKiApvNhs1mI5FIEI1GMRpH7+4y3ZtEN3u7cZVlc/HsNeQ4hzq4KvOqOBxvpVuYx8aWp5DfaiP36jvSHn+084z3tZMle3i5w8HVViOGFEk78Qs+Rud/3ok10Ys+bxaBD3YgWbLIXbhi3CVzR8aCbt2n8b61DeeCleiycghEZIrzbMOey1T5PtNh5cIi/rajMS15Xb52qkrsA82wIXmuYsSDpWoJWdPknMfDdPpOJ8oZbRJdWFg4pDvR6Yr6G9/4Bs8888yoY91yyy3ccccd+P1+Pv/5z7NixQq8Xi92+8lWana7HY/Hk1ZHpOncJFr2dKAqHrp0Iksci1PKs7KsmkP1b7J842cxs4nQ//wccfXNaXenGe08YwfeRciZhTdo5e29rZw3L0XdFsGOVLaI3p0vYLp4M6F9ryLNvQivL72GFONBrVqHeORdup/7LeYrv01Lpw+H1TDsuUyF7zNdqouz+M/uALUNPaO+cdU29VGcO/jcHA4LcW8PEcmGPE3OeTxMp+90opzxJtGjoapqWts5nUm7X1ZWFmvWrKG2tpbs7Gz8/pNFovx+/4xvFK2qKqE3/i8vWQrINxVQakvdm2xN5Xx+dMF3qSrKSabECyKKpyNjciRa9qMvX8b51XnsOtI97Hb6hZcSP/Y2ck8Tctcx9PPWZkyGVAiCiGn9l5G7jxM/vD0ZsjjNHaL95DnMlOXb2Hds5EqWAM1dKSJc5ARqyIuoxaBrnEbGFHq6ySX9ijuRSLBv3z7Ky8uZPXs2jY2NhEIhXC4XkiSlZW6ZziSOvona08i+LBMrC84bcdtCW9LWKkh6xOxCFHdmootUVUV2NSAVVbNqQSH7jvUQjadO89eVL0MwWAi//DukwupBCUiThWjLxXTRLUR3Pk7C00neNM4SPZ3l1XnsHUWhR2MyXX0hygsGz8jkYH9SkeYU1RhM2pmi4+XOO+/kwIEDWCwW9u/fj9fr5fjx48iyzNVXX01lZSUAt99+O1/84hcRBIF77rlnssU6qyghD5Gdj9NRvQni77CiKP3yp6KjBNndPmojiHRQg30QCyHmlrPQlI1OEjjU2Md51UPNLoIooV+wnth7z6Bb/rEMHD09dNUXITW8x6a618jOvvCMHXeyOa86jxd2NBEIx1M26QZodQUQRWFI31jZ3wuihGC2p9xP49wlYwp9uGYXW7duTWv/DRs2sGHDhkyJM+WIN7yP3FUHcgy5ux4pp4yDtnzo0VNoTT9aQXSWIPc0ZUQmpa8FjFYEiwOdIDC3NJu6Nm9KhQ6gr1mP3FGLvmp1Ro6fDoIgEFx6E8UNP8TSvRNKN42+0zSgoigLu1XP/uM9XLQ4tbmtuTtAca51SJtB2a8lFWmkJm2FrqoqO3fupLW1dVBP0f4GF7/97W8zL90MIdGyn8jLv0NXcR7ojEj5lRiWXUnzB69gjOUhjuGHKTpLidftyIhccm8rUk7ZgLmsqsTO4abhsxhFiwPLVd/NyLHHgitq4K3oGj6z92mUOefPiHA9QRBYXp3PvmPDK/SWLj/lKYqwyf4+zX6ukZK0Ffrtt9+OLMssXLhQ6yk6BhSfi/CrD2JYeS3G8z4+aJ0r1k6WOjZbtOgsQfX3oMajCPqJ+RmUvlbEnJNFt6pKs3lhZzOKok6peO8eb4RW60J0pX4ir/8R89VbZsTs9LzqPH737AHiCSVls++W7gCraoY+vBInZugaGqeTtkJvamri+eefn0xZZhxqIkb45d+iK5qHYflVg9YllARe1cV8/cgO0dNJpxFEuih9LeiXXD7wubLITiwu09YztPb22cTlCZPnsGBc93mC276D3HkMXYoyBdONmvKkUj7c5GbpnMH17RVFpcUV4Pr1c4bsJ2sKXWMY0p7mXHjhhbz//vuTKcuMI7r7KdR4BNNHbx0yo2zxt6OiUGwaWiZ2JE42gphYpIsqx5MPhVNm6BaTjuI8K8cn0J1+MkgqdBOixZFsutF17GyLlBH0OpEllTnsqxsa7dLlDhGLKykfrLK/F9E28QYnGjOPtGfof/vb33jkkUfIzs4e1CT6rbfemhTBZgJyVx2GRZchGIYmj9R7G9HFHOQ4rSn2HBnJUYLiSd10Il0UdzuoKqJzsDO7qthOfbuPS5andnKfDXq8kYHG0FLhXOTOurMsUeZYNjeP/36rYcjylu4AuXZjyggY2d+HXpuha6QgLYWuKApbt25lxYoVky3PjEINuhEsqROk6r1NqEEndothzOOKzlLkvuFbtaWD0teKYC8YYoevKrXz8nsTGzuTBCNxOnqDFJwomysWziF+9E1UVc1YY42zSU25k//yHqbHM7g0cFOXn1kFQzMCVUVGDrgxak5RjRSkZXIRRZGf/exnky3LjEJVlRPZfENnUqqqUu9tJOq2k20bj0Kf+Axd7mtByikbsnxOSTYdPUFCkfTLu04mj75US4HTMtDUQyqYixrxo/q6zrJkmSE320S+w8Th5sHRRYca3Cm7GqlhH6iKZkPXSEnaNvR169bx5JNP4vP5tCbRaaCG/aDKKX94vRE3vpifuM+B3To+ha76XKiJ8V9/pa8VMUVbudI8Kwa9REOnb9xjZ4pdh7vYU+vi1qsXopOSt6poyUbIykfuOj5o24R3+LIFU52acidHmk6W03X7ozR1+Vk2d6idXA30JxVln0kRNaYJadvQ+yNcHnjggYFlgiDwyiuvZF6qGYAaSs64BMvQH169t5FsfTbhuGl8JhdHMm5Z8XQgndZgItG8H8XfjWHRZSOOofS2oF/w0aFjiwKVxVnUt3kz2hlprLj9UR75+1E+uX7OkExJqXDuoHoyidaDdPztPqyfuW9aOgtrZjv562vHB8xI+4/3UOA0U5Qz1PeiBN1INuckVbnUmO5oTaInCTXoRjDbEcShl7je20ShsZQ+vYjJMLRc7WgIOiNCVh6Kp32IQo/ueRY1EhhRoSthH2rYm7LxM0BlSdIxejZ59KWjlBdmsWHlULOQVDiX+OHXBj7HD74Mqkqiae+oD7KpSE25E7c/SrcnTKHTwgd1vSybk5fSR6AG+5Cypt9DS+PMkLZCP71JRT/9maIag1GCHgRLajtng7eJEqEGu8Uwbsee6CxJRqqcguxuH2jorET8iKbUZTaVvlY48VBIxZySbN78oOOsOR7D0QQf1PVyz+bzEVMcXyqcS/TtR1Fj4WSzjeZ9mCqXEW+cngrdmWWkMMfC0WYPTpuRQ419fPOTQxt2Q3KGrtMUusYwpP3e5nK5Bv7a2tp44okn2L1792TKNq1RQ24EawqnlqrSFXKhT2SPyyHaj+QsHaLQE7VvJfuCGq0o3ceH2TOZUCTmlA6bbVlVYicQjtPcFRi3fBPhaIsHk0Gisih18Skxpwx0BuTueuKHtiPmV2FfeyNy+5EhzaWnCzXlDo40uTnS7EYUhZQOUQA10IcuS4tw0UjNmFL/T//85S9/OeMCzRTUoAcxxQzdFwsQV+IoETN2y/jtoKKzlHjt26ixMILBjKrIxGvfxnjBjXD8XeSu4+jKl6fcN1nDJbW5BcBhM7JmUSH3Pb6Xr167+Izb0g819lEz2zls+QFBlJAKqpDbDxM/8jrGNZ/GUDwXwWRL1nefO/2qMtaUO3n81WOYjDoWV+YMOIFPRVVVlL4WdHOWMjVikDSmGuPWKG63m87OzhG3SdUk+t577+Wmm27ixhtv5Nlnnx3Ydvny5WzevJnNmzfzxhtvjFesKYMScg8T4dKLKIhEQ3qybeOvxaKrWoVgziLy1sPJuuatH6LGI+gqVyIVzEE+YXoZIlegl0TDe0glC0Yc/9arF7Jx1Sx+8+QHvLS7Je0GJpngcKObhRUjh+VJhXOJHfgHcOJaCCK6ivNINO45EyJmnJpyB95AjB0HO1k2TK9Rxd2K4unEPHflGZZOY7qQ9gz94osvHvTZarXyzW9+c+TBUzSJ/l//639RUVFBLBbjmmuu4eqrr0an01FWVsYjjzwyRvGnLmowtcmlJ9xHjsmJvydBRdH4GzYIOgOmDV8n9Mz/JlH7FonmD9BVrUbQm5AKqojt/zuqqgzaR1VVIm/8CalwDro5F4w8viBwzdpKZuXbePC5gyiKyqYLysctb7p4AlHaeoIsHOWtQCqcA3uj6BdfhqBLmq50s88n/MrvUOU4gpSJivFnjmybkeJcC529IZZUpbaRJ47vQiqeh5SVA+dIazaNsZG2Qh9Pin+qJtEVFRUA6PV6JEkacLp1dHTw2c9+lqKiIu69996BVnXTleFMLr1hN3mmHPoCsQnN0AEkZwmmtZuJvPUIqArmE6VtpfwqiIeTreqc1QPbx4++gdxVh/XGn6ft7DxvXj5fv3YJ//7UfhxZBtYsnNxORYcb3eTYjRQ6R37YSYXViI6SQaGXUmnyrUNuO4yuPLVTcSqzcHYOFpMuZW6CqqrEj+/CcEoxNQ2N00lbod9yyy08+uijoy5Llz/+8Y9ceeWVSFIybO8f//gHOTk5/PWvf+XXv/41P/nJT9IaZ7ydsiezo7iaiOGPBrAXFWE47Rh+xUtJdgFN4TjF+cN3sU/7WKsvp89VS6yjjtya5ScUtYVITglGfwuSNB+Hw0LC10PnzsdxbvgCtrLh7eep+MgKC3FF5cFnD1BaaGfJnNQmgUxQ1+5jeXXBoC73qbHgvO03A58kScSZm41ctRyxYz+OpWsmTcbJ4vNXLyQSl3GkaLUX66wn4O8md/m6Sb13pxLnynlC5s51VIXu8Xjo6+vD7XbT2Ng4YEsNBAL09vaO66AvvvgiH3zwAb/5zckfZE5O8hX7qquuGjZEMrV843v1nMyO4oovmbUYUMyIpx2j3dtNTc48vIEoOmH88p+KuPaLmGJhvN7wyYW5lQQaj2Bbeilud5Dwi79HLJhDfNYF4zrmiuo8rrpwNv/y8Hv829cuxGLKvElDVVX2HXNx4yVzxixj//epli4jtPMJhNWfnZa1XnSkvieiH7yBVLIAf9yAQ1Ym7d6dSkzmb3SqMZZzzc9PHY4MaSj07du38/TTT9Pe3s4PfvCDgeU2m4277747LQFO5d133+Wxxx7jD3/4w0CjjFAohNFoRJIkdu3axezZs0cZZWqjhDwg6hCMQ0uf9oT7yJIcyIpnXGn/qRAEEYynZ1POIX5oOwCJ+l3IHUex3vjPE1JyH19bwfY9rRxo6GP1gsJRt+/xhDnc5GZRZQ45dtOo23f2hXD7oyyYPX5zm1S6CDXkQfV2IjhSdwKabqiqSrx+F4blV59tUTSmOKMq9Ouuu47rrruOl19+mcsuG3vSxulNol966SWsViu33norkOw52t7ezr333ovNZsNgMEz7QmD9DtHTlaesyHiiXgyKFfCQPY60/3SRCuYQffsREv5eou88hnHFtYjDJBKliygILKnKZf/x3rQU+jNvNvD+0W5iCYWyfCsbV81i3dLh678fanRTmm+dkG9BNNsRHcUkOo5imCEKXXHVowb60Fdq0S0aI5O2DX3u3Ll89atfpaenh7/+9a/U1tby+uuvDyjm4Ti9SfSWLVuGbJOTkzMohHG6M5xDtC/iQUVFStgwGiSM40j7TxcxpwxEPT1P/xuCOWtQZ6KJsHRuHo++dBRFVVNmcfYTiiR4/2g3t9+whHyHmT1HXTz84lGMemngYRCNyzz84hEaO/2YDBJ9/iira0Z/UIyGVDQfubMWFlwy4bFGIl6/CxIxBIsDwZQFSgI1FgFJh1Q0L2Mmn/ixHUilCxFMU6eLlMbUJG2Ffu+99/K9731vwOxSXV3Nt771rVEV+rnIcDHoPZFeTJKRaFgkO0PmluEQRAkpv4J45zEsn/inlDVlxsOiihxCkQQNHT7mlAxf8W/3kS5sFj0LZ+cgigJXrpmNxaTjoecP4zgRonf/X/cTiia48oLZxBIy0ZicsofmWJGK5xHd/dSExxkJxdNB5OXfITqKkya2WBgQQG+CRAzjRTdjWLRhwseR+9qIH96O+fI7Jy60xown7V95JBJhyZIlA58FQRiIUNEYzHCNLXrDfeSac/CH4hmzn4+Ebt5arHOXoxbOzdiYFpOO6rJsPqjrHVGhv/VhBxctLh6U7bl+eSk93gj//tR+rGY9DquBe25ZkbIrz0SQiuejBnpR/D0TNjMNR+zQdqSSBViu/h4AqpxIlrUVBOL1u4i8+iBS3mykCVx7VVGIvPFHdJWrpmUYpsaZJ+1M0YKCAg4fPjzwGvnkk08ya9bYwt/OFdSQJ2Vji55wH3mmHLzB2KTP0AEMNevJvuiTGR936Zw89h8f2gezn/aeIMfbfFy8ZGjM+vUfqWJlTQFzSrL59qeXZ1yZA4i2XARbbtLsMgmoiSjx2rfQLzwZAy9IuoHfhr5qNfrFGwn/47coofH3Z40fegXV24XxopsnLLPGuUHaCv2nP/0pDz30EC6Xi3Xr1rFjx460Y8XPNZRhskR7I8kZui8YOyMz9Mli2dxcmrsCuP3RlOvf/rCD+bMcFDiHxtUKgsDnN9Vw68cXotdN3hueVDwfuWNyFHri+C4ESY+u4vxhtzGuvhExu5DIK78fV9kExd9DdNdfMV50M6I5dZEyDY3TSVuh5+bm8qtf/Yq33nqLN998k1/96le8++67kynbtERVVdRhSuf2nDC5nKkZ+mRRlGMh32Hiw/qheQiyovDOgU4uXnp2I0ykonnInUcnZezYoe3oaz4yol9CECVMG76G3F2H3H54zMeIvvcMUlE1umlYaEzj7DGqQvf7/TzwwAP8+Mc/5o033kCWZR5++GE2btzIc889dyZknF7EQiDHUppceiNJk8t0n6ELgsDSOXl8UHfS7BKNy+w71sNDzx8mGpdZOX/izs2JoCuej+LpQAlntlGH3NOI0tOAPo0IGtHiQDdnDfGDY+vqpcYjJBp2Y1hy+bRMjtI4e4zqFP3Od76D0+lk+fLlPPXUU/z2t7/FarXyu9/9jvnz558JGacVSvBE67nTTC7hRIRgPES2wUlnXxdFKcwR04nlc/P41RP7+Nr/eR2dKBCNK+h1IkuqcrjjhqWTGpKZDkJ2EYLZjtxZi5jB+O34oVfRlS9Pu9WdYdEGQs/+BCXQm/Y+icY9ySJrpYsmIqrGOcioCr29vZ0HH3wQgE996lOsW7eO1157Db1+elWzO1OoIQ8YLAi6wckxveE+ALy9Iiowp3R620UXVji593MricZlZEXBoJOoKrGnrON9NhAEIWl26TiasYQcVVGI1+/GdEn6obpSfiVifiXxQ9sxrk7PQR0/9g66uRciiFoUmcbYGFWhS5JEPB4fcOzk5+cDEIslO84bDNPXdDAZqEE34jAO0WxDFrUtfqrLsifVIXgmEASBqpKp/VCSiucTP/J6xlrpKX0tEI+gG6WW/OkYFl1GdMc2DOdfM1Dqtx9VVUFJDJT7VUIe5LaDaSt/DY1TGVWh+3w+Nm3aNMhT3/9ZEAReeWVs9sGZjhJ0j+gQPXzQzfLqyatWqHESXcUKojufQG4/jK504YTHkzuOIObORjCMrY69rmoV0R3bSNTvRj9v7cByJeQh8uqDKN4uLNfcg5iVT6JuB6KjGDF3etcz0jg7jKrQX3311TMhx4xhuMYWvZE+HAYnOzr83Lxx3lmQ7NxDtOWgr1lP9L2nkUoWTHiWLnccTfZsHSOCpEe/4BKie58DVKTiGhRvF5HtDyI6SxFzywk9/69Yrvl+0txSfZHmDNUYF2kbPKPRKH/605/4+c9/DkBzc/O4ml7MdOSOo0h5FUOW94b7UKNmTAaJ2YXDl7/UyCyG865G6WlEbjs44nZqPELs4MskOo+lXq8qJMap0AEMS65AVzyP6Pv/TXDbdwj/7T70Cy/F/LHvYt74DcTsQkL//TOU3pZp2RNVY2qQdur/9773PZYsWcKOHTuApC39jjvuGNKa7lxG8XaieNrRzT5v8HJVodHXQmm0ZMTmxxqZR7Q60ddckozrLl2EIAhJ82E8jBoNoUaDJBr3EDv4MsQjSKWL0F35rSHjKO52iAbRFY3v7Uow2TB95EvJsfw9qHIMydFfeVLEfPkdhP/2a3AUpx0No6FxOmkr9ObmZn7zm98MxJ6bzePvhzlTSTTtQ8wtH1I/5LingYgcpbvJxkeXT+/WetMRw/KrCD6+hcSxt1FCPhK1bybb851AdJZiuuizCAYL4VcfQFXkIREmcscRxJyyjFQ8TFVfRtAZMV+9BeTEhMfXOHdJW6GbzWb8fv+Aba+2thaLZeRY6ng8zubNm6mrq+NnP/sZmzZtoq+vjy1bthAMBrnooou44447gGQjjQceeABBEPj+97/P0qXTrxhRomnvkNk5wD7XAeba57DXFRu1m71G5hGtTvQLPkrktYcQnWXoa9YjlS9FMNoQDBYEKfkzUOMRSMRRehqRCuYMGmO89vOxIAgi6LSoMY3xMyaTy9e//nXa2tr48pe/TGtrK7/61a9GHlyn4/777x/UUu4///M/ueGGG7jyyiu57bbbqKuro7Kykq1bt/Loo48SDAa566672LZt2/jP6iygRgLInbUY13xm8HJVZZ/rAAuNa3DYDBTlTO+EoumKcfUn0dd8BNFZOqzDUdCbEAsqSbQfGaTQVVVF7jiKce3mMyWuhsa4SFuhL126lD/+8Y80NDSgqipVVVWjJhcJgkBBweAU8D179nDnncnazpdccgm7d+9GEAQqKiqw2WzYbDYSiQTRaBSjcfyda840ieYPECwOxLzB4WbN/lZ8MT9Bbw4LZhu16IWzhKAzIOWUjbqdrrgmWXtl+VUDy1RvJ2rYhzRO+7mGxpkibYV+ekehw4cPY7PZqKmpoaxs9B9KP6FQCJMp2V/SbrfT2tqK1+vFbj+ZpGK32/F4PBQWjt69ZrydsjPdUbynfT+W6lVDutW/2HqEiqxK9rzv5Vs3n3/Gu5ifK53TM3WekXnL6Tn4D7KzDAOmmEBjA9GcUnJKpkZLO+07nXlk6lzTVuivvvoqhw4d4iMf+QgAb775JtXV1XR2dnLttdfyuc99Lq1xzGbzwOzb7/eTnZ1NdnY2fr9/YBu/34/DMTSWOxXj7QqeyY7iaiJGuH4f5o3fGDSmqqrsbN6Lr6mECxcXMa/Efsa7mJ8rndMzdZ6qbRaqnKDv2EGkomoAwvX7EQqrp8x11L7TmcdYzjU/f/iw57Tj0Ht7e3n22Wf54Q9/yA9/+EOeffZZvF4vf/nLX3j88cfTHYYVK1bw+uuvA/DGG2+wcuVKZs+eTWNjI6FQCJfLhSRJ08rcIrcfAUFAKlnAe137eLbuBbxRHx3BLlyRHmyJcm6+rPpsi6mRBoLOiFQwh0THESDZai7R8H5KZ7eGxlQj7Rl6T0/PoJZzoijS29uL2WwesZ7LnXfeyYEDB7BYLOzfv59bb72VLVu28Kc//Yk1a9ZQXZ1UdLfffjtf/OIXEQSBe+65ZwKndOZJNH+ArmwxgqTnjdYddAQ72d7yJibVAUEn3/z4qmlfu+VcQiqpQW4/grrsKsKvPYRu9nnoypedbbE0NEYlbYX++c9/nuuuu461a9cmTQk7d7J582bC4TAXXHDBsPtt3bp1yLKHHnpoyLINGzawYcPEm+qeDRLth9Av2oiiKrQG2rhl3mf5+7vNtLKfy+ZcSJ5Di9mfTkglC4h98CKxfc+j+l0YN911tkXS0EgLQR1Df6yuri4+/PBDAJYsWZKW03Kycbn8o2+UgkzZBCaTSwAAIABJREFU55Sgm+Bf7uaXgevRF2fRXfg3jLVXkmvJ4ivXLDrryvxcsUNm2icS+PPXQUlg2ngH+soVGRk3U2jf6cwjUzb0tGfoAEajkfz8fGKxGM3NzTQ3N7Nq1aqxDDHjkNsPExSzKCyfjbGwC3fUyrqFs/n42oopUxtcY2wIOgNS6QIEo23KKXMNjZFIW6E/9thjPP3007S0tHDeeeexa9cuVqxYcc4r9ETbIZqFUuaVOwk6W1gYqeS6JVVnWyyNCWLeeAdIWhOXVCiKgtfbizzJZQo8HpFEQpnUY0wVhjtXSdKRnZ2LKKY3OUxboW/bto1nnnmG66+/ngceeIDW1lZ++tOfpi/xDERVVeT2wxyJLWGB3cQRfxvzc+aebbE0MsDpjSg0TuL19mIyWTCbraNvPAEkSUSWzw2FPty5hsNBvN5enM78tMZJ2yZgNBrR6XRIkkQ4HKasrIzm5ub0JZ6BqL5u1EAvH/jzcNoMtATamJVVerbF0tCYVGQ5MenKXCOJ2Wwd05tQ2jP0hQsX4vP5uOGGG/jUpz6FzWZjyZIl4xJyppBoP4yaVYi7zwLGMOFERFPoGhoaZ420FLqqqnzta1/Dbrdzyy23sH79eoLBIDU1NZMt35RGbjtE2DkXsVnAI3fhMGZjN2jNKzQ0JpOurk5+85t/4/jxOmy2LGbPruDuu7cMKh8yGtFolO9+9078fh933PEt/u///SNbt/5uEqU+M6Sl0AVB4LbbbhuohT5r1qxJFWo6oKoKcvth3JXX4sgy0Bpo12bnGhqTjKqqfP/73+WTn7yJX/wiWe11166d+P2+MSn0Y8eOYrVauf/+BwA4//yVkyLvmSZtk8uKFSt45513uOiiiyZTnmmD0teGGgnQLpWSkxWkxX+IKkfF2RZLQ2NG8957u7BYLFx55dUDy1avXkMkEuFHP/o+DQ3HsVqt3HPPjygvn81//deD9PS4aGxsoLe3h+985x4WLFjET3/6Q7xeL1/4ws1s3fo7Nm++if/5n78jyzL/9v+3d+dxUVf748dfM8My7LsgIpCEW0amJmqaa+JVi5/mLVyQFLEyk7RcupdKCb1Wltfcbmq2ubRwzbKHW7i3Y1qWSsiWiiCbLMM+8/n8/uDrXFSUYR2YOc/Ho4fNZ+ZzPuc9o28OZz7nfd5cwe+//4af311kZWWyYsUqOnb0vkOv2g6DvxT95ptvmDlzJv369WPw4MH6/8yVNvVHlG6+5JSrcHaw4lJJJr5ihC4ILSojI53AwFvLGP/3v5/i4uLCRx99yvTpM3nrrdf1z2VnZ7Fu3SZiY1fy/vubcHBwYPHiGIKDB/DBBztwcvpfIcCjRw+j0WjYvj2eGTOiSE1NaZW4movBI/TvvvuuJfvRrmgvn6Xqt/3YjHmegpMV2DlVU6otE1MugtAK6tpT4I8/zhAePgOAgQMH8/rry/XPDRw4GJVKRdeu3cjKyrrl3JvbGTHiYQACA7vi6+vffB1vBQaP0CsrK3n//fdZvrzmjbp48SLffvtti3WsrZI0BVQc/g9W94/HovO9XCupBHURDlb2OFkZPocnCELD+fv7k5z8Z4POsbKqWSCmVCqRpDvf196ASihtksEJffHixUiSxA8//ACAh4cHb775Zot1rC2SdVrKD21A6e6HVZ9QAApKKqmwKKCzw+23NhMEoXn06xdMaamGAwf26o8lJv5Er15BHDr0DQA//vg9/v53Nar9e+8N4ujRQwCkpqZw8WJGk/vcmgxO6BcvXiQyMhILi5pZGhsb86sgWH3+CHJJHuoRT6FQKtFJEoWaSoqkHPwdxJ0/gtDSFAoFK1a8xeHD3/D446FMm/Y4+/d/TUjIWPLycomICOPDD7ewYMHiRrU/bNhI1Go1U6dOYuvWd/H19cPOrv0sojJ4Dt3GxoaSkhL9KDQ5ORlb24ZvmZSSksKyZcsAKC0tRZZlwsPD2bBhAx071mzx9fHHHze43ZYmyzLV549g2Ws0SnXNveZFmipkWeZqZRZjHYcZt4OCYCa8vLx4/fXVtxxftmzFLcciI5+64fFXXx0Aam5TrH2r4vXjKpWK+fMXYWNjw6VLF1m8eD6Ojk7N2f0WZXBCX7x4MXPmzCEzM5PIyEguX77MW2+91eAL3n333fqEvX37doqLiwGYPHkykZGRDW6vtUg5qUiFV7Hs+qD+WEFxJSqbcip0Ffg5ihG6IJiCBQvmUlZWU8r2hReWGLk3DWNwQg8KCmLr1q2kp6cjyzJdunTB0rJp1ei+/vpr3njjDRITE/n8889JSEggJCSEJ598sknttoTqpGNY+PVGafu/n9YFJRXYu5XioHbFwcreiL0TBKG5bNz4nrG70GgN2rFozJgxjB49Gjc3tyZf+PLly0iSROfOnXFyciI0NBSdTsfTTz9N79696d27t0HtNHan7Ibssi1VlqFJ+xm30AXY1DqnrFrC2rmEru53tdndyc1l53RziROMH2thoRJVK9X6b63rtAW3i9XCwvDP2+CE/sorr7Bv3z4iIyNxcHAgJCSE0aNH06FDB0ObuMHevXsZO3YsgH7JrkqlYuTIkZw7d87ghN7YHU0askNI1fmjYGVHhXMgpfka/cYVV3JK0FoX4K3u2mZ3VjGXXV/MJU4wfqxardQqZW1F+dwaWq10w+d9px2LDP7xFxAQwNy5c9m9ezexsbGcPn2aYcOGGd7jm9RO6CUlNdvIybLMyZMn8ff3b3S7LaE66RiW3YZw7EwWy95PRPq/e1Xzi8uoUBXg5+hr5B4KgiA0cAu6pKQkDhw4QEJCAh4eHixdurRRF71w4QLOzs54eNQUbd+6dSvfffcdCoWCfv36tal6Mbr8i0i5GViMepZDn6aSmVfK2fQC7u3iRm7lVUCms0P7qPMgCIJpMzih/+1vf6NTp06EhITw0Ucf4eLi0uiLBgYG8sEHH+gfR0dHEx0d3ej2WpI25UdU3t3JKLbkankWnXuVc+RUJvd2caNQysHF0gMrldjdRhBaUlbWFVavfoOysjLi4t7A2dm5/pOaya5dnzNx4t8bff7evXsoLCxkypTwZuxV3QxO6J988glOTjV3eKSmprJ9+3YOHDigL6lrimRZpjotEav7xnL0t8vYdz9PnuoaVy4ryLl2N1WW+fjYifotgmCqJEniiy+altBbk8EJPT8/n23btrF//34uXrzI888/z/r161uyb0Yn5WUga/LQdurNycQvsfWrYESnIRzVnmTXt91R2BVxt0t/Y3dTEIymrKKayuqmf3FpbanEVm3YbdBbt75LRkY6Li4uvPJKHCqV6pbXTJv2OMHBAzh79g969LiH6OgXKC8vJy6upmyulZU1MTFLcXW99Y695cuXYm2tJjv7Cj179iIr6wpz585mwoRJ+Pt3YeXKWJycnHFyciIgoGudI+/09DRWrFiGk5MTarUNPXv2QqvVEhOzCI1Gg0KhYMmSl6mqquLtt19HliXs7R2IjV2JlVXjf+OvN6Fv2LCBAwcOYG9vT0hICJs2bWLq1KnMmDGj0RdtL7Rpiai8e3IiNR+l9wUm3P0owd59OXnlLL9eO4rKo5Ru7o2rGSEI7Z1Okli48XvKK3VNbsvGWsU70UNQGbC7fd++/VmwYDH//vcqvv32GEOHjrjlNZWVFYSGPsZzzy0gImIypaUavv76S3r1uo/Jk6dx6NA3bNv2AfPmvVDnNXx9fXnxxZpFRUeOJLBu3SYAFi2az6JF/yQwsBsrViy7bR/ffXcdL7ywhO7de+hfd/VqNrIs69uSJInq6irWrNmIpaUF69e/w4kTRxk5cnS978Ht1JvQd+zYgY+PD1OmTGH48OHY2tqaRRGqmumWn7HqPZ5vLiTg4ODMoE4PoFQoibjn76zVvQuSCm97T2N3VRCMQqVU8uYzg5pthG5IMgfo2fMe/Z+XLtW9Ub21tRpfXz8APD09KSkp4dKli4we/TegpgjXvn23ny6+556gOo9fvZpFYGA3/fXLysrrfF1W1hW6d+9xw+s6dfKhf/+BLFsWg5OTM1FRT5Ofn8fataspLy+noCD/htrsjVFvQj9x4gSJiYns27ePt99+m549e1JeXo5Go8He3nRXR0q56ciaAv6wcKPMLp2orpEoFTV/4bq7B9CJXlyrLtAfEwRzZKu2xFbdutdMSjqHh0cHkpLOERRU93qVm8ecsizj49OZs2f/ICioN7//fobOnf1ue43ai3xqD2A9Pb24cCGZwMCunD9/Dj+/un9D9/LqSHJyEl27dufcubP4+3ehqqqKCRMm8dhjj/PBB1tISDhIenoqjzwygeHDR7Bu3Zoml++tN6ErFAr69+9P//79kWWZn3/+GXd3d8aPH09AQADvvdd+l8neSXVaIirvHiRkJKOWXLi/0427pCwaOo2yyioj9U4QzNfp07/w2Wc7cXZ2YfDgoQaf9+ijE3nttZf59ttjWFlZERNz+ymT2gIDu/LSSy8ybtyjREXN4fXX43Bycsbe3u625U+eeupZ/vWvWBwdnfU3k2RnZ7Fy5WuoVCpkWSYmJhZvb2/WrHmLffv2YGtr1+QRukJu5I8ESZJITEwkODgYgG3btjFt2rQmdaYxcnNLGnXenVbbybJM6c4XUQaNY2HS73TzcWfegKlN6abRGHtVYWsxlzjB+LHm5WXh7t6xxa/TFleKarVafQnxFSuWMXz4KAYOfLCes+p3p1hvfr/vtFK0QQuLalMqlfpkDvDf//7XKAm9JUj5fyGXFnCmyhdsv6W3T3D9JwmC0OqKi4v4xz8W3nDs0UcnMnr0GIPOf+21V7h6NVv/uGfPe5gz5/ZrYpKTk1i37t9otVp8ff0IDh7Y4DZaUqMT+s3a+9ZNtemyU1C6dubQ+XzoWIq/o4+xuyQIQh0cHZ30d400xssvxzbo9T179mLDhi1NaqMlNds3eqZ054suN40Kh86kX7uMUqGgo72XsbskCIJQr2ZL6CY1Qs9J43ypMx19tHSy88JS2Wy/yAiCILSYZkvo06dPb66mjEquLEUuyuboJWtcPCvo7CCW9guC0D7UO/RcsGDBHadTrm9DN3HixObrlRHpctORVFbkSI54KVPp7BBo7C4JgtlrSnGurKwrZGSkMXDg4EZff/nypTz22ON0796z0W20hnoTelhYWGv0o83Q5aSRo/Skdzc3TpddFSN0QWjHdDodWVlX+OGH75qU0NuLehN6//7NX3yqd+/e3HvvvQBERUXRv39/lixZQk5ODoGBgbz66qsoDVwG3Nyqs1M4p3GiSxclpzJlOtm3/P22gtBeyZWlyNqmL7BTWFihsLYz6LWGFucaMGAQFy4kY2try/nzZ0lLS2XRon+QlJTEzp0f0amTDxqNhqefnlvnyPvw4QQ+/ngrXl7elJTUbGafmXmZ2NiXsbKywsbGljfeWM3Bg/vZs+cLKirKCQ4exKxZTzftzWgCg7/tO3XqFCtXriQlJQWA8vJy3N3dOXHiRIMv6uPjw8cff6x/vH37dnr16sWsWbNYtmwZJ06cYOhQw1eANRdZlqnOTuWqciCudiV42XUQtc4F4TZkSYdmx4tQXXc9kwaxtME+Yh0K5a3J+WaGFOeqqCgnJORvzJ37PKdOneTo0UMsWLAYnU7HP/6xkC1bPkalUhERUfcMhE6nY+vWd9m8+SMsLCyYPv0JoGaV6vDhIwkLm4Yk1SwEGjJkKKNHj0GWZebMiWTChEm4ubk34c1oPIMTelxcHBs3buSpp55i9+7d7N+/n19++aVRF83KymLq1Kl4eXkRExPDyZMnmTt3LgDDhg0jMTHROAldk4eFthS3gB5c1mSI6RZBuAOFUoX9lFXNN0I3IJmDocW5rPVFtGorLLyGu7sHanVNAZrAwK63vAagqKgQd3cPbGxsAOjataatESMe5sMP32PZshgCA7syZcp0fvklkU8/3Y4sy2RmZpKbm9v2EzrUVC3T6WpKZY4ZM4aNGzc26qLffPMNrq6uxMfHs3r1aoqKivQbRTs6OlJUVGRwW43d/byundPzMv6iULJl8KBebEv9kQc792v3O8kbe4f41mIucYLxYy0sVP6veJXt7ZehN4fr17n+p0KhIDk5CS8vL/788zz33Xf/DYW0/neeSn/c2toaSZJQqZS4ubmRl5dLdXUVFhYqUlIuoFQqb2nD1dX1/15XiYWFBRcu1LzO0lLF3Lk1q0Cfe+5phg4dznvv/Ye1a9/F0dGR2bNnoFQq6uyTobHezMLC8M/b4ITu4OBAWVkZffv2JSYmBjc3N6ytrQ09/Qaurq4AjBs3jk8//ZROnTpRXFyMh4cHJSUl+mI2hmhsTYu66mFk/HaaAkUH+tipuFiUSehdY9t9fRBj1/1oLeYSJxg/Vq1WapUaK7Xrm1z/U5ZlfvklkU8+2Y6zswuDBg2psy+yLOuP+/vfRVpaKi+99CLPPDOP8PAZzJ79JF5e3ri4uKJUqupoQ8GTT85i9uwZeHl506FDByRJ4vjx48THf4JKpcLNzQ1Pz46MGDGaZ5+djb//XVhbq5Gkhr8/d6rlotVKN3zed6rlYnBxLo1Gg1qtRpZl9uzZg0ajYfz48frkbKiysjKsra1RqVQcO3aMPXv2cP/991NVVcWMGTOIi4tj8ODBDBs2zKD2mrM4V/KWl/jL/S6u9fTk+OXvefOhZdhYtHJt0GZm7H/8rcVc4gTjx9rei3NdL7BVXV3NrFnhrF+/xeilwFu9ONehQ4cYOXIk9vb2TbrnPC0tjZiYGOzt7bGysiIuLg4XFxeWLFnC1KlTCQgI4KGHHmp0+411pSCPz7wryVKn0FUjE9lrWrtP5oJg6hpTnOvrr3eTkHCQsrJSHnnk/yFJOubOnd2gNtoqg0fob775JocPH8bX15eQkBBGjRqln/c2puYaoX9y9AP+qPidBYMW4upoOrVbjD2aay3mEicYP9b2PkJvi5prhG7wzP3ChQvZt28f8+bNIyMjg8mTJzNr1qwGdLntkmWJ9LJzdK5yNalkLgiCeWnwV7GOjo44ODhgZ2eHRqNpiT61utwLJ7isBr+Ojd+cVRAEwdgMnkPfsGEDCQkJ2NjYEBISwtq1a/H0bP8bJMuSxM/n92JnYcEDfe81dncEQRAarUG3LW7cuNEkknht2tQf+c1Ki1wciJuT+BJUENoiYxfnai/qnXI5ePAgAOHh4eTl5d3wXHx8fMv0qpXIumoyf93NFWsVAY5Bxu6OIAjNrHZxLnNQ7wh948aNjB5dM7ccExPDF198oX9u+/btTJo0qeV618Iqf/qcX61lVOUu9Pb3NXZ3BKHdKasup0pq+tJ/K6UVtpY2Br22tYpzTZv2OMHBAzh79g969LiH6OgXSE9P4+23X0eSJBwcHIiNXYmVVdup91RvQq99V+PNdzi2512KytNOU3XuEKe7dqE81YvuQ12M3SVBaFd0ko6Xv/8XFbqKJrelVql5Y8irqNpIcS6AysoKQkMf47nnFhARMZnSUg3e3t6sWbMRpVLJf/6zjhMnjjJyZNu5maLehF57c4ubN7por/uISmWFFOxdz4V7h1BQ+ifu8l042zeujIEgmCuVUsVrg15qthG6IckcWqc4V00banx9/YCaOlYlJSVUVVWydu1qysvLKSjIx8nJ8Ln81lBvQk9OTmbw4JovEwoLC/X/f/1xe3Tl6/9gadeB3doruFf0oouPt7G7JAjtkq2lDbYYNlXSXJKSzuHh0YGkpHMEBfWu8zW191OwtLTUFxV0dnYhLy+XysoKVCoLUlIu3PY6N49XZVlm167PeeSRCTz00DA2blzb5mYp6k3oZ8+ebY1+tKpMOrCjvAxdST5yckfGjRHTLYLQXpw+/QuffbYTZ2cXBg+uv8x2ly4BpKenEROzSF+c6+mnZ+qLc1lYWBp87QcfHMKaNW+xd+9X2NnZt7kRusFL/9uqxiz9v1qaw/KfVxNsM45rV5yYObY7tmrDP9T2xNjLxFuLucQJxo+1vS/9F8W5TMznF76ir3cQU7uZ/n2pgmDKRHGuG5nlCP3rtIOM6zEMRWXbud2opRh7NNdazCVOMH6s7X2E3ha1enEuUzK+y2hcbNrW3JcgCEJTtfqUy+nTp1m5ciWWlpbY2tqyatUqPvzwQ/bv34+rqyvu7u6sXr26tbslCIKBVCoLystLsbGxM3ZXTF55eSkqleFputWnXK5evYqjoyM2Njbs3LmTwsJCtFotgYGBjBnT8Dmr5tyxyBSJOE2PsWOVJImionx0Om2LXsfCQolWax5TLreLVaWywMnJ7YbbMJtlC7qWEB8fT0FBAZWVlSQkJGBnZ8eUKVMYP368wW1UV+sadW1zmZ8TcZoec4nVXOKEhsVqaXn7BVhGS+jXrl0jMjKSLVu2oFAocHFxoaSkhIiICNavX0/HjoZ96SJG6Hcm4jQ95hKrucQJDYu1zX0pWl5eTnR0NDExMbi6uuLiUrOwx8HBgQEDBpCcnGyMbgmCILRrrZ7QtVot8+fPJzw8nD59+gBQUlKif+7XX3/F11dUPhQEQWioVp9y2b17N3FxcfTo0QOAoUOHkp6eTmpqKjqdjvHjxxMREdGaXRIEQTAJ7X5hkSAIglDDLBcWCYIgmCKR0AVBEEyESOiCIAgmQiR0QRAEEyESuiAIgokQCV0QBMFEiIQuCIJgIswuoX/22WeEhYURHh7OpUuXjN2dZnX69GmeeOIJpk2bxuzZsykuLqagoIBZs2YxefJk1q5da+wuNquTJ0/SrVs3CgoKTDrOM2fOMHPmTMLDw9myZYvJxhobG0tYWBiPP/44P/30ExUVFTz//PNMmTKFV199FUlqv4W6qqurCQsLo1+/fuzfvx/gtp/jkSNHeOKJJwgLC+PMmTMNu5BsRq5duyZPmjRJrq6uln/77Td53rx5xu5Ss8rOzpbLyspkWZblHTt2yBs2bJBXrlwp7927V5ZlWY6KipIvXLhgzC42q7lz58oTJ06U8/PzTTbOyspKOSoqSv+5yrJskrGmp6fL06dPl2VZlq9cuSJPmTJF3rZtm7x582ZZlmV56dKl8tGjR43ZxSaRJEm+evWq/M4778j79u2TZbnuz1Gr1cqhoaFySUmJnJ2dLYeFhTXoOmY1Qj9z5gz9+/fHwsKCoKAg0tPTjd2lZuXp6YmNjQ0AlpaWqFQqTp06xfDhwwEYNmwYiYmJxuxiszly5Ah9+/bF1tYWwGTj/PXXX1Gr1cybN4+ZM2eSlJRkkrG6u7ujVqvRarUUFxfj6urKyZMnTSZOhUJBhw4dbjhW1+eYkZGBv78/9vb2eHp6otVqqaysNPg6ZpXQi4qKcHJy0j+WTbTqwbVr19ixYweTJk2irKwMtVoNgKOjI0VFRUbuXdNJksSOHTuYPHmy/pgpxgmQk5NDSkoKa9as4Z///CfLli0zyVjt7Ozw9vZmzJgxREZGEhkZSVFREY6OjoDpxFlbXZ9j7ZivHy8sLDS4TbNK6I6OjhQXF+sf194FxFTcXJrYxsZG/xO+pKTkhh9o7dWePXsYMWIE1tbW+mOmGCfU/J3t06cPtra2BAQEoNFoTDLW7777jsLCQg4ePMiuXbuIjY294d+rqcRZW12fo5OTk7767PXjzs6G739sehntDu677z4SExPR6XScPXsWPz8/Y3epWdVVmrhv374cO3YMgOPHj9OvXz9jdrFZJCcnc+DAASIjI/nzzz958cUXTTJOqPk7m56ejiRJ5ObmYmVlZZKxSpKEk5MTSqUSe3t7ysrKeOCBBzh+/DhgOnHWVtfn6OfnR0ZGBmVlZeTm5qJSqW4YuNTH7Kot7ty5ky+//BILCwuWL19uUkm9rtLEEydOZNGiRZSWljJgwACio6ON3MvmFR4ezpo1awBMNs74+Hh27dqFVqtl4cKFBAQEmFysOp2OJUuWkJmZSWVlJRERETz88MMsWbKEvLw8AgICWLp0abv+rTo6Opo//vgDW1tbhgwZwqxZs+r8HA8dOsSmTZtQKBS89NJL3HfffQZfw+wSuiAIgqlqvz/uBEEQhBuIhC4IgmAiREIXBEEwESKhC4IgmAiR0AVBEEyESOiC2XjwwQcBuHz5sr5AUnNISEjgr7/+0j+Oioqiqqqq2doXBEOJhC6YnczMTA4cONCgc3Q63W2fuzmhb968GSsrq0b3TxAay8LYHRCE1rZ69WpSU1MJDQ0lMjKSkSNHsnTpUtLS0gB4+eWX6d27N0uWLEGtVvP7778zbtw4fH19effdd6mqqsLb25tVq1aRkpLC4cOHOXnyJHZ2dmzfvp1HH32Uffv2YW1tzYYNG9i7dy8KhYL58+czYsQIfvrpJzZt2oSlpSVpaWlMmDCBZ555xsjvimAKREIXzM78+fP55JNPWL16NQCrVq0iJCSEUaNGkZ2dzZw5c9i1axdQU0sjPj4ehUJBUVERo0aNAmpG4fHx8URERDBixAjGjh3LQw89dMN1zpw5w+HDh9m1axeFhYWEhYURHBwMwLlz59i7dy9qtZoxY8YQERGhrxwpCI0lErpg9r7//nuOHz+u32SgsLAQrVYLQEhICAqFAoCsrCyio6PJz8+nvLycQYMG3bHdU6dOERISgpWVFR06dKBnz56kpKQA0KdPH1xcXADw8fEhJycHf3//FopQMBcioQtmT5ZlNm/ejKen5y3PXS9vChAXF8dzzz1HcHAw+/fv5+jRo42+Zu05dqVSecc5ekEwlPhSVDA7dnZ2lJaW6h8PHDiQHTt26B8nJSXVeZ5Go8HDwwNJkvjyyy9v2951ffr0ISEhgerqanJzczl37hx33313M0YiCDcSCV0wO926daOiooLQ0FC++uornn32WXJycnjkkUcYO3Ysn3/+eZ3nzZkzh6ioKP7+97/j4+OjPz527FjWrl1LaGgoGo1GfzwoKIihQ4cyYcIEZs6cSUxMDHZ2di0en2C+RLVFQRAEEyFG6IIgCCahCslqAAAAMElEQVRCJHRBEAQTIRK6IAiCiRAJXRAEwUSIhC4IgmAiREIXBEEwESKhC4IgmIj/D++32+l5yotHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 410.4x216 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wANZ-Jo-wPdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 2"
      ],
      "metadata": {
        "id": "JIj1PMd1nMb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100, 200, 120 ,    lr = 0.01, 0.03, 0.05, 0.07\n"
      ],
      "metadata": {
        "id": "Th58Ic3rnOht"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name InvertedPendulum-v2 \\\n",
        "--ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 100 -lr 0.07 -rtg \\\n",
        "--exp_name q2_b100_r07"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-lI_uS3x8w7",
        "outputId": "9d023f0d-7b90-4f2a-eed6-c28aafa639f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q2_b100_r07_InvertedPendulum-v2_04-02-2022_17-32-37\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.1052632331848145\n",
            "Eval_StdReturn : 0.3068922162055969\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.1052631578947367\n",
            "Train_AverageReturn : 7.4285712242126465\n",
            "Train_StdReturn : 3.0169589519500732\n",
            "Train_MaxReturn : 14.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 7.428571428571429\n",
            "Train_EnvstepsSoFar : 104\n",
            "TimeSinceStart : 0.6292994022369385\n",
            "Training Loss : -7.7160491943359375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 33.53845977783203\n",
            "Eval_StdReturn : 21.004085540771484\n",
            "Eval_MaxReturn : 90.0\n",
            "Eval_MinReturn : 6.0\n",
            "Eval_AverageEpLen : 33.53846153846154\n",
            "Train_AverageReturn : 2.17391300201416\n",
            "Train_StdReturn : 0.37903469800949097\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.1739130434782608\n",
            "Train_EnvstepsSoFar : 204\n",
            "TimeSinceStart : 1.3244349956512451\n",
            "Training Loss : 446.63897705078125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.019999980926514\n",
            "Eval_StdReturn : 1.2647528648376465\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.02\n",
            "Train_AverageReturn : 21.200000762939453\n",
            "Train_StdReturn : 7.858753204345703\n",
            "Train_MaxReturn : 35.0\n",
            "Train_MinReturn : 13.0\n",
            "Train_AverageEpLen : 21.2\n",
            "Train_EnvstepsSoFar : 310\n",
            "TimeSinceStart : 2.4134700298309326\n",
            "Training Loss : 1090.2159423828125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.784722328186035\n",
            "Eval_StdReturn : 0.6683644652366638\n",
            "Eval_MaxReturn : 8.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.7847222222222223\n",
            "Train_AverageReturn : 3.814814805984497\n",
            "Train_StdReturn : 0.8181748390197754\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 3.814814814814815\n",
            "Train_EnvstepsSoFar : 413\n",
            "TimeSinceStart : 3.52144455909729\n",
            "Training Loss : 1.241363525390625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0512821674346924\n",
            "Eval_StdReturn : 0.22057245671749115\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.051282051282051\n",
            "Train_AverageReturn : 2.729729652404785\n",
            "Train_StdReturn : 0.5525688529014587\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.72972972972973\n",
            "Train_EnvstepsSoFar : 514\n",
            "TimeSinceStart : 4.40067458152771\n",
            "Training Loss : 297.06402587890625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.0199999809265137\n",
            "Train_StdReturn : 0.14000000059604645\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.02\n",
            "Train_EnvstepsSoFar : 615\n",
            "TimeSinceStart : 4.98650860786438\n",
            "Training Loss : 232.83120727539062\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.5247148275375366\n",
            "Eval_StdReturn : 0.49938884377479553\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.5247148288973384\n",
            "Train_AverageReturn : 2.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0\n",
            "Train_EnvstepsSoFar : 715\n",
            "TimeSinceStart : 5.601571798324585\n",
            "Training Loss : -740.7412109375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0050251483917236\n",
            "Eval_StdReturn : 0.07070979475975037\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0050251256281406\n",
            "Train_AverageReturn : 1.5384615659713745\n",
            "Train_StdReturn : 0.49851852655410767\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.5384615384615385\n",
            "Train_EnvstepsSoFar : 815\n",
            "TimeSinceStart : 6.2845458984375\n",
            "Training Loss : -474.4573974609375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 915\n",
            "TimeSinceStart : 6.947483062744141\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1015\n",
            "TimeSinceStart : 7.653175115585327\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1115\n",
            "TimeSinceStart : 8.348474740982056\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1215\n",
            "TimeSinceStart : 9.148165941238403\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1315\n",
            "TimeSinceStart : 10.445453882217407\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1415\n",
            "TimeSinceStart : 11.719835042953491\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1515\n",
            "TimeSinceStart : 12.390244483947754\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1615\n",
            "TimeSinceStart : 13.067455530166626\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1715\n",
            "TimeSinceStart : 13.745168447494507\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1815\n",
            "TimeSinceStart : 14.420470237731934\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1915\n",
            "TimeSinceStart : 15.098474979400635\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2015\n",
            "TimeSinceStart : 15.752132177352905\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2115\n",
            "TimeSinceStart : 16.41047239303589\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2215\n",
            "TimeSinceStart : 17.063544988632202\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2315\n",
            "TimeSinceStart : 17.740919589996338\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2415\n",
            "TimeSinceStart : 18.439305782318115\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2515\n",
            "TimeSinceStart : 19.136189460754395\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2615\n",
            "TimeSinceStart : 19.822554111480713\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2715\n",
            "TimeSinceStart : 20.50114417076111\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2815\n",
            "TimeSinceStart : 21.149571418762207\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2915\n",
            "TimeSinceStart : 21.83550715446472\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3015\n",
            "TimeSinceStart : 22.495078086853027\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3115\n",
            "TimeSinceStart : 23.161003828048706\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3215\n",
            "TimeSinceStart : 24.131442308425903\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3315\n",
            "TimeSinceStart : 25.427571535110474\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3415\n",
            "TimeSinceStart : 26.34116816520691\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3515\n",
            "TimeSinceStart : 27.031498670578003\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3615\n",
            "TimeSinceStart : 27.7405788898468\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3715\n",
            "TimeSinceStart : 28.438420057296753\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3815\n",
            "TimeSinceStart : 29.290695905685425\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3915\n",
            "TimeSinceStart : 30.48417091369629\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4015\n",
            "TimeSinceStart : 31.573572158813477\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4115\n",
            "TimeSinceStart : 32.41112542152405\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4215\n",
            "TimeSinceStart : 33.591004848480225\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4315\n",
            "TimeSinceStart : 34.65439558029175\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4415\n",
            "TimeSinceStart : 35.31568098068237\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4515\n",
            "TimeSinceStart : 35.995845556259155\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4615\n",
            "TimeSinceStart : 36.66976046562195\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4715\n",
            "TimeSinceStart : 37.34054255485535\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4815\n",
            "TimeSinceStart : 37.99304175376892\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4915\n",
            "TimeSinceStart : 38.686882734298706\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5015\n",
            "TimeSinceStart : 39.358903884887695\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5115\n",
            "TimeSinceStart : 40.00419616699219\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5215\n",
            "TimeSinceStart : 40.67616391181946\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5315\n",
            "TimeSinceStart : 41.61314105987549\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5415\n",
            "TimeSinceStart : 42.26260209083557\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5515\n",
            "TimeSinceStart : 42.898993492126465\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5615\n",
            "TimeSinceStart : 43.56143760681152\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5715\n",
            "TimeSinceStart : 44.214751958847046\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5815\n",
            "TimeSinceStart : 44.85611844062805\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5915\n",
            "TimeSinceStart : 45.529696226119995\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6015\n",
            "TimeSinceStart : 46.21745300292969\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6115\n",
            "TimeSinceStart : 46.89011287689209\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6215\n",
            "TimeSinceStart : 47.563764333724976\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6315\n",
            "TimeSinceStart : 48.20925736427307\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6415\n",
            "TimeSinceStart : 48.87830400466919\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6515\n",
            "TimeSinceStart : 49.55431771278381\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6615\n",
            "TimeSinceStart : 50.1976363658905\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6715\n",
            "TimeSinceStart : 50.84335112571716\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6815\n",
            "TimeSinceStart : 51.48970031738281\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6915\n",
            "TimeSinceStart : 52.13093900680542\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7015\n",
            "TimeSinceStart : 52.78161263465881\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7115\n",
            "TimeSinceStart : 53.43273067474365\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7215\n",
            "TimeSinceStart : 54.09664964675903\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7315\n",
            "TimeSinceStart : 54.79540038108826\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7415\n",
            "TimeSinceStart : 55.448909282684326\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7515\n",
            "TimeSinceStart : 56.084882736206055\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7615\n",
            "TimeSinceStart : 56.73843550682068\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7715\n",
            "TimeSinceStart : 57.39967203140259\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7815\n",
            "TimeSinceStart : 58.05587434768677\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7915\n",
            "TimeSinceStart : 58.73285794258118\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8015\n",
            "TimeSinceStart : 59.37217426300049\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8115\n",
            "TimeSinceStart : 60.016796588897705\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8215\n",
            "TimeSinceStart : 60.66329550743103\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8315\n",
            "TimeSinceStart : 61.29962921142578\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8415\n",
            "TimeSinceStart : 61.95785593986511\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8515\n",
            "TimeSinceStart : 62.59213185310364\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8615\n",
            "TimeSinceStart : 63.23608922958374\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8715\n",
            "TimeSinceStart : 63.91655611991882\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8815\n",
            "TimeSinceStart : 64.55804514884949\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8915\n",
            "TimeSinceStart : 65.21246767044067\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9015\n",
            "TimeSinceStart : 65.85520815849304\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9115\n",
            "TimeSinceStart : 66.51093864440918\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9215\n",
            "TimeSinceStart : 67.18075513839722\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9315\n",
            "TimeSinceStart : 67.83644723892212\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9415\n",
            "TimeSinceStart : 68.48336553573608\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9515\n",
            "TimeSinceStart : 69.13723301887512\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9615\n",
            "TimeSinceStart : 69.78037238121033\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9715\n",
            "TimeSinceStart : 70.43469142913818\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9815\n",
            "TimeSinceStart : 71.08877468109131\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9915\n",
            "TimeSinceStart : 71.73288083076477\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10015\n",
            "TimeSinceStart : 72.39003109931946\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name InvertedPendulum-v2 \\\n",
        "--ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 120 -lr 0.05 -rtg \\\n",
        "--exp_name q2_b120_r05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmD9V_WyyGbv",
        "outputId": "6beeb8de-032b-400f-c3eb-7bfbd356c940"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q2_b120_r05_InvertedPendulum-v2_04-02-2022_17-33-54\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.4512195587158203\n",
            "Eval_StdReturn : 0.5097211003303528\n",
            "Eval_MaxReturn : 4.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.451219512195122\n",
            "Train_AverageReturn : 7.875\n",
            "Train_StdReturn : 3.2379584312438965\n",
            "Train_MaxReturn : 14.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 7.875\n",
            "Train_EnvstepsSoFar : 126\n",
            "TimeSinceStart : 0.5600748062133789\n",
            "Training Loss : -14.594827651977539\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.282051086425781\n",
            "Eval_StdReturn : 3.104576587677002\n",
            "Eval_MaxReturn : 21.0\n",
            "Eval_MinReturn : 4.0\n",
            "Eval_AverageEpLen : 10.282051282051283\n",
            "Train_AverageReturn : 2.4897959232330322\n",
            "Train_StdReturn : 0.4998959004878998\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.489795918367347\n",
            "Train_EnvstepsSoFar : 248\n",
            "TimeSinceStart : 1.1005477905273438\n",
            "Training Loss : 1039.438232421875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.574467658996582\n",
            "Eval_StdReturn : 3.5294785499572754\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 8.574468085106384\n",
            "Train_AverageReturn : 10.0\n",
            "Train_StdReturn : 4.949747562408447\n",
            "Train_MaxReturn : 21.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 10.0\n",
            "Train_EnvstepsSoFar : 368\n",
            "TimeSinceStart : 1.6268622875213623\n",
            "Training Loss : -348.0208740234375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.259740352630615\n",
            "Eval_StdReturn : 1.6308915615081787\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.259740259740259\n",
            "Train_AverageReturn : 7.05555534362793\n",
            "Train_StdReturn : 2.738049268722534\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 7.055555555555555\n",
            "Train_EnvstepsSoFar : 495\n",
            "TimeSinceStart : 2.1966593265533447\n",
            "Training Loss : 156.04664611816406\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.666666507720947\n",
            "Eval_StdReturn : 2.8905980587005615\n",
            "Eval_MaxReturn : 18.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 6.666666666666667\n",
            "Train_AverageReturn : 5.809523582458496\n",
            "Train_StdReturn : 2.107109308242798\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.809523809523809\n",
            "Train_EnvstepsSoFar : 617\n",
            "TimeSinceStart : 2.741396427154541\n",
            "Training Loss : -348.53997802734375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.333333015441895\n",
            "Eval_StdReturn : 5.766096591949463\n",
            "Eval_MaxReturn : 29.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 10.333333333333334\n",
            "Train_AverageReturn : 5.904761791229248\n",
            "Train_StdReturn : 1.71560800075531\n",
            "Train_MaxReturn : 9.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.904761904761905\n",
            "Train_EnvstepsSoFar : 741\n",
            "TimeSinceStart : 3.2965316772460938\n",
            "Training Loss : -530.2098388671875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.0600004196167\n",
            "Eval_StdReturn : 4.369942665100098\n",
            "Eval_MaxReturn : 23.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 8.06\n",
            "Train_AverageReturn : 10.166666984558105\n",
            "Train_StdReturn : 4.412734031677246\n",
            "Train_MaxReturn : 22.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 10.166666666666666\n",
            "Train_EnvstepsSoFar : 863\n",
            "TimeSinceStart : 3.8525807857513428\n",
            "Training Loss : 403.95745849609375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.757143020629883\n",
            "Eval_StdReturn : 2.924945116043091\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.757142857142857\n",
            "Train_AverageReturn : 8.266666412353516\n",
            "Train_StdReturn : 3.2957887649536133\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 8.266666666666667\n",
            "Train_EnvstepsSoFar : 987\n",
            "TimeSinceStart : 4.391949415206909\n",
            "Training Loss : -899.8110961914062\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.263157844543457\n",
            "Eval_StdReturn : 2.5668349266052246\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 5.2631578947368425\n",
            "Train_AverageReturn : 5.333333492279053\n",
            "Train_StdReturn : 1.624465823173523\n",
            "Train_MaxReturn : 9.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.333333333333333\n",
            "Train_EnvstepsSoFar : 1115\n",
            "TimeSinceStart : 4.9414026737213135\n",
            "Training Loss : -439.515869140625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.809523820877075\n",
            "Eval_StdReturn : 1.5436211824417114\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.8095238095238093\n",
            "Train_AverageReturn : 4.800000190734863\n",
            "Train_StdReturn : 2.11660099029541\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.8\n",
            "Train_EnvstepsSoFar : 1235\n",
            "TimeSinceStart : 5.48380970954895\n",
            "Training Loss : -2056.8759765625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.539823055267334\n",
            "Eval_StdReturn : 2.471198320388794\n",
            "Eval_MaxReturn : 21.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.5398230088495577\n",
            "Train_AverageReturn : 4.333333492279053\n",
            "Train_StdReturn : 2.27058482170105\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.333333333333333\n",
            "Train_EnvstepsSoFar : 1365\n",
            "TimeSinceStart : 6.047613859176636\n",
            "Training Loss : -5575.4990234375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.8591549396514893\n",
            "Eval_StdReturn : 2.0472981929779053\n",
            "Eval_MaxReturn : 17.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.859154929577465\n",
            "Train_AverageReturn : 3.270270347595215\n",
            "Train_StdReturn : 1.5183320045471191\n",
            "Train_MaxReturn : 7.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.27027027027027\n",
            "Train_EnvstepsSoFar : 1486\n",
            "TimeSinceStart : 6.614275932312012\n",
            "Training Loss : -11910.83203125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.005000114440918\n",
            "Eval_StdReturn : 0.07053367793560028\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.005\n",
            "Train_AverageReturn : 2.9069766998291016\n",
            "Train_StdReturn : 1.950717806816101\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.9069767441860463\n",
            "Train_EnvstepsSoFar : 1611\n",
            "TimeSinceStart : 7.216042518615723\n",
            "Training Loss : -27383.60546875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.015075445175171\n",
            "Eval_StdReturn : 0.1218528151512146\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0150753768844223\n",
            "Train_AverageReturn : 2.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0\n",
            "Train_EnvstepsSoFar : 1731\n",
            "TimeSinceStart : 7.804764270782471\n",
            "Training Loss : -13700.357421875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.0\n",
            "Eval_StdReturn : 3.92203426361084\n",
            "Eval_MaxReturn : 24.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.0\n",
            "Train_AverageReturn : 2.03389835357666\n",
            "Train_StdReturn : 0.18096742033958435\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0338983050847457\n",
            "Train_EnvstepsSoFar : 1851\n",
            "TimeSinceStart : 8.36210298538208\n",
            "Training Loss : -29689.6328125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.568181991577148\n",
            "Eval_StdReturn : 2.060425043106079\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 4.568181818181818\n",
            "Train_AverageReturn : 7.823529243469238\n",
            "Train_StdReturn : 8.438255310058594\n",
            "Train_MaxReturn : 40.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 7.823529411764706\n",
            "Train_EnvstepsSoFar : 1984\n",
            "TimeSinceStart : 8.91590666770935\n",
            "Training Loss : 29345.158203125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.7383177280426025\n",
            "Eval_StdReturn : 2.154484272003174\n",
            "Eval_MaxReturn : 20.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 3.7383177570093458\n",
            "Train_AverageReturn : 4.0333333015441895\n",
            "Train_StdReturn : 0.9480975270271301\n",
            "Train_MaxReturn : 7.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.033333333333333\n",
            "Train_EnvstepsSoFar : 2105\n",
            "TimeSinceStart : 9.488526821136475\n",
            "Training Loss : 42723.66015625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.204081535339355\n",
            "Eval_StdReturn : 4.685973644256592\n",
            "Eval_MaxReturn : 27.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 8.204081632653061\n",
            "Train_AverageReturn : 3.4571428298950195\n",
            "Train_StdReturn : 1.2269091606140137\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 3.4571428571428573\n",
            "Train_EnvstepsSoFar : 2226\n",
            "TimeSinceStart : 10.045539617538452\n",
            "Training Loss : 19371.94921875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.4691357612609863\n",
            "Eval_StdReturn : 1.0609384775161743\n",
            "Eval_MaxReturn : 8.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.4691358024691357\n",
            "Train_AverageReturn : 8.785714149475098\n",
            "Train_StdReturn : 4.27916145324707\n",
            "Train_MaxReturn : 19.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 8.785714285714286\n",
            "Train_EnvstepsSoFar : 2349\n",
            "TimeSinceStart : 10.625513076782227\n",
            "Training Loss : 128458.6640625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.4814814329147339\n",
            "Eval_StdReturn : 0.4996569752693176\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.4814814814814814\n",
            "Train_AverageReturn : 2.5744681358337402\n",
            "Train_StdReturn : 1.4838805198669434\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.574468085106383\n",
            "Train_EnvstepsSoFar : 2470\n",
            "TimeSinceStart : 11.258226871490479\n",
            "Training Loss : 72746.8984375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0498688220977783\n",
            "Eval_StdReturn : 0.21767374873161316\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0498687664041995\n",
            "Train_AverageReturn : 1.4634146690368652\n",
            "Train_StdReturn : 0.4986597001552582\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.4634146341463414\n",
            "Train_EnvstepsSoFar : 2590\n",
            "TimeSinceStart : 11.928845882415771\n",
            "Training Loss : -483410.84375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0526316165924072\n",
            "Train_StdReturn : 0.2232968658208847\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0526315789473684\n",
            "Train_EnvstepsSoFar : 2710\n",
            "TimeSinceStart : 12.650063037872314\n",
            "Training Loss : -78197.953125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2830\n",
            "TimeSinceStart : 13.327783346176147\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2950\n",
            "TimeSinceStart : 13.997135639190674\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3070\n",
            "TimeSinceStart : 14.667382955551147\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3190\n",
            "TimeSinceStart : 15.362040519714355\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3310\n",
            "TimeSinceStart : 16.029590606689453\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3430\n",
            "TimeSinceStart : 16.7114520072937\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3550\n",
            "TimeSinceStart : 17.420623779296875\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3670\n",
            "TimeSinceStart : 18.099796295166016\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3790\n",
            "TimeSinceStart : 18.781006574630737\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3910\n",
            "TimeSinceStart : 19.93006467819214\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4030\n",
            "TimeSinceStart : 21.19962167739868\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4150\n",
            "TimeSinceStart : 22.00427556037903\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4270\n",
            "TimeSinceStart : 22.688312768936157\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4390\n",
            "TimeSinceStart : 23.354780673980713\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4510\n",
            "TimeSinceStart : 24.035656452178955\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4630\n",
            "TimeSinceStart : 24.722813844680786\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4750\n",
            "TimeSinceStart : 25.912723541259766\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4870\n",
            "TimeSinceStart : 26.584559679031372\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4990\n",
            "TimeSinceStart : 27.252248525619507\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5110\n",
            "TimeSinceStart : 27.941357374191284\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5230\n",
            "TimeSinceStart : 28.659374713897705\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5350\n",
            "TimeSinceStart : 29.37205934524536\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5470\n",
            "TimeSinceStart : 30.676252603530884\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5590\n",
            "TimeSinceStart : 32.03426909446716\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5710\n",
            "TimeSinceStart : 33.425153493881226\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5830\n",
            "TimeSinceStart : 34.45650815963745\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5950\n",
            "TimeSinceStart : 35.15683078765869\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6070\n",
            "TimeSinceStart : 35.8880558013916\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6190\n",
            "TimeSinceStart : 36.58951425552368\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6310\n",
            "TimeSinceStart : 37.29998588562012\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6430\n",
            "TimeSinceStart : 37.98983287811279\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6550\n",
            "TimeSinceStart : 38.67312002182007\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6670\n",
            "TimeSinceStart : 39.383208990097046\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6790\n",
            "TimeSinceStart : 40.09699535369873\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6910\n",
            "TimeSinceStart : 40.77105951309204\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7030\n",
            "TimeSinceStart : 41.47950601577759\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7150\n",
            "TimeSinceStart : 42.169933795928955\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7270\n",
            "TimeSinceStart : 42.863025188446045\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7390\n",
            "TimeSinceStart : 43.5846049785614\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7510\n",
            "TimeSinceStart : 44.28736662864685\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7630\n",
            "TimeSinceStart : 44.96912932395935\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7750\n",
            "TimeSinceStart : 45.64608550071716\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7870\n",
            "TimeSinceStart : 46.32214689254761\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7990\n",
            "TimeSinceStart : 46.990511655807495\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8110\n",
            "TimeSinceStart : 47.67761301994324\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8230\n",
            "TimeSinceStart : 48.40135145187378\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8350\n",
            "TimeSinceStart : 49.09812378883362\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8470\n",
            "TimeSinceStart : 49.78218865394592\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8590\n",
            "TimeSinceStart : 50.459308385849\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8710\n",
            "TimeSinceStart : 51.14117908477783\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8830\n",
            "TimeSinceStart : 51.860554933547974\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8950\n",
            "TimeSinceStart : 52.581496238708496\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9070\n",
            "TimeSinceStart : 53.26185965538025\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9190\n",
            "TimeSinceStart : 53.94465732574463\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9310\n",
            "TimeSinceStart : 54.64170217514038\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9430\n",
            "TimeSinceStart : 55.36697292327881\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9550\n",
            "TimeSinceStart : 56.079448223114014\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9670\n",
            "TimeSinceStart : 56.76018762588501\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9790\n",
            "TimeSinceStart : 57.45270299911499\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9910\n",
            "TimeSinceStart : 58.13181781768799\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10030\n",
            "TimeSinceStart : 58.805856466293335\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10150\n",
            "TimeSinceStart : 59.75361156463623\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10270\n",
            "TimeSinceStart : 61.078025579452515\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10390\n",
            "TimeSinceStart : 62.77109980583191\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10510\n",
            "TimeSinceStart : 63.53106737136841\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10630\n",
            "TimeSinceStart : 64.24635672569275\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10750\n",
            "TimeSinceStart : 64.9547266960144\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10870\n",
            "TimeSinceStart : 65.61728286743164\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10990\n",
            "TimeSinceStart : 66.29930591583252\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11110\n",
            "TimeSinceStart : 66.99574375152588\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11230\n",
            "TimeSinceStart : 67.70546913146973\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11350\n",
            "TimeSinceStart : 68.36907863616943\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11470\n",
            "TimeSinceStart : 69.04531121253967\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11590\n",
            "TimeSinceStart : 69.72509264945984\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11710\n",
            "TimeSinceStart : 70.40382599830627\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11830\n",
            "TimeSinceStart : 71.08701467514038\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11950\n",
            "TimeSinceStart : 71.80396914482117\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12070\n",
            "TimeSinceStart : 73.10971355438232\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name InvertedPendulum-v2 \\\n",
        "--ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 100 -lr 0.05 -rtg \\\n",
        "--exp_name q2_b100_r05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syIQuF02yKpb",
        "outputId": "25cdb720-ac78-41a1-c76d-a90d365646d7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q2_b100_r05_InvertedPendulum-v2_04-02-2022_17-35-13\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.3786981105804443\n",
            "Eval_StdReturn : 0.48506274819374084\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.378698224852071\n",
            "Train_AverageReturn : 7.4285712242126465\n",
            "Train_StdReturn : 3.0169589519500732\n",
            "Train_MaxReturn : 14.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 7.428571428571429\n",
            "Train_EnvstepsSoFar : 104\n",
            "TimeSinceStart : 0.5675415992736816\n",
            "Training Loss : -7.7160491943359375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.074999809265137\n",
            "Eval_StdReturn : 3.717442035675049\n",
            "Eval_MaxReturn : 22.0\n",
            "Eval_MinReturn : 4.0\n",
            "Eval_AverageEpLen : 10.075\n",
            "Train_AverageReturn : 2.2727272510528564\n",
            "Train_StdReturn : 0.44536176323890686\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.272727272727273\n",
            "Train_EnvstepsSoFar : 204\n",
            "TimeSinceStart : 1.0941753387451172\n",
            "Training Loss : 425.2433166503906\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 11.054054260253906\n",
            "Eval_StdReturn : 4.64377498626709\n",
            "Eval_MaxReturn : 23.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 11.054054054054054\n",
            "Train_AverageReturn : 8.75\n",
            "Train_StdReturn : 3.442988157272339\n",
            "Train_MaxReturn : 19.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 8.75\n",
            "Train_EnvstepsSoFar : 309\n",
            "TimeSinceStart : 1.610241174697876\n",
            "Training Loss : -661.357421875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.5892856121063232\n",
            "Eval_StdReturn : 0.807924747467041\n",
            "Eval_MaxReturn : 6.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.5892857142857144\n",
            "Train_AverageReturn : 9.272727012634277\n",
            "Train_StdReturn : 2.4155144691467285\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 9.272727272727273\n",
            "Train_EnvstepsSoFar : 411\n",
            "TimeSinceStart : 2.138681173324585\n",
            "Training Loss : 1746.64697265625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.2160000801086426\n",
            "Eval_StdReturn : 0.6995312571525574\n",
            "Eval_MaxReturn : 6.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.216\n",
            "Train_AverageReturn : 3.607142925262451\n",
            "Train_StdReturn : 0.7717922329902649\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.607142857142857\n",
            "Train_EnvstepsSoFar : 512\n",
            "TimeSinceStart : 2.6797468662261963\n",
            "Training Loss : -10.35110092163086\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.816901445388794\n",
            "Eval_StdReturn : 0.6458940505981445\n",
            "Eval_MaxReturn : 7.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.816901408450704\n",
            "Train_AverageReturn : 3.2903225421905518\n",
            "Train_StdReturn : 0.5788503289222717\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.2903225806451615\n",
            "Train_EnvstepsSoFar : 614\n",
            "TimeSinceStart : 3.215655565261841\n",
            "Training Loss : 694.7784423828125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.401197671890259\n",
            "Eval_StdReturn : 0.4901408553123474\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.4011976047904193\n",
            "Train_AverageReturn : 3.0\n",
            "Train_StdReturn : 0.5940885543823242\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.0\n",
            "Train_EnvstepsSoFar : 716\n",
            "TimeSinceStart : 3.784658670425415\n",
            "Training Loss : 9090.640625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.1052632331848145\n",
            "Eval_StdReturn : 0.3068922162055969\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.1052631578947367\n",
            "Train_AverageReturn : 2.4285714626312256\n",
            "Train_StdReturn : 0.49487167596817017\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.4285714285714284\n",
            "Train_EnvstepsSoFar : 818\n",
            "TimeSinceStart : 4.375215291976929\n",
            "Training Loss : 181.35389709472656\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.4096386432647705\n",
            "Eval_StdReturn : 0.5496141910552979\n",
            "Eval_MaxReturn : 4.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.4096385542168677\n",
            "Train_AverageReturn : 2.0199999809265137\n",
            "Train_StdReturn : 0.14000000059604645\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.02\n",
            "Train_EnvstepsSoFar : 919\n",
            "TimeSinceStart : 4.943457841873169\n",
            "Training Loss : -685.173828125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.115384578704834\n",
            "Eval_StdReturn : 1.244276762008667\n",
            "Eval_MaxReturn : 7.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.1153846153846154\n",
            "Train_AverageReturn : 2.487804889678955\n",
            "Train_StdReturn : 0.5464721322059631\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.4878048780487805\n",
            "Train_EnvstepsSoFar : 1021\n",
            "TimeSinceStart : 5.572248935699463\n",
            "Training Loss : -511.567138671875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.0225563049316406\n",
            "Eval_StdReturn : 1.2473500967025757\n",
            "Eval_MaxReturn : 9.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.0225563909774436\n",
            "Train_AverageReturn : 3.4193549156188965\n",
            "Train_StdReturn : 1.8452062606811523\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.4193548387096775\n",
            "Train_EnvstepsSoFar : 1127\n",
            "TimeSinceStart : 6.719751834869385\n",
            "Training Loss : -5059.8076171875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.7862069606781006\n",
            "Eval_StdReturn : 1.8761155605316162\n",
            "Eval_MaxReturn : 15.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.786206896551724\n",
            "Train_AverageReturn : 3.15625\n",
            "Train_StdReturn : 1.543319821357727\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.15625\n",
            "Train_EnvstepsSoFar : 1228\n",
            "TimeSinceStart : 7.621551036834717\n",
            "Training Loss : -4567.2587890625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.2528090476989746\n",
            "Eval_StdReturn : 0.8919854760169983\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.252808988764045\n",
            "Train_AverageReturn : 2.4634146690368652\n",
            "Train_StdReturn : 1.398563027381897\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.4634146341463414\n",
            "Train_EnvstepsSoFar : 1329\n",
            "TimeSinceStart : 8.72543478012085\n",
            "Training Loss : -3544.5654296875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.9950249195098877\n",
            "Eval_StdReturn : 0.15764160454273224\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9950248756218905\n",
            "Train_AverageReturn : 4.3913044929504395\n",
            "Train_StdReturn : 6.44534969329834\n",
            "Train_MaxReturn : 29.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.391304347826087\n",
            "Train_EnvstepsSoFar : 1430\n",
            "TimeSinceStart : 9.800264120101929\n",
            "Training Loss : -5810.853515625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.965686321258545\n",
            "Eval_StdReturn : 0.18203376233577728\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9656862745098038\n",
            "Train_AverageReturn : 2.0199999809265137\n",
            "Train_StdReturn : 0.14000000059604645\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.02\n",
            "Train_EnvstepsSoFar : 1531\n",
            "TimeSinceStart : 10.531267404556274\n",
            "Training Loss : -1048.80615234375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.56640625\n",
            "Eval_StdReturn : 0.4955706000328064\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.56640625\n",
            "Train_AverageReturn : 1.8703703880310059\n",
            "Train_StdReturn : 0.33589550852775574\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.8703703703703705\n",
            "Train_EnvstepsSoFar : 1632\n",
            "TimeSinceStart : 11.128366947174072\n",
            "Training Loss : -50825.6484375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0723860263824463\n",
            "Eval_StdReturn : 0.25912603735923767\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0723860589812333\n",
            "Train_AverageReturn : 1.5151515007019043\n",
            "Train_StdReturn : 0.4997704029083252\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.5151515151515151\n",
            "Train_EnvstepsSoFar : 1732\n",
            "TimeSinceStart : 11.75992488861084\n",
            "Training Loss : -191894.125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0989011526107788\n",
            "Train_StdReturn : 0.2985292077064514\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.098901098901099\n",
            "Train_EnvstepsSoFar : 1832\n",
            "TimeSinceStart : 12.398017644882202\n",
            "Training Loss : -83405.359375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 1932\n",
            "TimeSinceStart : 13.054007053375244\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2032\n",
            "TimeSinceStart : 13.725006103515625\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2132\n",
            "TimeSinceStart : 14.45859169960022\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2232\n",
            "TimeSinceStart : 15.66173267364502\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2332\n",
            "TimeSinceStart : 16.827091455459595\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2432\n",
            "TimeSinceStart : 17.473541259765625\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2532\n",
            "TimeSinceStart : 18.115573406219482\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2632\n",
            "TimeSinceStart : 18.78983163833618\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2732\n",
            "TimeSinceStart : 19.486531972885132\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2832\n",
            "TimeSinceStart : 20.170943021774292\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 2932\n",
            "TimeSinceStart : 20.83247208595276\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3032\n",
            "TimeSinceStart : 21.524075746536255\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3132\n",
            "TimeSinceStart : 22.194542407989502\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3232\n",
            "TimeSinceStart : 22.84696340560913\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3332\n",
            "TimeSinceStart : 23.4981107711792\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0025062561035156\n",
            "Eval_StdReturn : 0.04999983683228493\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0025062656641603\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 3432\n",
            "TimeSinceStart : 24.147571086883545\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.3875432014465332\n",
            "Eval_StdReturn : 0.4871893525123596\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.3875432525951557\n",
            "Train_AverageReturn : 1.0526316165924072\n",
            "Train_StdReturn : 0.510282039642334\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0526315789473684\n",
            "Train_EnvstepsSoFar : 3532\n",
            "TimeSinceStart : 24.765403270721436\n",
            "Training Loss : 1382850.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.9138755798339844\n",
            "Eval_StdReturn : 0.29711341857910156\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9138755980861244\n",
            "Train_AverageReturn : 1.3888888359069824\n",
            "Train_StdReturn : 0.487498015165329\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.3888888888888888\n",
            "Train_EnvstepsSoFar : 3632\n",
            "TimeSinceStart : 25.339195013046265\n",
            "Training Loss : -407242.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 7.754716873168945\n",
            "Eval_StdReturn : 3.4581358432769775\n",
            "Eval_MaxReturn : 20.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 7.754716981132075\n",
            "Train_AverageReturn : 1.9056603908538818\n",
            "Train_StdReturn : 0.29230067133903503\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.9056603773584906\n",
            "Train_EnvstepsSoFar : 3733\n",
            "TimeSinceStart : 25.85601234436035\n",
            "Training Loss : -120688.453125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.445945739746094\n",
            "Eval_StdReturn : 1.4532526731491089\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.445945945945946\n",
            "Train_AverageReturn : 7.133333206176758\n",
            "Train_StdReturn : 2.6042699813842773\n",
            "Train_MaxReturn : 11.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 7.133333333333334\n",
            "Train_EnvstepsSoFar : 3840\n",
            "TimeSinceStart : 26.379150390625\n",
            "Training Loss : 122368.53125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.451612949371338\n",
            "Eval_StdReturn : 2.7337639331817627\n",
            "Eval_MaxReturn : 16.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 6.451612903225806\n",
            "Train_AverageReturn : 5.263157844543457\n",
            "Train_StdReturn : 1.331485390663147\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 5.2631578947368425\n",
            "Train_EnvstepsSoFar : 3940\n",
            "TimeSinceStart : 26.884916067123413\n",
            "Training Loss : 32715.390625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 11.108108520507812\n",
            "Eval_StdReturn : 5.119035720825195\n",
            "Eval_MaxReturn : 25.0\n",
            "Eval_MinReturn : 5.0\n",
            "Eval_AverageEpLen : 11.108108108108109\n",
            "Train_AverageReturn : 7.692307472229004\n",
            "Train_StdReturn : 4.444263458251953\n",
            "Train_MaxReturn : 17.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 7.6923076923076925\n",
            "Train_EnvstepsSoFar : 4040\n",
            "TimeSinceStart : 27.400779962539673\n",
            "Training Loss : 12620.4091796875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.547618865966797\n",
            "Eval_StdReturn : 6.2686004638671875\n",
            "Eval_MaxReturn : 45.0\n",
            "Eval_MinReturn : 4.0\n",
            "Eval_AverageEpLen : 9.547619047619047\n",
            "Train_AverageReturn : 13.125\n",
            "Train_StdReturn : 7.218682289123535\n",
            "Train_MaxReturn : 31.0\n",
            "Train_MinReturn : 6.0\n",
            "Train_AverageEpLen : 13.125\n",
            "Train_EnvstepsSoFar : 4145\n",
            "TimeSinceStart : 27.906919956207275\n",
            "Training Loss : 23789.1796875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.590163707733154\n",
            "Eval_StdReturn : 2.8879458904266357\n",
            "Eval_MaxReturn : 16.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 6.590163934426229\n",
            "Train_AverageReturn : 8.583333015441895\n",
            "Train_StdReturn : 3.3029868602752686\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 8.583333333333334\n",
            "Train_EnvstepsSoFar : 4248\n",
            "TimeSinceStart : 28.422415733337402\n",
            "Training Loss : 12770.49609375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.302631378173828\n",
            "Eval_StdReturn : 2.2942702770233154\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 5.302631578947368\n",
            "Train_AverageReturn : 5.941176414489746\n",
            "Train_StdReturn : 2.5314629077911377\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.9411764705882355\n",
            "Train_EnvstepsSoFar : 4349\n",
            "TimeSinceStart : 28.93728232383728\n",
            "Training Loss : -3747.64990234375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.520547866821289\n",
            "Eval_StdReturn : 2.433271646499634\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.52054794520548\n",
            "Train_AverageReturn : 5.55555534362793\n",
            "Train_StdReturn : 3.1485838890075684\n",
            "Train_MaxReturn : 14.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.555555555555555\n",
            "Train_EnvstepsSoFar : 4449\n",
            "TimeSinceStart : 29.492471933364868\n",
            "Training Loss : -9523.564453125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.761904716491699\n",
            "Eval_StdReturn : 1.6876235008239746\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.761904761904762\n",
            "Train_AverageReturn : 4.952381134033203\n",
            "Train_StdReturn : 1.4952683448791504\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.9523809523809526\n",
            "Train_EnvstepsSoFar : 4553\n",
            "TimeSinceStart : 30.032032251358032\n",
            "Training Loss : 29241.01171875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.015075445175171\n",
            "Eval_StdReturn : 0.121852807700634\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0150753768844223\n",
            "Train_AverageReturn : 4.904761791229248\n",
            "Train_StdReturn : 1.4110029935836792\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.904761904761905\n",
            "Train_EnvstepsSoFar : 4656\n",
            "TimeSinceStart : 30.604065895080566\n",
            "Training Loss : 72819.890625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.579545497894287\n",
            "Eval_StdReturn : 1.8260836601257324\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.579545454545454\n",
            "Train_AverageReturn : 2.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0\n",
            "Train_EnvstepsSoFar : 4756\n",
            "TimeSinceStart : 31.132397174835205\n",
            "Training Loss : -775.8444213867188\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.638888835906982\n",
            "Eval_StdReturn : 3.2201859951019287\n",
            "Eval_MaxReturn : 20.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 5.638888888888889\n",
            "Train_AverageReturn : 4.636363506317139\n",
            "Train_StdReturn : 1.3329888582229614\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.636363636363637\n",
            "Train_EnvstepsSoFar : 4858\n",
            "TimeSinceStart : 31.65880823135376\n",
            "Training Loss : 24474.001953125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.830508708953857\n",
            "Eval_StdReturn : 4.651051998138428\n",
            "Eval_MaxReturn : 24.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.830508474576271\n",
            "Train_AverageReturn : 6.294117450714111\n",
            "Train_StdReturn : 3.907228708267212\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 6.294117647058823\n",
            "Train_EnvstepsSoFar : 4965\n",
            "TimeSinceStart : 32.16726469993591\n",
            "Training Loss : -53057.1953125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.076923370361328\n",
            "Eval_StdReturn : 4.362630367279053\n",
            "Eval_MaxReturn : 21.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 8.076923076923077\n",
            "Train_AverageReturn : 5.75\n",
            "Train_StdReturn : 3.41869854927063\n",
            "Train_MaxReturn : 16.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 5.75\n",
            "Train_EnvstepsSoFar : 5080\n",
            "TimeSinceStart : 32.708378076553345\n",
            "Training Loss : -31749.669921875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.680850982666016\n",
            "Eval_StdReturn : 6.510569095611572\n",
            "Eval_MaxReturn : 34.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 8.680851063829786\n",
            "Train_AverageReturn : 8.5\n",
            "Train_StdReturn : 4.873397350311279\n",
            "Train_MaxReturn : 17.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 8.5\n",
            "Train_EnvstepsSoFar : 5182\n",
            "TimeSinceStart : 33.220399618148804\n",
            "Training Loss : 26142.5234375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.17391300201416\n",
            "Eval_StdReturn : 5.760828495025635\n",
            "Eval_MaxReturn : 30.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 9.173913043478262\n",
            "Train_AverageReturn : 9.636363983154297\n",
            "Train_StdReturn : 4.791521072387695\n",
            "Train_MaxReturn : 21.0\n",
            "Train_MinReturn : 6.0\n",
            "Train_AverageEpLen : 9.636363636363637\n",
            "Train_EnvstepsSoFar : 5288\n",
            "TimeSinceStart : 33.756935834884644\n",
            "Training Loss : 443.864013671875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.458333015441895\n",
            "Eval_StdReturn : 3.668323516845703\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 4.0\n",
            "Eval_AverageEpLen : 8.458333333333334\n",
            "Train_AverageReturn : 7.285714149475098\n",
            "Train_StdReturn : 2.6302788257598877\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 7.285714285714286\n",
            "Train_EnvstepsSoFar : 5390\n",
            "TimeSinceStart : 34.280027866363525\n",
            "Training Loss : 12062.134765625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.5\n",
            "Eval_StdReturn : 4.752192497253418\n",
            "Eval_MaxReturn : 23.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 8.5\n",
            "Train_AverageReturn : 7.769230842590332\n",
            "Train_StdReturn : 2.7777645587921143\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 7.769230769230769\n",
            "Train_EnvstepsSoFar : 5491\n",
            "TimeSinceStart : 34.80674695968628\n",
            "Training Loss : -8698.16796875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.564102172851562\n",
            "Eval_StdReturn : 6.213281631469727\n",
            "Eval_MaxReturn : 35.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 10.564102564102564\n",
            "Train_AverageReturn : 8.076923370361328\n",
            "Train_StdReturn : 4.99940824508667\n",
            "Train_MaxReturn : 20.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 8.076923076923077\n",
            "Train_EnvstepsSoFar : 5596\n",
            "TimeSinceStart : 35.338446855545044\n",
            "Training Loss : -4892.2900390625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.95555591583252\n",
            "Eval_StdReturn : 4.713835716247559\n",
            "Eval_MaxReturn : 21.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 8.955555555555556\n",
            "Train_AverageReturn : 10.100000381469727\n",
            "Train_StdReturn : 7.917701721191406\n",
            "Train_MaxReturn : 32.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 10.1\n",
            "Train_EnvstepsSoFar : 5697\n",
            "TimeSinceStart : 35.846405267715454\n",
            "Training Loss : -10269.626953125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 11.485713958740234\n",
            "Eval_StdReturn : 6.330747604370117\n",
            "Eval_MaxReturn : 29.0\n",
            "Eval_MinReturn : 4.0\n",
            "Eval_AverageEpLen : 11.485714285714286\n",
            "Train_AverageReturn : 9.545454978942871\n",
            "Train_StdReturn : 4.53999662399292\n",
            "Train_MaxReturn : 16.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 9.545454545454545\n",
            "Train_EnvstepsSoFar : 5802\n",
            "TimeSinceStart : 36.34583830833435\n",
            "Training Loss : 6233.8740234375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.181818008422852\n",
            "Eval_StdReturn : 5.1929707527160645\n",
            "Eval_MaxReturn : 27.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 9.181818181818182\n",
            "Train_AverageReturn : 9.333333015441895\n",
            "Train_StdReturn : 4.588633060455322\n",
            "Train_MaxReturn : 18.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 9.333333333333334\n",
            "Train_EnvstepsSoFar : 5914\n",
            "TimeSinceStart : 36.86813306808472\n",
            "Training Loss : 4270.515625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.25\n",
            "Eval_StdReturn : 5.135106563568115\n",
            "Eval_MaxReturn : 25.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 9.25\n",
            "Train_AverageReturn : 14.285714149475098\n",
            "Train_StdReturn : 5.749888896942139\n",
            "Train_MaxReturn : 22.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 14.285714285714286\n",
            "Train_EnvstepsSoFar : 6014\n",
            "TimeSinceStart : 37.37445306777954\n",
            "Training Loss : 3422.564453125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.631579399108887\n",
            "Eval_StdReturn : 5.819301605224609\n",
            "Eval_MaxReturn : 24.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 10.631578947368421\n",
            "Train_AverageReturn : 10.0\n",
            "Train_StdReturn : 4.880387306213379\n",
            "Train_MaxReturn : 18.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 10.0\n",
            "Train_EnvstepsSoFar : 6124\n",
            "TimeSinceStart : 37.902690410614014\n",
            "Training Loss : 8900.357421875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.547618865966797\n",
            "Eval_StdReturn : 5.896739482879639\n",
            "Eval_MaxReturn : 31.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 9.547619047619047\n",
            "Train_AverageReturn : 16.428571701049805\n",
            "Train_StdReturn : 9.24055004119873\n",
            "Train_MaxReturn : 37.0\n",
            "Train_MinReturn : 7.0\n",
            "Train_AverageEpLen : 16.428571428571427\n",
            "Train_EnvstepsSoFar : 6239\n",
            "TimeSinceStart : 38.42658257484436\n",
            "Training Loss : 1209.885498046875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.744680404663086\n",
            "Eval_StdReturn : 4.9614176750183105\n",
            "Eval_MaxReturn : 25.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 8.74468085106383\n",
            "Train_AverageReturn : 14.571428298950195\n",
            "Train_StdReturn : 6.521549701690674\n",
            "Train_MaxReturn : 24.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 14.571428571428571\n",
            "Train_EnvstepsSoFar : 6341\n",
            "TimeSinceStart : 38.951990365982056\n",
            "Training Loss : -3511.17578125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.372093200683594\n",
            "Eval_StdReturn : 6.32464075088501\n",
            "Eval_MaxReturn : 30.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 9.372093023255815\n",
            "Train_AverageReturn : 20.600000381469727\n",
            "Train_StdReturn : 11.689311027526855\n",
            "Train_MaxReturn : 43.0\n",
            "Train_MinReturn : 11.0\n",
            "Train_AverageEpLen : 20.6\n",
            "Train_EnvstepsSoFar : 6444\n",
            "TimeSinceStart : 39.50598120689392\n",
            "Training Loss : -4116.9970703125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.829268455505371\n",
            "Eval_StdReturn : 5.621832370758057\n",
            "Eval_MaxReturn : 29.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 9.829268292682928\n",
            "Train_AverageReturn : 7.142857074737549\n",
            "Train_StdReturn : 2.9726643562316895\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 7.142857142857143\n",
            "Train_EnvstepsSoFar : 6544\n",
            "TimeSinceStart : 40.032782793045044\n",
            "Training Loss : -26248.544921875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.181818008422852\n",
            "Eval_StdReturn : 6.143261909484863\n",
            "Eval_MaxReturn : 30.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 9.181818181818182\n",
            "Train_AverageReturn : 9.818181991577148\n",
            "Train_StdReturn : 5.812497138977051\n",
            "Train_MaxReturn : 23.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 9.818181818181818\n",
            "Train_EnvstepsSoFar : 6652\n",
            "TimeSinceStart : 40.56021451950073\n",
            "Training Loss : -19750.1875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.307692527770996\n",
            "Eval_StdReturn : 7.328042030334473\n",
            "Eval_MaxReturn : 37.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 10.307692307692308\n",
            "Train_AverageReturn : 10.600000381469727\n",
            "Train_StdReturn : 4.58693790435791\n",
            "Train_MaxReturn : 18.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 10.6\n",
            "Train_EnvstepsSoFar : 6758\n",
            "TimeSinceStart : 41.07904505729675\n",
            "Training Loss : -21668.404296875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.256410598754883\n",
            "Eval_StdReturn : 7.634999752044678\n",
            "Eval_MaxReturn : 34.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 10.256410256410257\n",
            "Train_AverageReturn : 10.699999809265137\n",
            "Train_StdReturn : 6.466065406799316\n",
            "Train_MaxReturn : 23.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 10.7\n",
            "Train_EnvstepsSoFar : 6865\n",
            "TimeSinceStart : 41.63004755973816\n",
            "Training Loss : -19253.2421875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 9.780488014221191\n",
            "Eval_StdReturn : 6.682748794555664\n",
            "Eval_MaxReturn : 30.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 9.78048780487805\n",
            "Train_AverageReturn : 11.333333015441895\n",
            "Train_StdReturn : 8.53749942779541\n",
            "Train_MaxReturn : 31.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 11.333333333333334\n",
            "Train_EnvstepsSoFar : 6967\n",
            "TimeSinceStart : 42.16236639022827\n",
            "Training Loss : -6751.529296875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 7.462963104248047\n",
            "Eval_StdReturn : 4.503961086273193\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 7.462962962962963\n",
            "Train_AverageReturn : 7.846153736114502\n",
            "Train_StdReturn : 4.09213399887085\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 7.846153846153846\n",
            "Train_EnvstepsSoFar : 7069\n",
            "TimeSinceStart : 42.70112228393555\n",
            "Training Loss : -43784.3046875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 7.921568393707275\n",
            "Eval_StdReturn : 6.443854808807373\n",
            "Eval_MaxReturn : 31.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 7.921568627450981\n",
            "Train_AverageReturn : 14.0\n",
            "Train_StdReturn : 6.928203105926514\n",
            "Train_MaxReturn : 21.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 14.0\n",
            "Train_EnvstepsSoFar : 7181\n",
            "TimeSinceStart : 43.24120378494263\n",
            "Training Loss : 1501.701171875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.933333396911621\n",
            "Eval_StdReturn : 6.611102104187012\n",
            "Eval_MaxReturn : 30.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 8.933333333333334\n",
            "Train_AverageReturn : 9.454545021057129\n",
            "Train_StdReturn : 7.487730979919434\n",
            "Train_MaxReturn : 27.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 9.454545454545455\n",
            "Train_EnvstepsSoFar : 7285\n",
            "TimeSinceStart : 43.80040621757507\n",
            "Training Loss : -13889.078125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.683333396911621\n",
            "Eval_StdReturn : 5.527180194854736\n",
            "Eval_MaxReturn : 25.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.683333333333334\n",
            "Train_AverageReturn : 5.684210300445557\n",
            "Train_StdReturn : 3.372530460357666\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 5.684210526315789\n",
            "Train_EnvstepsSoFar : 7393\n",
            "TimeSinceStart : 44.337289571762085\n",
            "Training Loss : -11910.037109375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.84782600402832\n",
            "Eval_StdReturn : 7.006106853485107\n",
            "Eval_MaxReturn : 32.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 8.847826086956522\n",
            "Train_AverageReturn : 7.199999809265137\n",
            "Train_StdReturn : 5.153639316558838\n",
            "Train_MaxReturn : 22.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 7.2\n",
            "Train_EnvstepsSoFar : 7501\n",
            "TimeSinceStart : 44.8666832447052\n",
            "Training Loss : -4636.09033203125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.4285712242126465\n",
            "Eval_StdReturn : 5.156947135925293\n",
            "Eval_MaxReturn : 26.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.428571428571429\n",
            "Train_AverageReturn : 5.777777671813965\n",
            "Train_StdReturn : 3.5048468112945557\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 5.777777777777778\n",
            "Train_EnvstepsSoFar : 7605\n",
            "TimeSinceStart : 45.39924931526184\n",
            "Training Loss : 26140.0625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.25\n",
            "Eval_StdReturn : 5.74456262588501\n",
            "Eval_MaxReturn : 39.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.25\n",
            "Train_AverageReturn : 7.769230842590332\n",
            "Train_StdReturn : 7.9629316329956055\n",
            "Train_MaxReturn : 27.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 7.769230769230769\n",
            "Train_EnvstepsSoFar : 7706\n",
            "TimeSinceStart : 45.91748261451721\n",
            "Training Loss : -1623.71826171875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.453333377838135\n",
            "Eval_StdReturn : 4.186624050140381\n",
            "Eval_MaxReturn : 20.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 5.453333333333333\n",
            "Train_AverageReturn : 9.454545021057129\n",
            "Train_StdReturn : 7.16511344909668\n",
            "Train_MaxReturn : 30.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 9.454545454545455\n",
            "Train_EnvstepsSoFar : 7810\n",
            "TimeSinceStart : 46.443785190582275\n",
            "Training Loss : -51302.015625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.134020805358887\n",
            "Eval_StdReturn : 3.2031843662261963\n",
            "Eval_MaxReturn : 15.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.134020618556701\n",
            "Train_AverageReturn : 5.045454502105713\n",
            "Train_StdReturn : 3.649125337600708\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 5.045454545454546\n",
            "Train_EnvstepsSoFar : 7921\n",
            "TimeSinceStart : 46.99318552017212\n",
            "Training Loss : 3097.94140625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.761904716491699\n",
            "Eval_StdReturn : 2.3221304416656494\n",
            "Eval_MaxReturn : 24.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.761904761904762\n",
            "Train_AverageReturn : 4.590909004211426\n",
            "Train_StdReturn : 5.175634860992432\n",
            "Train_MaxReturn : 22.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.590909090909091\n",
            "Train_EnvstepsSoFar : 8022\n",
            "TimeSinceStart : 47.542534589767456\n",
            "Training Loss : -2822.05517578125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.5641026496887207\n",
            "Eval_StdReturn : 1.9121055603027344\n",
            "Eval_MaxReturn : 15.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.5641025641025643\n",
            "Train_AverageReturn : 2.8205127716064453\n",
            "Train_StdReturn : 2.405888080596924\n",
            "Train_MaxReturn : 16.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.8205128205128207\n",
            "Train_EnvstepsSoFar : 8132\n",
            "TimeSinceStart : 48.11794590950012\n",
            "Training Loss : -19030.11328125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.5705127716064453\n",
            "Eval_StdReturn : 2.7482211589813232\n",
            "Eval_MaxReturn : 24.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.5705128205128207\n",
            "Train_AverageReturn : 2.3181817531585693\n",
            "Train_StdReturn : 1.2205201387405396\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.3181818181818183\n",
            "Train_EnvstepsSoFar : 8234\n",
            "TimeSinceStart : 48.6714129447937\n",
            "Training Loss : -22934.67578125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.2087912559509277\n",
            "Eval_StdReturn : 1.1484962701797485\n",
            "Eval_MaxReturn : 15.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.208791208791209\n",
            "Train_AverageReturn : 2.0833332538604736\n",
            "Train_StdReturn : 0.2763853967189789\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0833333333333335\n",
            "Train_EnvstepsSoFar : 8334\n",
            "TimeSinceStart : 49.255291223526\n",
            "Training Loss : -30492.7109375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.2154695987701416\n",
            "Eval_StdReturn : 1.1089707612991333\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.2154696132596685\n",
            "Train_AverageReturn : 2.0612244606018066\n",
            "Train_StdReturn : 0.23974162340164185\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.061224489795918\n",
            "Train_EnvstepsSoFar : 8435\n",
            "TimeSinceStart : 49.82529330253601\n",
            "Training Loss : -31006.591796875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.030456781387329\n",
            "Eval_StdReturn : 0.19920220971107483\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.030456852791878\n",
            "Train_AverageReturn : 2.222222328186035\n",
            "Train_StdReturn : 0.9162456393241882\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.2222222222222223\n",
            "Train_EnvstepsSoFar : 8535\n",
            "TimeSinceStart : 50.37394642829895\n",
            "Training Loss : -27708.5234375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.22360679507255554\n",
            "Eval_MaxReturn : 4.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.040816307067871\n",
            "Train_StdReturn : 0.19786445796489716\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0408163265306123\n",
            "Train_EnvstepsSoFar : 8635\n",
            "TimeSinceStart : 50.93396878242493\n",
            "Training Loss : -23396.60546875\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.26457512378692627\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.5897436141967773\n",
            "Train_StdReturn : 3.476794719696045\n",
            "Train_MaxReturn : 24.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.58974358974359\n",
            "Train_EnvstepsSoFar : 8736\n",
            "TimeSinceStart : 51.490599393844604\n",
            "Training Loss : -28853.974609375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0459184646606445\n",
            "Eval_StdReturn : 0.6722893714904785\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.045918367346939\n",
            "Train_AverageReturn : 1.9056603908538818\n",
            "Train_StdReturn : 0.29230067133903503\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.9056603773584906\n",
            "Train_EnvstepsSoFar : 8837\n",
            "TimeSinceStart : 52.067161083221436\n",
            "Training Loss : -76351.625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.909523844718933\n",
            "Eval_StdReturn : 0.5401981472969055\n",
            "Eval_MaxReturn : 8.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9095238095238096\n",
            "Train_AverageReturn : 2.127659559249878\n",
            "Train_StdReturn : 1.4820488691329956\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.127659574468085\n",
            "Train_EnvstepsSoFar : 8937\n",
            "TimeSinceStart : 52.65719985961914\n",
            "Training Loss : -22811.109375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.3125\n",
            "Eval_StdReturn : 5.040073871612549\n",
            "Eval_MaxReturn : 26.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.3125\n",
            "Train_AverageReturn : 1.8703703880310059\n",
            "Train_StdReturn : 0.3871212303638458\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.8703703703703705\n",
            "Train_EnvstepsSoFar : 9038\n",
            "TimeSinceStart : 53.22426199913025\n",
            "Training Loss : -170034.78125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.77647066116333\n",
            "Eval_StdReturn : 4.486257553100586\n",
            "Eval_MaxReturn : 28.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.776470588235294\n",
            "Train_AverageReturn : 6.0\n",
            "Train_StdReturn : 5.280374526977539\n",
            "Train_MaxReturn : 20.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 6.0\n",
            "Train_EnvstepsSoFar : 9140\n",
            "TimeSinceStart : 53.76540780067444\n",
            "Training Loss : 26498.98828125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.3466668128967285\n",
            "Eval_StdReturn : 4.862080097198486\n",
            "Eval_MaxReturn : 22.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 5.346666666666667\n",
            "Train_AverageReturn : 4.761904716491699\n",
            "Train_StdReturn : 4.173667907714844\n",
            "Train_MaxReturn : 17.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.761904761904762\n",
            "Train_EnvstepsSoFar : 9240\n",
            "TimeSinceStart : 54.30279731750488\n",
            "Training Loss : 56430.9609375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.864285707473755\n",
            "Eval_StdReturn : 2.925773859024048\n",
            "Eval_MaxReturn : 21.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.8642857142857143\n",
            "Train_AverageReturn : 6.3125\n",
            "Train_StdReturn : 6.110633850097656\n",
            "Train_MaxReturn : 21.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 6.3125\n",
            "Train_EnvstepsSoFar : 9341\n",
            "TimeSinceStart : 54.83144474029541\n",
            "Training Loss : 16692.2578125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.203296661376953\n",
            "Eval_StdReturn : 1.5397266149520874\n",
            "Eval_MaxReturn : 12.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.2032967032967035\n",
            "Train_AverageReturn : 3.117647171020508\n",
            "Train_StdReturn : 2.529411792755127\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 3.1176470588235294\n",
            "Train_EnvstepsSoFar : 9447\n",
            "TimeSinceStart : 55.422157764434814\n",
            "Training Loss : 48437.828125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.6326531171798706\n",
            "Eval_StdReturn : 0.5956947207450867\n",
            "Eval_MaxReturn : 7.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.6326530612244898\n",
            "Train_AverageReturn : 2.0612244606018066\n",
            "Train_StdReturn : 1.3310754299163818\n",
            "Train_MaxReturn : 11.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.061224489795918\n",
            "Train_EnvstepsSoFar : 9548\n",
            "TimeSinceStart : 56.042885303497314\n",
            "Training Loss : 5175.3994140625\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.2121212482452393\n",
            "Eval_StdReturn : 0.40881022810935974\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.2121212121212122\n",
            "Train_AverageReturn : 1.5303030014038086\n",
            "Train_StdReturn : 0.4990808963775635\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.5303030303030303\n",
            "Train_EnvstepsSoFar : 9649\n",
            "TimeSinceStart : 56.66712045669556\n",
            "Training Loss : -393693.25\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0152283906936646\n",
            "Eval_StdReturn : 0.12246029078960419\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.015228426395939\n",
            "Train_AverageReturn : 1.1904761791229248\n",
            "Train_StdReturn : 0.39267677068710327\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.1904761904761905\n",
            "Train_EnvstepsSoFar : 9749\n",
            "TimeSinceStart : 57.334418296813965\n",
            "Training Loss : -291976.8125\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0204081535339355\n",
            "Train_StdReturn : 0.14139191806316376\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0204081632653061\n",
            "Train_EnvstepsSoFar : 9849\n",
            "TimeSinceStart : 57.97894763946533\n",
            "Training Loss : -21394.396484375\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9949\n",
            "TimeSinceStart : 58.64021587371826\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10049\n",
            "TimeSinceStart : 59.305601835250854\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10149\n",
            "TimeSinceStart : 59.951537132263184\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10249\n",
            "TimeSinceStart : 60.62364959716797\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.4285712242126465\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name InvertedPendulum-v2 \\\n",
        "--ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 120 -lr 0.03 -rtg \\\n",
        "--exp_name q2_b120_r03"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbHnyB5enSR7",
        "outputId": "bb17ae26-0dd6-4f7e-8446-c43b2196259e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q2_b120_r03_InvertedPendulum-v2_04-02-2022_17-36-22\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.007518768310547\n",
            "Eval_StdReturn : 0.498060017824173\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.007518796992481\n",
            "Train_AverageReturn : 7.875\n",
            "Train_StdReturn : 3.2379584312438965\n",
            "Train_MaxReturn : 14.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 7.875\n",
            "Train_EnvstepsSoFar : 126\n",
            "TimeSinceStart : 0.5688455104827881\n",
            "Training Loss : -14.594827651977539\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.5\n",
            "Eval_StdReturn : 1.793806552886963\n",
            "Eval_MaxReturn : 12.0\n",
            "Eval_MinReturn : 4.0\n",
            "Eval_AverageEpLen : 6.5\n",
            "Train_AverageReturn : 3.0\n",
            "Train_StdReturn : 0.4472135901451111\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.0\n",
            "Train_EnvstepsSoFar : 246\n",
            "TimeSinceStart : 1.1336274147033691\n",
            "Training Loss : 414.792236328125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 27.866666793823242\n",
            "Eval_StdReturn : 15.19151782989502\n",
            "Eval_MaxReturn : 60.0\n",
            "Eval_MinReturn : 9.0\n",
            "Eval_AverageEpLen : 27.866666666666667\n",
            "Train_AverageReturn : 6.050000190734863\n",
            "Train_StdReturn : 2.2017040252685547\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 6.05\n",
            "Train_EnvstepsSoFar : 367\n",
            "TimeSinceStart : 1.7361674308776855\n",
            "Training Loss : -509.168212890625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.86486530303955\n",
            "Eval_StdReturn : 3.8984553813934326\n",
            "Eval_MaxReturn : 22.0\n",
            "Eval_MinReturn : 6.0\n",
            "Eval_AverageEpLen : 10.864864864864865\n",
            "Train_AverageReturn : 30.0\n",
            "Train_StdReturn : 12.469964027404785\n",
            "Train_MaxReturn : 50.0\n",
            "Train_MinReturn : 16.0\n",
            "Train_AverageEpLen : 30.0\n",
            "Train_EnvstepsSoFar : 487\n",
            "TimeSinceStart : 2.2877988815307617\n",
            "Training Loss : -16.82489776611328\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.7971014976501465\n",
            "Eval_StdReturn : 1.620339035987854\n",
            "Eval_MaxReturn : 12.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.797101449275362\n",
            "Train_AverageReturn : 13.55555534362793\n",
            "Train_StdReturn : 7.119786739349365\n",
            "Train_MaxReturn : 29.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 13.555555555555555\n",
            "Train_EnvstepsSoFar : 609\n",
            "TimeSinceStart : 2.8561785221099854\n",
            "Training Loss : 0.5870590209960938\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.962963104248047\n",
            "Eval_StdReturn : 1.4353941679000854\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.962962962962963\n",
            "Train_AverageReturn : 5.041666507720947\n",
            "Train_StdReturn : 1.2409393787384033\n",
            "Train_MaxReturn : 7.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.041666666666667\n",
            "Train_EnvstepsSoFar : 730\n",
            "TimeSinceStart : 3.4257662296295166\n",
            "Training Loss : -872.9203491210938\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.568181991577148\n",
            "Eval_StdReturn : 1.1946419477462769\n",
            "Eval_MaxReturn : 8.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.568181818181818\n",
            "Train_AverageReturn : 4.692307472229004\n",
            "Train_StdReturn : 1.2015769481658936\n",
            "Train_MaxReturn : 9.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.6923076923076925\n",
            "Train_EnvstepsSoFar : 852\n",
            "TimeSinceStart : 3.991607189178467\n",
            "Training Loss : -689.891357421875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.878048896789551\n",
            "Eval_StdReturn : 1.5412241220474243\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.878048780487805\n",
            "Train_AverageReturn : 5.041666507720947\n",
            "Train_StdReturn : 1.1357510089874268\n",
            "Train_MaxReturn : 7.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.041666666666667\n",
            "Train_EnvstepsSoFar : 973\n",
            "TimeSinceStart : 4.544520139694214\n",
            "Training Loss : -1805.84814453125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.395604610443115\n",
            "Eval_StdReturn : 1.4516313076019287\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.395604395604396\n",
            "Train_AverageReturn : 5.5\n",
            "Train_StdReturn : 1.8027756214141846\n",
            "Train_MaxReturn : 9.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 5.5\n",
            "Train_EnvstepsSoFar : 1094\n",
            "TimeSinceStart : 5.133481740951538\n",
            "Training Loss : 5219.4521484375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.5714285373687744\n",
            "Eval_StdReturn : 0.8630746603012085\n",
            "Eval_MaxReturn : 7.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.5714285714285716\n",
            "Train_AverageReturn : 4.1724138259887695\n",
            "Train_StdReturn : 1.1467785835266113\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.172413793103448\n",
            "Train_EnvstepsSoFar : 1215\n",
            "TimeSinceStart : 5.712438106536865\n",
            "Training Loss : -6631.32861328125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.8041958808898926\n",
            "Eval_StdReturn : 0.5824801325798035\n",
            "Eval_MaxReturn : 4.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.804195804195804\n",
            "Train_AverageReturn : 3.5142858028411865\n",
            "Train_StdReturn : 0.9062143564224243\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.5142857142857142\n",
            "Train_EnvstepsSoFar : 1338\n",
            "TimeSinceStart : 6.299292802810669\n",
            "Training Loss : -19552.6875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.6845638751983643\n",
            "Eval_StdReturn : 0.8280910849571228\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.684563758389262\n",
            "Train_AverageReturn : 2.904761791229248\n",
            "Train_StdReturn : 0.5694409608840942\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.9047619047619047\n",
            "Train_EnvstepsSoFar : 1460\n",
            "TimeSinceStart : 6.887686252593994\n",
            "Training Loss : -59312.2109375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.312138795852661\n",
            "Eval_StdReturn : 0.5747304558753967\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.3121387283236996\n",
            "Train_AverageReturn : 2.9512195587158203\n",
            "Train_StdReturn : 0.9865243434906006\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.951219512195122\n",
            "Train_EnvstepsSoFar : 1581\n",
            "TimeSinceStart : 7.492365121841431\n",
            "Training Loss : -32743.306640625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.040816307067871\n",
            "Eval_StdReturn : 0.19786447286605835\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0408163265306123\n",
            "Train_AverageReturn : 2.1607143878936768\n",
            "Train_StdReturn : 0.3672672212123871\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.1607142857142856\n",
            "Train_EnvstepsSoFar : 1702\n",
            "TimeSinceStart : 8.121378898620605\n",
            "Training Loss : -83748.734375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.9801980257034302\n",
            "Eval_StdReturn : 0.1393193006515503\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9801980198019802\n",
            "Train_AverageReturn : 2.03389835357666\n",
            "Train_StdReturn : 0.18096742033958435\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0338983050847457\n",
            "Train_EnvstepsSoFar : 1822\n",
            "TimeSinceStart : 8.737473964691162\n",
            "Training Loss : -34991.0703125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.8957345485687256\n",
            "Eval_StdReturn : 0.3056045472621918\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.8957345971563981\n",
            "Train_AverageReturn : 1.951612949371338\n",
            "Train_StdReturn : 0.2798282504081726\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.9516129032258065\n",
            "Train_EnvstepsSoFar : 1943\n",
            "TimeSinceStart : 9.356582641601562\n",
            "Training Loss : -86956.078125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.7665197849273682\n",
            "Eval_StdReturn : 0.4230451285839081\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.7665198237885462\n",
            "Train_AverageReturn : 1.9672131538391113\n",
            "Train_StdReturn : 0.17807838320732117\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.9672131147540983\n",
            "Train_EnvstepsSoFar : 2063\n",
            "TimeSinceStart : 9.959851264953613\n",
            "Training Loss : -24317.41015625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.2349727153778076\n",
            "Eval_StdReturn : 1.147332787513733\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.2349726775956285\n",
            "Train_AverageReturn : 1.6438356637954712\n",
            "Train_StdReturn : 0.4788646101951599\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.643835616438356\n",
            "Train_EnvstepsSoFar : 2183\n",
            "TimeSinceStart : 10.567286491394043\n",
            "Training Loss : -506188.875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.7971014976501465\n",
            "Eval_StdReturn : 4.9154582023620605\n",
            "Eval_MaxReturn : 23.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 5.797101449275362\n",
            "Train_AverageReturn : 2.4000000953674316\n",
            "Train_StdReturn : 1.3416407108306885\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.4\n",
            "Train_EnvstepsSoFar : 2303\n",
            "TimeSinceStart : 11.164809942245483\n",
            "Training Loss : 192563.71875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.38095235824585\n",
            "Eval_StdReturn : 2.9082727432250977\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.380952380952381\n",
            "Train_AverageReturn : 6.833333492279053\n",
            "Train_StdReturn : 5.688682556152344\n",
            "Train_MaxReturn : 22.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 6.833333333333333\n",
            "Train_EnvstepsSoFar : 2426\n",
            "TimeSinceStart : 11.755331039428711\n",
            "Training Loss : 224958.734375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.7093024253845215\n",
            "Eval_StdReturn : 3.3231613636016846\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.709302325581396\n",
            "Train_AverageReturn : 7.588235378265381\n",
            "Train_StdReturn : 3.3441576957702637\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 7.588235294117647\n",
            "Train_EnvstepsSoFar : 2555\n",
            "TimeSinceStart : 12.344346284866333\n",
            "Training Loss : 279496.40625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.6143791675567627\n",
            "Eval_StdReturn : 2.349034547805786\n",
            "Eval_MaxReturn : 18.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.6143790849673203\n",
            "Train_AverageReturn : 5.545454502105713\n",
            "Train_StdReturn : 3.985511064529419\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 5.545454545454546\n",
            "Train_EnvstepsSoFar : 2677\n",
            "TimeSinceStart : 12.93639850616455\n",
            "Training Loss : 235528.765625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.127659559249878\n",
            "Eval_StdReturn : 0.9252261519432068\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.127659574468085\n",
            "Train_AverageReturn : 2.2407407760620117\n",
            "Train_StdReturn : 0.8805551528930664\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.240740740740741\n",
            "Train_EnvstepsSoFar : 2798\n",
            "TimeSinceStart : 13.57099437713623\n",
            "Training Loss : 36195.0390625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.970370292663574\n",
            "Eval_StdReturn : 2.9538190364837646\n",
            "Eval_MaxReturn : 23.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.9703703703703703\n",
            "Train_AverageReturn : 2.03389835357666\n",
            "Train_StdReturn : 0.18096742033958435\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0338983050847457\n",
            "Train_EnvstepsSoFar : 2918\n",
            "TimeSinceStart : 14.18926739692688\n",
            "Training Loss : -28491.6953125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.3333332538604736\n",
            "Eval_StdReturn : 2.927835702896118\n",
            "Eval_MaxReturn : 21.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.3333333333333335\n",
            "Train_AverageReturn : 2.6666667461395264\n",
            "Train_StdReturn : 2.581989049911499\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.6666666666666665\n",
            "Train_EnvstepsSoFar : 3038\n",
            "TimeSinceStart : 14.791727781295776\n",
            "Training Loss : 79264.890625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.1052632331848145\n",
            "Eval_StdReturn : 0.3068922162055969\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.1052631578947367\n",
            "Train_AverageReturn : 2.857142925262451\n",
            "Train_StdReturn : 1.9219462871551514\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.857142857142857\n",
            "Train_EnvstepsSoFar : 3158\n",
            "TimeSinceStart : 15.421349287033081\n",
            "Training Loss : 18785.96484375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.846153736114502\n",
            "Eval_StdReturn : 3.7359204292297363\n",
            "Eval_MaxReturn : 25.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.8461538461538463\n",
            "Train_AverageReturn : 2.068965435028076\n",
            "Train_StdReturn : 0.2533954679965973\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0689655172413794\n",
            "Train_EnvstepsSoFar : 3278\n",
            "TimeSinceStart : 16.003727197647095\n",
            "Training Loss : -108435.59375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.36734676361084\n",
            "Eval_StdReturn : 5.2826008796691895\n",
            "Eval_MaxReturn : 16.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 8.36734693877551\n",
            "Train_AverageReturn : 4.0\n",
            "Train_StdReturn : 3.5683796405792236\n",
            "Train_MaxReturn : 21.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.0\n",
            "Train_EnvstepsSoFar : 3398\n",
            "TimeSinceStart : 16.57456374168396\n",
            "Training Loss : 24394.48046875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 8.574467658996582\n",
            "Eval_StdReturn : 5.7306365966796875\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 8.574468085106384\n",
            "Train_AverageReturn : 8.928571701049805\n",
            "Train_StdReturn : 5.311232089996338\n",
            "Train_MaxReturn : 16.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 8.928571428571429\n",
            "Train_EnvstepsSoFar : 3523\n",
            "TimeSinceStart : 17.1238751411438\n",
            "Training Loss : 19461.734375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.605262756347656\n",
            "Eval_StdReturn : 7.645223617553711\n",
            "Eval_MaxReturn : 26.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 10.605263157894736\n",
            "Train_AverageReturn : 10.230769157409668\n",
            "Train_StdReturn : 4.041207790374756\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 10.23076923076923\n",
            "Train_EnvstepsSoFar : 3656\n",
            "TimeSinceStart : 17.71604633331299\n",
            "Training Loss : 25258.3046875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.144330024719238\n",
            "Eval_StdReturn : 1.9530029296875\n",
            "Eval_MaxReturn : 13.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.144329896907217\n",
            "Train_AverageReturn : 15.11111068725586\n",
            "Train_StdReturn : 5.56665563583374\n",
            "Train_MaxReturn : 22.0\n",
            "Train_MinReturn : 5.0\n",
            "Train_AverageEpLen : 15.11111111111111\n",
            "Train_EnvstepsSoFar : 3792\n",
            "TimeSinceStart : 18.287978649139404\n",
            "Training Loss : 25809.6484375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.603896141052246\n",
            "Eval_StdReturn : 0.7063609957695007\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.603896103896104\n",
            "Train_AverageReturn : 4.392857074737549\n",
            "Train_StdReturn : 1.5887839794158936\n",
            "Train_MaxReturn : 7.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.392857142857143\n",
            "Train_EnvstepsSoFar : 3915\n",
            "TimeSinceStart : 18.86418628692627\n",
            "Training Loss : -44872.625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.4188034534454346\n",
            "Eval_StdReturn : 1.1999951601028442\n",
            "Eval_MaxReturn : 7.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.4188034188034186\n",
            "Train_AverageReturn : 2.711111068725586\n",
            "Train_StdReturn : 0.7489910125732422\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.7111111111111112\n",
            "Train_EnvstepsSoFar : 4037\n",
            "TimeSinceStart : 19.443713188171387\n",
            "Training Loss : -184374.328125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.314049482345581\n",
            "Eval_StdReturn : 1.0987074375152588\n",
            "Eval_MaxReturn : 6.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.3140495867768593\n",
            "Train_AverageReturn : 3.4857141971588135\n",
            "Train_StdReturn : 1.1306490898132324\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.4857142857142858\n",
            "Train_EnvstepsSoFar : 4159\n",
            "TimeSinceStart : 20.021372318267822\n",
            "Training Loss : -145518.34375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.823943614959717\n",
            "Eval_StdReturn : 0.9061549305915833\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.823943661971831\n",
            "Train_AverageReturn : 3.2432432174682617\n",
            "Train_StdReturn : 1.1005011796951294\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.2432432432432434\n",
            "Train_EnvstepsSoFar : 4279\n",
            "TimeSinceStart : 20.620835542678833\n",
            "Training Loss : -200052.625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.395209550857544\n",
            "Eval_StdReturn : 0.7337712049484253\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.395209580838323\n",
            "Train_AverageReturn : 3.0250000953674316\n",
            "Train_StdReturn : 1.1288821697235107\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.025\n",
            "Train_EnvstepsSoFar : 4400\n",
            "TimeSinceStart : 21.257151126861572\n",
            "Training Loss : -231286.40625\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.17391300201416\n",
            "Eval_StdReturn : 0.5338273644447327\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.1739130434782608\n",
            "Train_AverageReturn : 2.4693877696990967\n",
            "Train_StdReturn : 0.7586788535118103\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.4693877551020407\n",
            "Train_EnvstepsSoFar : 4521\n",
            "TimeSinceStart : 22.425615549087524\n",
            "Training Loss : -229753.859375\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0459184646606445\n",
            "Eval_StdReturn : 0.7374346852302551\n",
            "Eval_MaxReturn : 7.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.045918367346939\n",
            "Train_AverageReturn : 2.307692289352417\n",
            "Train_StdReturn : 0.7216024398803711\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.3076923076923075\n",
            "Train_EnvstepsSoFar : 4641\n",
            "TimeSinceStart : 23.04290509223938\n",
            "Training Loss : -211496.96875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.5564202070236206\n",
            "Eval_StdReturn : 0.4968065619468689\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.556420233463035\n",
            "Train_AverageReturn : 2.142857074737549\n",
            "Train_StdReturn : 0.8329930305480957\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.142857142857143\n",
            "Train_EnvstepsSoFar : 4761\n",
            "TimeSinceStart : 23.698699712753296\n",
            "Training Loss : -250718.078125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.1834319829940796\n",
            "Eval_StdReturn : 0.6023308634757996\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.183431952662722\n",
            "Train_AverageReturn : 1.5125000476837158\n",
            "Train_StdReturn : 0.4998437464237213\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.5125\n",
            "Train_EnvstepsSoFar : 4882\n",
            "TimeSinceStart : 24.380834817886353\n",
            "Training Loss : -836565.6875\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.1214953660964966\n",
            "Train_StdReturn : 0.3267020285129547\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.1214953271028036\n",
            "Train_EnvstepsSoFar : 5002\n",
            "TimeSinceStart : 25.076058626174927\n",
            "Training Loss : -414948.125\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5122\n",
            "TimeSinceStart : 25.770503044128418\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5242\n",
            "TimeSinceStart : 26.448551416397095\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5362\n",
            "TimeSinceStart : 27.139400959014893\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5482\n",
            "TimeSinceStart : 27.82888412475586\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5602\n",
            "TimeSinceStart : 28.52637481689453\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5722\n",
            "TimeSinceStart : 29.780943393707275\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5842\n",
            "TimeSinceStart : 31.00110650062561\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5962\n",
            "TimeSinceStart : 32.13572120666504\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6082\n",
            "TimeSinceStart : 32.908674478530884\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6202\n",
            "TimeSinceStart : 33.590195178985596\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6322\n",
            "TimeSinceStart : 34.356518507003784\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6442\n",
            "TimeSinceStart : 35.05259299278259\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6562\n",
            "TimeSinceStart : 35.734034299850464\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6682\n",
            "TimeSinceStart : 36.432366609573364\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6802\n",
            "TimeSinceStart : 37.15039372444153\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6922\n",
            "TimeSinceStart : 37.86742067337036\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7042\n",
            "TimeSinceStart : 38.57505440711975\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7162\n",
            "TimeSinceStart : 39.28648591041565\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7282\n",
            "TimeSinceStart : 40.00933289527893\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7402\n",
            "TimeSinceStart : 40.711718797683716\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7522\n",
            "TimeSinceStart : 41.46100831031799\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7642\n",
            "TimeSinceStart : 42.16585564613342\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7762\n",
            "TimeSinceStart : 42.83646535873413\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7882\n",
            "TimeSinceStart : 43.51925802230835\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8002\n",
            "TimeSinceStart : 44.20556950569153\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8122\n",
            "TimeSinceStart : 44.89189100265503\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8242\n",
            "TimeSinceStart : 45.61178708076477\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8362\n",
            "TimeSinceStart : 46.34912419319153\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8482\n",
            "TimeSinceStart : 47.03786635398865\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8602\n",
            "TimeSinceStart : 47.7257604598999\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8722\n",
            "TimeSinceStart : 48.41383123397827\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8842\n",
            "TimeSinceStart : 49.09940457344055\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8962\n",
            "TimeSinceStart : 49.79683017730713\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9082\n",
            "TimeSinceStart : 50.48420071601868\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9202\n",
            "TimeSinceStart : 51.16214656829834\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9322\n",
            "TimeSinceStart : 51.852866888046265\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9442\n",
            "TimeSinceStart : 52.551509857177734\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9562\n",
            "TimeSinceStart : 53.218066453933716\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9682\n",
            "TimeSinceStart : 53.895825147628784\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9802\n",
            "TimeSinceStart : 54.57058787345886\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9922\n",
            "TimeSinceStart : 55.26479411125183\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10042\n",
            "TimeSinceStart : 55.970539808273315\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10162\n",
            "TimeSinceStart : 56.693552017211914\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10282\n",
            "TimeSinceStart : 57.414249897003174\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10402\n",
            "TimeSinceStart : 58.09380555152893\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10522\n",
            "TimeSinceStart : 58.785658836364746\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10642\n",
            "TimeSinceStart : 59.46799159049988\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10762\n",
            "TimeSinceStart : 60.15763235092163\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10882\n",
            "TimeSinceStart : 60.855063915252686\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11002\n",
            "TimeSinceStart : 61.53988218307495\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11122\n",
            "TimeSinceStart : 62.204808950424194\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11242\n",
            "TimeSinceStart : 62.92306900024414\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11362\n",
            "TimeSinceStart : 63.60777139663696\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11482\n",
            "TimeSinceStart : 64.27713298797607\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11602\n",
            "TimeSinceStart : 64.98045492172241\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11722\n",
            "TimeSinceStart : 65.70112919807434\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11842\n",
            "TimeSinceStart : 66.38520884513855\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11962\n",
            "TimeSinceStart : 67.10080885887146\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12082\n",
            "TimeSinceStart : 67.81336784362793\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 7.875\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name InvertedPendulum-v2 \\\n",
        "--ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 200 -lr 0.03 -rtg \\\n",
        "--exp_name q2_b200_r03"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL9RZH-xyX-H",
        "outputId": "06812a27-25d3-48f5-80de-e6fb5a2c4852"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q2_b200_r03_InvertedPendulum-v2_04-02-2022_17-37-33\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.053030252456665\n",
            "Eval_StdReturn : 0.4491472542285919\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.053030303030303\n",
            "Train_AverageReturn : 8.038461685180664\n",
            "Train_StdReturn : 4.164693832397461\n",
            "Train_MaxReturn : 20.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 8.038461538461538\n",
            "Train_EnvstepsSoFar : 209\n",
            "TimeSinceStart : 0.6589186191558838\n",
            "Training Loss : -4.060882568359375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.451612949371338\n",
            "Eval_StdReturn : 2.2481348514556885\n",
            "Eval_MaxReturn : 16.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 6.451612903225806\n",
            "Train_AverageReturn : 3.0303030014038086\n",
            "Train_StdReturn : 0.45956823229789734\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.0303030303030303\n",
            "Train_EnvstepsSoFar : 409\n",
            "TimeSinceStart : 1.333871603012085\n",
            "Training Loss : 595.477783203125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 20.75\n",
            "Eval_StdReturn : 10.8068265914917\n",
            "Eval_MaxReturn : 44.0\n",
            "Eval_MinReturn : 6.0\n",
            "Eval_AverageEpLen : 20.75\n",
            "Train_AverageReturn : 6.25\n",
            "Train_StdReturn : 2.1505813598632812\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 6.25\n",
            "Train_EnvstepsSoFar : 609\n",
            "TimeSinceStart : 1.9900038242340088\n",
            "Training Loss : 343.5094299316406\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 16.074073791503906\n",
            "Eval_StdReturn : 8.939823150634766\n",
            "Eval_MaxReturn : 41.0\n",
            "Eval_MinReturn : 4.0\n",
            "Eval_AverageEpLen : 16.074074074074073\n",
            "Train_AverageReturn : 19.545454025268555\n",
            "Train_StdReturn : 11.219817161560059\n",
            "Train_MaxReturn : 43.0\n",
            "Train_MinReturn : 10.0\n",
            "Train_AverageEpLen : 19.545454545454547\n",
            "Train_EnvstepsSoFar : 824\n",
            "TimeSinceStart : 2.6821649074554443\n",
            "Training Loss : -1128.838623046875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 10.125\n",
            "Eval_StdReturn : 5.139005184173584\n",
            "Eval_MaxReturn : 26.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 10.125\n",
            "Train_AverageReturn : 17.75\n",
            "Train_StdReturn : 9.417404174804688\n",
            "Train_MaxReturn : 41.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 17.75\n",
            "Train_EnvstepsSoFar : 1037\n",
            "TimeSinceStart : 3.345538377761841\n",
            "Training Loss : 822.4147338867188\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 7.052631378173828\n",
            "Eval_StdReturn : 3.9131853580474854\n",
            "Eval_MaxReturn : 25.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 7.052631578947368\n",
            "Train_AverageReturn : 12.5625\n",
            "Train_StdReturn : 5.957859992980957\n",
            "Train_MaxReturn : 28.0\n",
            "Train_MinReturn : 6.0\n",
            "Train_AverageEpLen : 12.5625\n",
            "Train_EnvstepsSoFar : 1238\n",
            "TimeSinceStart : 4.008117198944092\n",
            "Training Loss : 356.699951171875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.432432651519775\n",
            "Eval_StdReturn : 2.3020617961883545\n",
            "Eval_MaxReturn : 17.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.4324324324324325\n",
            "Train_AverageReturn : 8.625\n",
            "Train_StdReturn : 4.279919147491455\n",
            "Train_MaxReturn : 17.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 8.625\n",
            "Train_EnvstepsSoFar : 1445\n",
            "TimeSinceStart : 4.6939780712127686\n",
            "Training Loss : 217.2564239501953\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.556818008422852\n",
            "Eval_StdReturn : 1.2956041097640991\n",
            "Eval_MaxReturn : 9.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 4.556818181818182\n",
            "Train_AverageReturn : 5.205128192901611\n",
            "Train_StdReturn : 1.8560985326766968\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.205128205128205\n",
            "Train_EnvstepsSoFar : 1648\n",
            "TimeSinceStart : 5.3334619998931885\n",
            "Training Loss : -284.8529968261719\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.406593322753906\n",
            "Eval_StdReturn : 1.3258049488067627\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 4.406593406593407\n",
            "Train_AverageReturn : 4.255319118499756\n",
            "Train_StdReturn : 1.0612735748291016\n",
            "Train_MaxReturn : 7.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.25531914893617\n",
            "Train_EnvstepsSoFar : 1848\n",
            "TimeSinceStart : 5.992068290710449\n",
            "Training Loss : 2003.777587890625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.333333492279053\n",
            "Eval_StdReturn : 1.378274917602539\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 4.333333333333333\n",
            "Train_AverageReturn : 4.6744184494018555\n",
            "Train_StdReturn : 1.93764328956604\n",
            "Train_MaxReturn : 11.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.674418604651163\n",
            "Train_EnvstepsSoFar : 2049\n",
            "TimeSinceStart : 6.626298189163208\n",
            "Training Loss : 489.49554443359375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.439560413360596\n",
            "Eval_StdReturn : 1.3441675901412964\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 4.43956043956044\n",
            "Train_AverageReturn : 4.545454502105713\n",
            "Train_StdReturn : 1.51439368724823\n",
            "Train_MaxReturn : 9.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.545454545454546\n",
            "Train_EnvstepsSoFar : 2249\n",
            "TimeSinceStart : 7.268354415893555\n",
            "Training Loss : 1376.9569091796875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.539325714111328\n",
            "Eval_StdReturn : 1.521798014640808\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 4.53932584269663\n",
            "Train_AverageReturn : 4.97560977935791\n",
            "Train_StdReturn : 2.1694979667663574\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.975609756097561\n",
            "Train_EnvstepsSoFar : 2453\n",
            "TimeSinceStart : 7.944937467575073\n",
            "Training Loss : 164.48475646972656\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.406593322753906\n",
            "Eval_StdReturn : 1.2223020792007446\n",
            "Eval_MaxReturn : 8.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.406593406593407\n",
            "Train_AverageReturn : 4.651162624359131\n",
            "Train_StdReturn : 1.3957363367080688\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.651162790697675\n",
            "Train_EnvstepsSoFar : 2653\n",
            "TimeSinceStart : 8.590618133544922\n",
            "Training Loss : 172.56271362304688\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.342105388641357\n",
            "Eval_StdReturn : 1.781133770942688\n",
            "Eval_MaxReturn : 11.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.342105263157895\n",
            "Train_AverageReturn : 4.208333492279053\n",
            "Train_StdReturn : 1.1539485454559326\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.208333333333333\n",
            "Train_EnvstepsSoFar : 2855\n",
            "TimeSinceStart : 9.234873533248901\n",
            "Training Loss : -546.5414428710938\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 6.7166666984558105\n",
            "Eval_StdReturn : 2.58258056640625\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 6.716666666666667\n",
            "Train_AverageReturn : 5.074999809265137\n",
            "Train_StdReturn : 1.0813764333724976\n",
            "Train_MaxReturn : 7.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 5.075\n",
            "Train_EnvstepsSoFar : 3058\n",
            "TimeSinceStart : 9.881115198135376\n",
            "Training Loss : -2168.810546875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 7.622641563415527\n",
            "Eval_StdReturn : 3.5193610191345215\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 7.622641509433962\n",
            "Train_AverageReturn : 6.34375\n",
            "Train_StdReturn : 2.39281964302063\n",
            "Train_MaxReturn : 17.0\n",
            "Train_MinReturn : 4.0\n",
            "Train_AverageEpLen : 6.34375\n",
            "Train_EnvstepsSoFar : 3261\n",
            "TimeSinceStart : 10.505630016326904\n",
            "Training Loss : -1997.339599609375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 7.070175647735596\n",
            "Eval_StdReturn : 4.719722270965576\n",
            "Eval_MaxReturn : 23.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 7.0701754385964914\n",
            "Train_AverageReturn : 6.483870983123779\n",
            "Train_StdReturn : 2.0770788192749023\n",
            "Train_MaxReturn : 11.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 6.483870967741935\n",
            "Train_EnvstepsSoFar : 3462\n",
            "TimeSinceStart : 11.139694690704346\n",
            "Training Loss : -5655.0791015625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 4.773809432983398\n",
            "Eval_StdReturn : 3.152290105819702\n",
            "Eval_MaxReturn : 19.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 4.773809523809524\n",
            "Train_AverageReturn : 6.838709831237793\n",
            "Train_StdReturn : 5.708492755889893\n",
            "Train_MaxReturn : 33.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 6.838709677419355\n",
            "Train_EnvstepsSoFar : 3674\n",
            "TimeSinceStart : 11.778247833251953\n",
            "Training Loss : -920.2520141601562\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.0300750732421875\n",
            "Eval_StdReturn : 1.1167881488800049\n",
            "Eval_MaxReturn : 8.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.030075187969925\n",
            "Train_AverageReturn : 4.902439117431641\n",
            "Train_StdReturn : 2.8353590965270996\n",
            "Train_MaxReturn : 14.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 4.902439024390244\n",
            "Train_EnvstepsSoFar : 3875\n",
            "TimeSinceStart : 12.42346739768982\n",
            "Training Loss : 7399.97802734375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.3179190158843994\n",
            "Eval_StdReturn : 0.5015262961387634\n",
            "Eval_MaxReturn : 4.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.3179190751445087\n",
            "Train_AverageReturn : 3.0923078060150146\n",
            "Train_StdReturn : 1.1730706691741943\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.0923076923076924\n",
            "Train_EnvstepsSoFar : 4076\n",
            "TimeSinceStart : 13.100948810577393\n",
            "Training Loss : -44092.328125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.3179190158843994\n",
            "Eval_StdReturn : 0.7112410664558411\n",
            "Eval_MaxReturn : 6.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.3179190751445087\n",
            "Train_AverageReturn : 2.3809523582458496\n",
            "Train_StdReturn : 0.5543070435523987\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.380952380952381\n",
            "Train_EnvstepsSoFar : 4276\n",
            "TimeSinceStart : 13.772655248641968\n",
            "Training Loss : -108020.828125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.1164021492004395\n",
            "Eval_StdReturn : 0.5415984392166138\n",
            "Eval_MaxReturn : 7.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.1164021164021163\n",
            "Train_AverageReturn : 2.4390244483947754\n",
            "Train_StdReturn : 0.7978256940841675\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.4390243902439024\n",
            "Train_EnvstepsSoFar : 4476\n",
            "TimeSinceStart : 14.440195083618164\n",
            "Training Loss : -74742.0234375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.22360679507255554\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.1847825050354004\n",
            "Train_StdReturn : 0.6243143677711487\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.1847826086956523\n",
            "Train_EnvstepsSoFar : 4677\n",
            "TimeSinceStart : 15.145942211151123\n",
            "Training Loss : -70789.640625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.9138755798339844\n",
            "Eval_StdReturn : 0.6578555703163147\n",
            "Eval_MaxReturn : 10.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9138755980861244\n",
            "Train_AverageReturn : 2.040816307067871\n",
            "Train_StdReturn : 0.42662328481674194\n",
            "Train_MaxReturn : 6.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.0408163265306123\n",
            "Train_EnvstepsSoFar : 4877\n",
            "TimeSinceStart : 15.854892492294312\n",
            "Training Loss : -22155.140625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.6949152946472168\n",
            "Eval_StdReturn : 0.4784947335720062\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.694915254237288\n",
            "Train_AverageReturn : 1.8867924213409424\n",
            "Train_StdReturn : 0.37165501713752747\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.8867924528301887\n",
            "Train_EnvstepsSoFar : 5077\n",
            "TimeSinceStart : 16.554112672805786\n",
            "Training Loss : -342362.8125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0984456539154053\n",
            "Eval_StdReturn : 1.1982922554016113\n",
            "Eval_MaxReturn : 12.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.098445595854922\n",
            "Train_AverageReturn : 1.6393442153930664\n",
            "Train_StdReturn : 0.4801907539367676\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.639344262295082\n",
            "Train_EnvstepsSoFar : 5277\n",
            "TimeSinceStart : 17.268558979034424\n",
            "Training Loss : -926095.75\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.7586207389831543\n",
            "Eval_StdReturn : 2.1664395332336426\n",
            "Eval_MaxReturn : 14.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.7586206896551726\n",
            "Train_AverageReturn : 2.3529412746429443\n",
            "Train_StdReturn : 1.671250820159912\n",
            "Train_MaxReturn : 12.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.3529411764705883\n",
            "Train_EnvstepsSoFar : 5477\n",
            "TimeSinceStart : 17.94032120704651\n",
            "Training Loss : 335183.8125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.638157844543457\n",
            "Eval_StdReturn : 2.959261655807495\n",
            "Eval_MaxReturn : 25.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 2.638157894736842\n",
            "Train_AverageReturn : 2.5443038940429688\n",
            "Train_StdReturn : 1.9860107898712158\n",
            "Train_MaxReturn : 15.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.5443037974683542\n",
            "Train_EnvstepsSoFar : 5678\n",
            "TimeSinceStart : 18.609217882156372\n",
            "Training Loss : 505046.625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.7777777910232544\n",
            "Eval_StdReturn : 1.2664717435836792\n",
            "Eval_MaxReturn : 13.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.7777777777777777\n",
            "Train_AverageReturn : 2.680000066757202\n",
            "Train_StdReturn : 2.0471768379211426\n",
            "Train_MaxReturn : 13.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 2.68\n",
            "Train_EnvstepsSoFar : 5879\n",
            "TimeSinceStart : 19.30549144744873\n",
            "Training Loss : 554326.0625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.3651877641677856\n",
            "Eval_StdReturn : 0.5287789702415466\n",
            "Eval_MaxReturn : 5.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.3651877133105803\n",
            "Train_AverageReturn : 1.7857142686843872\n",
            "Train_StdReturn : 0.880630612373352\n",
            "Train_MaxReturn : 8.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.7857142857142858\n",
            "Train_EnvstepsSoFar : 6079\n",
            "TimeSinceStart : 20.045976638793945\n",
            "Training Loss : -12170.53125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.2084592580795288\n",
            "Eval_StdReturn : 0.40620681643486023\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.2084592145015105\n",
            "Train_AverageReturn : 1.3958333730697632\n",
            "Train_StdReturn : 0.4890289306640625\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.3958333333333333\n",
            "Train_EnvstepsSoFar : 6280\n",
            "TimeSinceStart : 20.79373002052307\n",
            "Training Loss : -1374158.125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0608465671539307\n",
            "Eval_StdReturn : 0.23904864490032196\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.060846560846561\n",
            "Train_AverageReturn : 1.1428571939468384\n",
            "Train_StdReturn : 0.36589279770851135\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.1428571428571428\n",
            "Train_EnvstepsSoFar : 6480\n",
            "TimeSinceStart : 21.585614681243896\n",
            "Training Loss : -467350.5625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.007556676864624\n",
            "Eval_StdReturn : 0.08660006523132324\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0075566750629723\n",
            "Train_AverageReturn : 1.0752688646316528\n",
            "Train_StdReturn : 0.26382461190223694\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.075268817204301\n",
            "Train_EnvstepsSoFar : 6680\n",
            "TimeSinceStart : 22.393248319625854\n",
            "Training Loss : -371727.5\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0101009607315063\n",
            "Train_StdReturn : 0.09999489784240723\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0101010101010102\n",
            "Train_EnvstepsSoFar : 6880\n",
            "TimeSinceStart : 23.195853233337402\n",
            "Training Loss : -25521.8671875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7080\n",
            "TimeSinceStart : 24.02946925163269\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7280\n",
            "TimeSinceStart : 24.814414024353027\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7480\n",
            "TimeSinceStart : 25.61452054977417\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7680\n",
            "TimeSinceStart : 26.415523767471313\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7880\n",
            "TimeSinceStart : 27.212233066558838\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8080\n",
            "TimeSinceStart : 28.01475238800049\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8280\n",
            "TimeSinceStart : 28.84208035469055\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8480\n",
            "TimeSinceStart : 29.645159482955933\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8680\n",
            "TimeSinceStart : 30.44982624053955\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8880\n",
            "TimeSinceStart : 31.281699180603027\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9080\n",
            "TimeSinceStart : 32.10282301902771\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9280\n",
            "TimeSinceStart : 32.88154721260071\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9480\n",
            "TimeSinceStart : 33.66922044754028\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9680\n",
            "TimeSinceStart : 34.47362732887268\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9880\n",
            "TimeSinceStart : 35.29137349128723\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10080\n",
            "TimeSinceStart : 36.076552867889404\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10280\n",
            "TimeSinceStart : 36.90904664993286\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10480\n",
            "TimeSinceStart : 37.72496175765991\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10680\n",
            "TimeSinceStart : 38.53304362297058\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10880\n",
            "TimeSinceStart : 39.353962659835815\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11080\n",
            "TimeSinceStart : 40.140992164611816\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11280\n",
            "TimeSinceStart : 40.979562759399414\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11480\n",
            "TimeSinceStart : 41.805851221084595\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11680\n",
            "TimeSinceStart : 42.61484122276306\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11880\n",
            "TimeSinceStart : 43.416186571121216\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12080\n",
            "TimeSinceStart : 44.243529319763184\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12280\n",
            "TimeSinceStart : 45.05466938018799\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12480\n",
            "TimeSinceStart : 45.85220456123352\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12680\n",
            "TimeSinceStart : 46.644131898880005\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12880\n",
            "TimeSinceStart : 47.43052649497986\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13080\n",
            "TimeSinceStart : 48.235697507858276\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13280\n",
            "TimeSinceStart : 49.05921959877014\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13480\n",
            "TimeSinceStart : 49.918190479278564\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13680\n",
            "TimeSinceStart : 51.298583984375\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13880\n",
            "TimeSinceStart : 53.15369510650635\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14080\n",
            "TimeSinceStart : 54.79387187957764\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14280\n",
            "TimeSinceStart : 55.654746294021606\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14480\n",
            "TimeSinceStart : 56.51621198654175\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14680\n",
            "TimeSinceStart : 57.36797261238098\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14880\n",
            "TimeSinceStart : 58.16321778297424\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15080\n",
            "TimeSinceStart : 58.98021864891052\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15280\n",
            "TimeSinceStart : 59.7823531627655\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15480\n",
            "TimeSinceStart : 60.63620901107788\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15680\n",
            "TimeSinceStart : 61.48163151741028\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15880\n",
            "TimeSinceStart : 62.289658308029175\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16080\n",
            "TimeSinceStart : 63.101622343063354\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16280\n",
            "TimeSinceStart : 63.92165803909302\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16480\n",
            "TimeSinceStart : 64.72673749923706\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16680\n",
            "TimeSinceStart : 65.53649640083313\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16880\n",
            "TimeSinceStart : 66.39611029624939\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17080\n",
            "TimeSinceStart : 67.19823408126831\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17280\n",
            "TimeSinceStart : 67.98552393913269\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17480\n",
            "TimeSinceStart : 68.81049656867981\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17680\n",
            "TimeSinceStart : 69.61681413650513\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17880\n",
            "TimeSinceStart : 70.4133665561676\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18080\n",
            "TimeSinceStart : 71.19995427131653\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18280\n",
            "TimeSinceStart : 71.9871780872345\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18480\n",
            "TimeSinceStart : 72.82450294494629\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18680\n",
            "TimeSinceStart : 73.62663388252258\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18880\n",
            "TimeSinceStart : 74.4357054233551\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19080\n",
            "TimeSinceStart : 75.22246885299683\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19280\n",
            "TimeSinceStart : 76.0128698348999\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19480\n",
            "TimeSinceStart : 76.81296396255493\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19680\n",
            "TimeSinceStart : 77.63663220405579\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19880\n",
            "TimeSinceStart : 78.43128108978271\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 20080\n",
            "TimeSinceStart : 79.24347639083862\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name InvertedPendulum-v2 \\\n",
        "--ep_len 1000 --discount 0.9 -n 100 -l 2 -s 64 -b 200 -lr 0.01 -rtg \\\n",
        "--exp_name q2_b200_r01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZcn92jNygh0",
        "outputId": "65ffec3e-1989-4b79-97b9-e0f3b997abfa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q2_b200_r01_InvertedPendulum-v2_04-02-2022_17-38-56\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 5.141025543212891\n",
            "Eval_StdReturn : 1.9593586921691895\n",
            "Eval_MaxReturn : 12.0\n",
            "Eval_MinReturn : 3.0\n",
            "Eval_AverageEpLen : 5.141025641025641\n",
            "Train_AverageReturn : 8.038461685180664\n",
            "Train_StdReturn : 4.164693832397461\n",
            "Train_MaxReturn : 20.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 8.038461538461538\n",
            "Train_EnvstepsSoFar : 209\n",
            "TimeSinceStart : 0.8765451908111572\n",
            "Training Loss : -4.060882568359375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.580357074737549\n",
            "Eval_StdReturn : 0.786474883556366\n",
            "Eval_MaxReturn : 6.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.580357142857143\n",
            "Train_AverageReturn : 4.95121955871582\n",
            "Train_StdReturn : 1.695779800415039\n",
            "Train_MaxReturn : 10.0\n",
            "Train_MinReturn : 3.0\n",
            "Train_AverageEpLen : 4.951219512195122\n",
            "Train_EnvstepsSoFar : 412\n",
            "TimeSinceStart : 2.2912023067474365\n",
            "Training Loss : 28.856098175048828\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 3.0303030014038086\n",
            "Eval_StdReturn : 0.6388839483261108\n",
            "Eval_MaxReturn : 6.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 3.0303030303030303\n",
            "Train_AverageReturn : 3.5714285373687744\n",
            "Train_StdReturn : 0.7525466680526733\n",
            "Train_MaxReturn : 5.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.5714285714285716\n",
            "Train_EnvstepsSoFar : 612\n",
            "TimeSinceStart : 3.7078299522399902\n",
            "Training Loss : 19.38526153564453\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.5379745960235596\n",
            "Eval_StdReturn : 0.5110930800437927\n",
            "Eval_MaxReturn : 4.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.537974683544304\n",
            "Train_AverageReturn : 3.014925479888916\n",
            "Train_StdReturn : 0.5323150157928467\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 3.014925373134328\n",
            "Train_EnvstepsSoFar : 814\n",
            "TimeSinceStart : 4.937572479248047\n",
            "Training Loss : -33.6097412109375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.2277777194976807\n",
            "Eval_StdReturn : 0.4193984568119049\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.227777777777778\n",
            "Train_AverageReturn : 2.5443038940429688\n",
            "Train_StdReturn : 0.5228323340415955\n",
            "Train_MaxReturn : 4.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.5443037974683542\n",
            "Train_EnvstepsSoFar : 1015\n",
            "TimeSinceStart : 5.615034103393555\n",
            "Training Loss : 797.3751220703125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.030456781387329\n",
            "Eval_StdReturn : 0.17184071242809296\n",
            "Eval_MaxReturn : 3.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.030456852791878\n",
            "Train_AverageReturn : 2.219780206680298\n",
            "Train_StdReturn : 0.41409769654273987\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.21978021978022\n",
            "Train_EnvstepsSoFar : 1217\n",
            "TimeSinceStart : 6.3303749561309814\n",
            "Training Loss : 1257.4200439453125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.0303030014038086\n",
            "Train_StdReturn : 0.17141982913017273\n",
            "Train_MaxReturn : 3.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0303030303030303\n",
            "Train_EnvstepsSoFar : 1418\n",
            "TimeSinceStart : 7.1076271533966064\n",
            "Training Loss : 2395.1298828125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0\n",
            "Train_EnvstepsSoFar : 1618\n",
            "TimeSinceStart : 8.473040580749512\n",
            "Training Loss : 693.2285766601562\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 8 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0\n",
            "Train_EnvstepsSoFar : 1818\n",
            "TimeSinceStart : 9.80394172668457\n",
            "Training Loss : 71.46658325195312\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 9 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 2.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 2.0\n",
            "Eval_AverageEpLen : 2.0\n",
            "Train_AverageReturn : 2.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0\n",
            "Train_EnvstepsSoFar : 2018\n",
            "TimeSinceStart : 10.88434362411499\n",
            "Training Loss : 16.712783813476562\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 10 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.9950249195098877\n",
            "Eval_StdReturn : 0.07035887986421585\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9950248756218905\n",
            "Train_AverageReturn : 2.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 2.0\n",
            "Train_AverageEpLen : 2.0\n",
            "Train_EnvstepsSoFar : 2218\n",
            "TimeSinceStart : 11.57622218132019\n",
            "Training Loss : -600.74072265625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 11 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.9950249195098877\n",
            "Eval_StdReturn : 0.07035887986421585\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9950248756218905\n",
            "Train_AverageReturn : 1.9900989532470703\n",
            "Train_StdReturn : 0.0990099161863327\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.99009900990099\n",
            "Train_EnvstepsSoFar : 2419\n",
            "TimeSinceStart : 12.280822515487671\n",
            "Training Loss : 688.3045043945312\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 12 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.9704433679580688\n",
            "Eval_StdReturn : 0.1693607121706009\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.9704433497536946\n",
            "Train_AverageReturn : 1.9801980257034302\n",
            "Train_StdReturn : 0.1393192857503891\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.9801980198019802\n",
            "Train_EnvstepsSoFar : 2619\n",
            "TimeSinceStart : 13.01168417930603\n",
            "Training Loss : -77.9031982421875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 13 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.8691588640213013\n",
            "Eval_StdReturn : 0.3372265100479126\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.8691588785046729\n",
            "Train_AverageReturn : 1.9047619104385376\n",
            "Train_StdReturn : 0.2935435175895691\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.9047619047619047\n",
            "Train_EnvstepsSoFar : 2819\n",
            "TimeSinceStart : 13.719263076782227\n",
            "Training Loss : -233.00747680664062\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 14 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.6708333492279053\n",
            "Eval_StdReturn : 0.4699106216430664\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.6708333333333334\n",
            "Train_AverageReturn : 1.8348623514175415\n",
            "Train_StdReturn : 0.3713046908378601\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.834862385321101\n",
            "Train_EnvstepsSoFar : 3019\n",
            "TimeSinceStart : 14.43629765510559\n",
            "Training Loss : 349.94403076171875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 15 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.5503876209259033\n",
            "Eval_StdReturn : 0.49745461344718933\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.550387596899225\n",
            "Train_AverageReturn : 1.6666666269302368\n",
            "Train_StdReturn : 0.4714045524597168\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.6666666666666667\n",
            "Train_EnvstepsSoFar : 3219\n",
            "TimeSinceStart : 15.162601947784424\n",
            "Training Loss : 21.09356689453125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 16 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.3513513803482056\n",
            "Eval_StdReturn : 0.4773924648761749\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.3513513513513513\n",
            "Train_AverageReturn : 1.5151515007019043\n",
            "Train_StdReturn : 0.4997703731060028\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.5151515151515151\n",
            "Train_EnvstepsSoFar : 3419\n",
            "TimeSinceStart : 15.914430856704712\n",
            "Training Loss : -111.61544799804688\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 17 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.1869436502456665\n",
            "Eval_StdReturn : 0.3898662328720093\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.1869436201780414\n",
            "Train_AverageReturn : 1.2738853693008423\n",
            "Train_StdReturn : 0.44595086574554443\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.2738853503184713\n",
            "Train_EnvstepsSoFar : 3619\n",
            "TimeSinceStart : 17.45166039466858\n",
            "Training Loss : 42.28363037109375\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 18 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0958904027938843\n",
            "Eval_StdReturn : 0.294440895318985\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.095890410958904\n",
            "Train_AverageReturn : 1.1627906560897827\n",
            "Train_StdReturn : 0.369174599647522\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.1627906976744187\n",
            "Train_EnvstepsSoFar : 3819\n",
            "TimeSinceStart : 19.705907344818115\n",
            "Training Loss : 3.7706298828125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 19 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0498688220977783\n",
            "Eval_StdReturn : 0.21767374873161316\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0498687664041995\n",
            "Train_AverageReturn : 1.1235954761505127\n",
            "Train_StdReturn : 0.3291195034980774\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.1235955056179776\n",
            "Train_EnvstepsSoFar : 4019\n",
            "TimeSinceStart : 20.76677393913269\n",
            "Training Loss : -14.59869384765625\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 20 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0389610528945923\n",
            "Eval_StdReturn : 0.19350212812423706\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0389610389610389\n",
            "Train_AverageReturn : 1.0256410837173462\n",
            "Train_StdReturn : 0.1580618917942047\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0256410256410255\n",
            "Train_EnvstepsSoFar : 4219\n",
            "TimeSinceStart : 22.143650770187378\n",
            "Training Loss : 0.8729248046875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 21 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0204081535339355\n",
            "Eval_StdReturn : 0.14139191806316376\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0204081632653061\n",
            "Train_AverageReturn : 1.0309277772903442\n",
            "Train_StdReturn : 0.17312222719192505\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0309278350515463\n",
            "Train_EnvstepsSoFar : 4419\n",
            "TimeSinceStart : 23.54438042640686\n",
            "Training Loss : -0.8165283203125\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 22 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.007556676864624\n",
            "Eval_StdReturn : 0.08660006523132324\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0075566750629723\n",
            "Train_AverageReturn : 1.0152283906936646\n",
            "Train_StdReturn : 0.12246029824018478\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.015228426395939\n",
            "Train_EnvstepsSoFar : 4619\n",
            "TimeSinceStart : 24.355462789535522\n",
            "Training Loss : -0.3055419921875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 23 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 4819\n",
            "TimeSinceStart : 25.264881372451782\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 24 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0101009607315063\n",
            "Train_StdReturn : 0.09999490529298782\n",
            "Train_MaxReturn : 2.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0101010101010102\n",
            "Train_EnvstepsSoFar : 5019\n",
            "TimeSinceStart : 26.147709608078003\n",
            "Training Loss : 0.194305419921875\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 25 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0025062561035156\n",
            "Eval_StdReturn : 0.049999840557575226\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0025062656641603\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5219\n",
            "TimeSinceStart : 27.014959573745728\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 26 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5419\n",
            "TimeSinceStart : 27.877590894699097\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 27 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0050251483917236\n",
            "Eval_StdReturn : 0.07070979475975037\n",
            "Eval_MaxReturn : 2.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0050251256281406\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5619\n",
            "TimeSinceStart : 28.697996377944946\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 28 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 5819\n",
            "TimeSinceStart : 29.51886796951294\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 29 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6019\n",
            "TimeSinceStart : 30.316664934158325\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 30 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6219\n",
            "TimeSinceStart : 31.115505933761597\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 31 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6419\n",
            "TimeSinceStart : 31.904730319976807\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 32 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6619\n",
            "TimeSinceStart : 32.70109152793884\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 33 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 6819\n",
            "TimeSinceStart : 33.509360790252686\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 34 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7019\n",
            "TimeSinceStart : 34.329363107681274\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 35 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7219\n",
            "TimeSinceStart : 35.13485884666443\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 36 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7419\n",
            "TimeSinceStart : 35.92694067955017\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 37 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7619\n",
            "TimeSinceStart : 36.73137617111206\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 38 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 7819\n",
            "TimeSinceStart : 37.510390520095825\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 39 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8019\n",
            "TimeSinceStart : 38.459049701690674\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 40 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8219\n",
            "TimeSinceStart : 39.29603958129883\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 41 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8419\n",
            "TimeSinceStart : 40.15593075752258\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 42 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8619\n",
            "TimeSinceStart : 41.01355195045471\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 43 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 8819\n",
            "TimeSinceStart : 41.842546224594116\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 44 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9019\n",
            "TimeSinceStart : 42.63003206253052\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 45 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9219\n",
            "TimeSinceStart : 43.42971754074097\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 46 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9419\n",
            "TimeSinceStart : 44.292890787124634\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 47 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9619\n",
            "TimeSinceStart : 45.08415174484253\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 48 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 9819\n",
            "TimeSinceStart : 45.89687418937683\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 49 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10019\n",
            "TimeSinceStart : 46.725878953933716\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 50 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10219\n",
            "TimeSinceStart : 47.554034948349\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 51 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10419\n",
            "TimeSinceStart : 48.37035584449768\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 52 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10619\n",
            "TimeSinceStart : 49.17631554603577\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 53 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 10819\n",
            "TimeSinceStart : 49.974228382110596\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 54 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11019\n",
            "TimeSinceStart : 50.78811430931091\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 55 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11219\n",
            "TimeSinceStart : 51.59841585159302\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 56 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11419\n",
            "TimeSinceStart : 52.4066743850708\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 57 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11619\n",
            "TimeSinceStart : 53.23713135719299\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 58 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 11819\n",
            "TimeSinceStart : 54.01690149307251\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 59 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12019\n",
            "TimeSinceStart : 54.85272002220154\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 60 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12219\n",
            "TimeSinceStart : 55.68383598327637\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 61 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12419\n",
            "TimeSinceStart : 56.504743576049805\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 62 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12619\n",
            "TimeSinceStart : 57.31989550590515\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 63 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 12819\n",
            "TimeSinceStart : 58.096307039260864\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 64 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13019\n",
            "TimeSinceStart : 58.91077923774719\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 65 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13219\n",
            "TimeSinceStart : 59.71083688735962\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 66 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13419\n",
            "TimeSinceStart : 60.51718473434448\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 67 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13619\n",
            "TimeSinceStart : 61.32684564590454\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 68 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 13819\n",
            "TimeSinceStart : 62.11305260658264\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 69 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14019\n",
            "TimeSinceStart : 62.92070436477661\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 70 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14219\n",
            "TimeSinceStart : 63.72649097442627\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 71 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14419\n",
            "TimeSinceStart : 64.74695253372192\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 72 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14619\n",
            "TimeSinceStart : 66.31583285331726\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 73 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 14819\n",
            "TimeSinceStart : 67.52114152908325\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 74 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15019\n",
            "TimeSinceStart : 68.33724069595337\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 75 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15219\n",
            "TimeSinceStart : 69.13992118835449\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 76 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15419\n",
            "TimeSinceStart : 69.92262244224548\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 77 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15619\n",
            "TimeSinceStart : 70.7301378250122\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 78 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 15819\n",
            "TimeSinceStart : 71.53471064567566\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 79 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16019\n",
            "TimeSinceStart : 72.41433787345886\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 80 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16219\n",
            "TimeSinceStart : 73.22549080848694\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 81 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16419\n",
            "TimeSinceStart : 74.01695561408997\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 82 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16619\n",
            "TimeSinceStart : 74.85307264328003\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 83 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 16819\n",
            "TimeSinceStart : 75.6786630153656\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 84 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17019\n",
            "TimeSinceStart : 76.4745671749115\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 85 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17219\n",
            "TimeSinceStart : 77.27983522415161\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 86 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17419\n",
            "TimeSinceStart : 78.07573866844177\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 87 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17619\n",
            "TimeSinceStart : 78.9101288318634\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 88 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 17819\n",
            "TimeSinceStart : 79.69683003425598\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 89 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18019\n",
            "TimeSinceStart : 80.50153040885925\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 90 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18219\n",
            "TimeSinceStart : 81.31806015968323\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 91 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18419\n",
            "TimeSinceStart : 82.13007354736328\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 92 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18619\n",
            "TimeSinceStart : 82.9337227344513\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 93 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 18819\n",
            "TimeSinceStart : 83.74776649475098\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 94 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19019\n",
            "TimeSinceStart : 84.57278227806091\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 95 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19219\n",
            "TimeSinceStart : 85.36800599098206\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 96 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19419\n",
            "TimeSinceStart : 86.30183744430542\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 97 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19619\n",
            "TimeSinceStart : 87.78531002998352\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 98 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 19819\n",
            "TimeSinceStart : 88.58505463600159\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 99 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : 1.0\n",
            "Eval_StdReturn : 0.0\n",
            "Eval_MaxReturn : 1.0\n",
            "Eval_MinReturn : 1.0\n",
            "Eval_AverageEpLen : 1.0\n",
            "Train_AverageReturn : 1.0\n",
            "Train_StdReturn : 0.0\n",
            "Train_MaxReturn : 1.0\n",
            "Train_MinReturn : 1.0\n",
            "Train_AverageEpLen : 1.0\n",
            "Train_EnvstepsSoFar : 20019\n",
            "TimeSinceStart : 89.41069602966309\n",
            "Training Loss : -0.0\n",
            "Initial_DataCollection_AverageReturn : 8.038461685180664\n",
            "Done logging...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plots"
      ],
      "metadata": {
        "id": "9H_ueUtlzQcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_q2_data():\n",
        "    full_data = pd.DataFrame()\n",
        "\n",
        "    for folder in os.listdir('data'):\n",
        "        split = folder.split('_')\n",
        "        if 'q2' in split and 'b500' not in split:\n",
        "            config_list = split[split.index('q2')+1:split.index('InvertedPendulum-v2')]\n",
        "            # print('_'.join(config))\n",
        "            config = '_'.join(config_list)\n",
        "\n",
        "            logdir = os.path.join('data', folder, 'events*')\n",
        "            eventfile = glob.glob(logdir)[0]\n",
        "\n",
        "            X, Y = get_section_results(eventfile)\n",
        "            data = pd.DataFrame({'Iteration': range(len(X)), \n",
        "                                 'Config': np.repeat(config, len(X)), \n",
        "                                 'Train_EnvstepsSoFar': X, \n",
        "                                 'Eval_AverageReturn': Y})\n",
        "            data['Eval_AverageReturn_Smooth'] = data['Eval_AverageReturn'].ewm(alpha=0.6).mean()\n",
        "            full_data = pd.concat([full_data, data], axis=0, ignore_index=True)\n",
        "        \n",
        "    return full_data\n",
        "\n",
        "data_q2 = read_q2_data()\n",
        "data_q2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "4KcncCRwzTAE",
        "outputId": "ff160e73-2e3f-4bf3-9ab2-4ab9acc682b5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ca0c84f5-c2ad-40c9-9c2e-0ee50ed3f172\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iteration</th>\n",
              "      <th>Config</th>\n",
              "      <th>Train_EnvstepsSoFar</th>\n",
              "      <th>Eval_AverageReturn</th>\n",
              "      <th>Eval_AverageReturn_Smooth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>b100_r07</td>\n",
              "      <td>104.0</td>\n",
              "      <td>2.105263</td>\n",
              "      <td>2.105263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>b100_r07</td>\n",
              "      <td>204.0</td>\n",
              "      <td>33.538460</td>\n",
              "      <td>24.557546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>b100_r07</td>\n",
              "      <td>310.0</td>\n",
              "      <td>4.020000</td>\n",
              "      <td>11.392453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>b100_r07</td>\n",
              "      <td>413.0</td>\n",
              "      <td>2.784722</td>\n",
              "      <td>6.092126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>b100_r07</td>\n",
              "      <td>514.0</td>\n",
              "      <td>2.051282</td>\n",
              "      <td>3.642536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>95</td>\n",
              "      <td>b200_r01</td>\n",
              "      <td>19219.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>96</td>\n",
              "      <td>b200_r01</td>\n",
              "      <td>19419.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>97</td>\n",
              "      <td>b200_r01</td>\n",
              "      <td>19619.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>98</td>\n",
              "      <td>b200_r01</td>\n",
              "      <td>19819.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>99</td>\n",
              "      <td>b200_r01</td>\n",
              "      <td>20019.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca0c84f5-c2ad-40c9-9c2e-0ee50ed3f172')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca0c84f5-c2ad-40c9-9c2e-0ee50ed3f172 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca0c84f5-c2ad-40c9-9c2e-0ee50ed3f172');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Iteration    Config  ...  Eval_AverageReturn  Eval_AverageReturn_Smooth\n",
              "0            0  b100_r07  ...            2.105263                   2.105263\n",
              "1            1  b100_r07  ...           33.538460                  24.557546\n",
              "2            2  b100_r07  ...            4.020000                  11.392453\n",
              "3            3  b100_r07  ...            2.784722                   6.092126\n",
              "4            4  b100_r07  ...            2.051282                   3.642536\n",
              "..         ...       ...  ...                 ...                        ...\n",
              "595         95  b200_r01  ...            1.000000                   1.000000\n",
              "596         96  b200_r01  ...            1.000000                   1.000000\n",
              "597         97  b200_r01  ...            1.000000                   1.000000\n",
              "598         98  b200_r01  ...            1.000000                   1.000000\n",
              "599         99  b200_r01  ...            1.000000                   1.000000\n",
              "\n",
              "[600 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=figsize)\n",
        "sns.lineplot(data=data_q2, x='Iteration', y='Eval_AverageReturn', hue='Config')\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "l16qz4egzaUs",
        "outputId": "1581fd78-2358-4cc6-a8c9-2add4be632fc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa4239851d0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 410.4x216 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 3"
      ],
      "metadata": {
        "id": "fnBh8eChzfak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py \\\n",
        "--env_name LunarLanderContinuous-v2 --ep_len 1000 \\\n",
        "--discount 0.99 -n 100 -l 2 -s 64 -b 40000 -lr 0.005 \\\n",
        "--reward_to_go --nn_baseline --exp_name q3_b40000_r0.005"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIMqsjlYzhih",
        "outputId": "15caa676-aef4-4ee5-e8f8-e7ff28d97181"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################\n",
            "logging outputs to  /content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/scripts/../../data/q2_pg_q3_b40000_r0.005_LunarLanderContinuous-v2_04-02-2022_17-50-48\n",
            "########################\n",
            "Using GPU id 0\n",
            "\n",
            "\n",
            "********** Iteration 0 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -161.91680908203125\n",
            "Eval_StdReturn : 44.274375915527344\n",
            "Eval_MaxReturn : -119.49398803710938\n",
            "Eval_MinReturn : -236.24136352539062\n",
            "Eval_AverageEpLen : 104.75\n",
            "Train_AverageReturn : -317.3604431152344\n",
            "Train_StdReturn : 160.43338012695312\n",
            "Train_MaxReturn : -16.78313446044922\n",
            "Train_MinReturn : -719.3887939453125\n",
            "Train_AverageEpLen : 106.46684350132627\n",
            "Train_EnvstepsSoFar : 40138\n",
            "TimeSinceStart : 55.4792046546936\n",
            "Training Loss : -178.64212036132812\n",
            "Initial_DataCollection_AverageReturn : -317.3604431152344\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 1 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -238.4137725830078\n",
            "Eval_StdReturn : 137.66709899902344\n",
            "Eval_MaxReturn : -84.84529113769531\n",
            "Eval_MinReturn : -418.7413635253906\n",
            "Eval_AverageEpLen : 93.4\n",
            "Train_AverageReturn : -193.23199462890625\n",
            "Train_StdReturn : 117.60343933105469\n",
            "Train_MaxReturn : 36.60380554199219\n",
            "Train_MinReturn : -579.9155883789062\n",
            "Train_AverageEpLen : 103.63565891472868\n",
            "Train_EnvstepsSoFar : 80245\n",
            "TimeSinceStart : 111.40604496002197\n",
            "Training Loss : 23.8612060546875\n",
            "Initial_DataCollection_AverageReturn : -317.3604431152344\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 2 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -135.4813690185547\n",
            "Eval_StdReturn : 64.16896057128906\n",
            "Eval_MaxReturn : -46.05133819580078\n",
            "Eval_MinReturn : -237.09695434570312\n",
            "Eval_AverageEpLen : 96.6\n",
            "Train_AverageReturn : -157.54013061523438\n",
            "Train_StdReturn : 88.36693572998047\n",
            "Train_MaxReturn : 36.532440185546875\n",
            "Train_MinReturn : -506.3791809082031\n",
            "Train_AverageEpLen : 95.61336515513126\n",
            "Train_EnvstepsSoFar : 120307\n",
            "TimeSinceStart : 165.24663400650024\n",
            "Training Loss : 39.8948974609375\n",
            "Initial_DataCollection_AverageReturn : -317.3604431152344\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 3 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -212.2796173095703\n",
            "Eval_StdReturn : 88.91096496582031\n",
            "Eval_MaxReturn : -115.65506744384766\n",
            "Eval_MinReturn : -318.70318603515625\n",
            "Eval_AverageEpLen : 106.75\n",
            "Train_AverageReturn : -175.91842651367188\n",
            "Train_StdReturn : 109.0797119140625\n",
            "Train_MaxReturn : 44.757537841796875\n",
            "Train_MinReturn : -546.0711059570312\n",
            "Train_AverageEpLen : 95.92805755395683\n",
            "Train_EnvstepsSoFar : 160309\n",
            "TimeSinceStart : 219.9750154018402\n",
            "Training Loss : 33.39117431640625\n",
            "Initial_DataCollection_AverageReturn : -317.3604431152344\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 4 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -132.16928100585938\n",
            "Eval_StdReturn : 45.148372650146484\n",
            "Eval_MaxReturn : -86.42227935791016\n",
            "Eval_MinReturn : -217.65452575683594\n",
            "Eval_AverageEpLen : 91.0\n",
            "Train_AverageReturn : -161.86167907714844\n",
            "Train_StdReturn : 92.90079498291016\n",
            "Train_MaxReturn : 50.594390869140625\n",
            "Train_MinReturn : -519.046142578125\n",
            "Train_AverageEpLen : 94.60756501182033\n",
            "Train_EnvstepsSoFar : 200328\n",
            "TimeSinceStart : 279.2591042518616\n",
            "Training Loss : -47.35272216796875\n",
            "Initial_DataCollection_AverageReturn : -317.3604431152344\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 5 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -124.83363342285156\n",
            "Eval_StdReturn : 58.493568420410156\n",
            "Eval_MaxReturn : -74.30706787109375\n",
            "Eval_MinReturn : -231.5379180908203\n",
            "Eval_AverageEpLen : 86.4\n",
            "Train_AverageReturn : -136.16551208496094\n",
            "Train_StdReturn : 73.5666275024414\n",
            "Train_MaxReturn : 16.004653930664062\n",
            "Train_MinReturn : -447.4822692871094\n",
            "Train_AverageEpLen : 90.7641723356009\n",
            "Train_EnvstepsSoFar : 240355\n",
            "TimeSinceStart : 332.31229853630066\n",
            "Training Loss : 99.075439453125\n",
            "Initial_DataCollection_AverageReturn : -317.3604431152344\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 6 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "\n",
            "Training agent using sampled data from replay buffer...\n",
            "\n",
            "Beginning logging procedure...\n",
            "\n",
            "Collecting data for eval...\n",
            "Eval_AverageReturn : -121.3127212524414\n",
            "Eval_StdReturn : 28.59843635559082\n",
            "Eval_MaxReturn : -76.74423217773438\n",
            "Eval_MinReturn : -157.28985595703125\n",
            "Eval_AverageEpLen : 71.66666666666667\n",
            "Train_AverageReturn : -136.24571228027344\n",
            "Train_StdReturn : 75.48980712890625\n",
            "Train_MaxReturn : 55.53025817871094\n",
            "Train_MinReturn : -477.6856689453125\n",
            "Train_AverageEpLen : 93.15581395348838\n",
            "Train_EnvstepsSoFar : 280412\n",
            "TimeSinceStart : 390.42649269104004\n",
            "Training Loss : 130.1187744140625\n",
            "Initial_DataCollection_AverageReturn : -317.3604431152344\n",
            "Done logging...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "********** Iteration 7 ************\n",
            "\n",
            "Collecting data to be used for training...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py\", line 119, in sample\n",
            "    return self.rsample(sample_shape)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributions/multivariate_normal.py\", line 202, in rsample\n",
            "    return self.loc + _batch_mv(self._unbroadcasted_scale_tril, eps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributions/multivariate_normal.py\", line 20, in _batch_mv\n",
            "    return torch.matmul(bmat, bvec.unsqueeze(-1)).squeeze(-1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"cs285/scripts/run_hw2.py\", line 123, in <module>\n",
            "    main()\n",
            "  File \"cs285/scripts/run_hw2.py\", line 119, in main\n",
            "    trainer.run_training_loop()\n",
            "  File \"cs285/scripts/run_hw2.py\", line 51, in run_training_loop\n",
            "    eval_policy = self.rl_trainer.agent.actor,\n",
            "  File \"/content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/infrastructure/rl_trainer.py\", line 134, in run_training_loop\n",
            "    self.params['batch_size'])\n",
            "  File \"/content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/infrastructure/rl_trainer.py\", line 160, in collect_training_trajectories\n",
            "    paths, envsteps_this_batch = utils.sample_trajectories(self.env, collect_policy, batch_size, self.params['ep_len'])\n",
            "  File \"/content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/infrastructure/utils.py\", line 115, in sample_trajectories\n",
            "    path = sample_trajectory(env, policy, max_path_length, render, render_mode)\n",
            "  File \"/content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/infrastructure/utils.py\", line 80, in sample_trajectory\n",
            "    ac = policy.get_action(ob) # HINT: query the policy's get_action function\n",
            "  File \"/content/gdrive/My Drive/cs285_f2021/DRL_policy_grads/cs285/policies/MLP_policy.py\", line 99, in get_action\n",
            "    ac = self(obs).sample()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py\", line 119, in sample\n",
            "    return self.rsample(sample_shape)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plots"
      ],
      "metadata": {
        "id": "Tu650n0xziJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_q3_data():\n",
        "    full_data = pd.DataFrame()\n",
        "\n",
        "    for folder in os.listdir('data'):\n",
        "        split = folder.split('_')\n",
        "        if 'LunarLanderContinuous-v2' in split:\n",
        "            config_list = split[split.index('q3')+1:split.index('LunarLanderContinuous-v2')]\n",
        "            # print('_'.join(config))\n",
        "            config = '_'.join(config_list)\n",
        "\n",
        "            logdir = os.path.join('data', folder, 'events*')\n",
        "            eventfile = glob.glob(logdir)[0]\n",
        "\n",
        "            X, Y = get_section_results(eventfile)\n",
        "            data = pd.DataFrame({'Iteration': range(len(X)), \n",
        "                                 'Config': np.repeat(config, len(X)), \n",
        "                                 'Train_EnvstepsSoFar': X, \n",
        "                                 'Eval_AverageReturn': Y})\n",
        "            data['Eval_AverageReturn_Smooth'] = data['Eval_AverageReturn'].ewm(alpha=0.6).mean()\n",
        "            full_data = pd.concat([full_data, data], axis=0, ignore_index=True)\n",
        "        \n",
        "    return full_data\n",
        "\n",
        "data_q3 = read_q3_data()\n",
        "data_q3"
      ],
      "metadata": {
        "id": "Dvv7f1xtzjyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=figsize)\n",
        "sns.lineplot(data=data_q3, x='Iteration', y='Eval_AverageReturn_Smooth', hue='Config')\n",
        "\n",
        "plt.savefig(os.path.join(export_dir, 'q3.pdf'), bbox_inches='tight')"
      ],
      "metadata": {
        "id": "hfw9m4gg0Xbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Expirement4"
      ],
      "metadata": {
        "id": "WtpB85Ml0sAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10000, 30000, 50000  ,, 0.005, 0.01, 0.02"
      ],
      "metadata": {
        "id": "nlmXzf5b3_Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.005 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b10000_lr005_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "r7Lv3fFu0rrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.01 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b10000_lr01_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "VAdjfTss746S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 10000 -lr 0.02 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b10000_lr02_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "OndS8iGu8UcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 30000 -lr 0.005 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b30000_lr005_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "JPf-aE6a8WYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 30000 -lr 0.01 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b30000_lr01_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "oJvcIWYs8Zm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 30000 -lr 0.02 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b30000_lr02_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "Z-IJ-aw58bbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 50000 -lr 0.005 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b50000_lr005_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "imA3uAZl8f2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 50000 -lr 0.01 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b50000_lr01_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "mmhsNWDv8fnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 50000 -lr 0.02 -rtg --nn_baseline \\\n",
        "--exp_name q4_search_b50000_lr02_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "j9liUxOv8fjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plots"
      ],
      "metadata": {
        "id": "YJxO8TAC8pwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_q4_data():\n",
        "    full_data = pd.DataFrame()\n",
        "\n",
        "    for folder in os.listdir('data'):\n",
        "        split = folder.split('_')\n",
        "        if 'HalfCheetah-v2' in split and 'search' in split:\n",
        "            config_list = split[split.index('search')+1:split.index('rtg')]\n",
        "            # print('_'.join(config))\n",
        "            config = '_'.join(config_list)\n",
        "\n",
        "            logdir = os.path.join('data', folder, 'events*')\n",
        "            eventfile = glob.glob(logdir)[0]\n",
        "\n",
        "            X, Y = get_section_results(eventfile)\n",
        "            data = pd.DataFrame({'Iteration': range(len(X)), \n",
        "                                 'Config': np.repeat(config, len(X)), \n",
        "                                 'Train_EnvstepsSoFar': X, \n",
        "                                 'Eval_AverageReturn': Y})\n",
        "            data['Eval_AverageReturn_Smooth'] = data['Eval_AverageReturn'].ewm(alpha=0.6).mean()\n",
        "\n",
        "            full_data = pd.concat([full_data, data], axis=0, ignore_index=True)\n",
        "        \n",
        "    return full_data\n",
        "\n",
        "data_q4 = read_q4_data()\n",
        "data_q4"
      ],
      "metadata": {
        "id": "jixE28vS8pg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=figsize)\n",
        "sns.lineplot(data=data_q4, x='Iteration', y='Eval_AverageReturn_Smooth', hue='Config')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n"
      ],
      "metadata": {
        "id": "poac9s0K8u_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 50000 -lr 0.02 \\\n",
        "--exp_name q4_b50000_r02"
      ],
      "metadata": {
        "id": "r6Vr55Y-9Fo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 50000 -lr 0.02 -rtg \\\n",
        "--exp_name q4_b50000_r02_rtg"
      ],
      "metadata": {
        "id": "H5Q79wMI9Qe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 50000 -lr 0.02 --nn_baseline \\\n",
        "--exp_name q4_b50000_r02_nnbaseline"
      ],
      "metadata": {
        "id": "lwYZZrOv9XuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py --env_name HalfCheetah-v2 --ep_len 150 \\\n",
        "--discount 0.95 -n 100 -l 2 -s 32 -b 50000 -lr 0.02 -rtg --nn_baseline \\\n",
        "--exp_name q4_b50000_r02_rtg_nnbaseline"
      ],
      "metadata": {
        "id": "G9T2Dgjd9e6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_q4_optimal_data():\n",
        "    full_data = pd.DataFrame()\n",
        "\n",
        "    for folder in os.listdir('data'):\n",
        "        split = folder.split('_')\n",
        "        if 'HalfCheetah-v2' in split and 'search' not in split:\n",
        "            config_list = split[split.index('q4')+1:split.index('HalfCheetah-v2')]\n",
        "            # print('_'.join(config))\n",
        "            config = '_'.join(config_list)\n",
        "\n",
        "            logdir = os.path.join('data', folder, 'events*')\n",
        "            eventfile = glob.glob(logdir)[0]\n",
        "\n",
        "            X, Y = get_section_results(eventfile)\n",
        "            data = pd.DataFrame({'Iteration': range(len(X)), \n",
        "                                 'Config': np.repeat(config, len(X)), \n",
        "                                 'Train_EnvstepsSoFar': X, \n",
        "                                 'Eval_AverageReturn': Y})\n",
        "            data['Eval_AverageReturn_Smooth'] = data['Eval_AverageReturn'].ewm(alpha=0.6).mean()\n",
        "\n",
        "            full_data = pd.concat([full_data, data], axis=0, ignore_index=True)\n",
        "        \n",
        "    return full_data\n",
        "\n",
        "data_q4_optimal = read_q4_optimal_data()\n",
        "data_q4_optimal"
      ],
      "metadata": {
        "id": "kgs_LZ8m8zov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=figsize)\n",
        "sns.lineplot(data=data_q4_optimal, x='Iteration', y='Eval_AverageReturn_Smooth', hue='Config')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n"
      ],
      "metadata": {
        "id": "r1lup24A82-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 5"
      ],
      "metadata": {
        "id": "fybtCZON880y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0, 0.95, 0.99, 1"
      ],
      "metadata": {
        "id": "sFg2O8kF904X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py \\\n",
        "--env_name Hopper-v2 --ep_len 1000 \\\n",
        "--discount 0.99 -n 300 -l 2 -s 32 -b 2000 -lr 0.001 \\\n",
        "--reward_to_go --nn_baseline --action_noise_std 0.5 --gae_lambda 0 \\\n",
        "--exp_name q5_b2000_r0.001_lambda0"
      ],
      "metadata": {
        "id": "Ks2SNQ5I9qdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py \\\n",
        "--env_name Hopper-v2 --ep_len 1000 \\\n",
        "--discount 0.99 -n 300 -l 2 -s 32 -b 2000 -lr 0.001 \\\n",
        "--reward_to_go --nn_baseline --action_noise_std 0.5 --gae_lambda 0.95 \\\n",
        "--exp_name q5_b2000_r0.001_lambda95"
      ],
      "metadata": {
        "id": "3Jqjmyaa9w9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py \\\n",
        "--env_name Hopper-v2 --ep_len 1000 \\\n",
        "--discount 0.99 -n 300 -l 2 -s 32 -b 2000 -lr 0.001 \\\n",
        "--reward_to_go --nn_baseline --action_noise_std 0.5 --gae_lambda 0.99 \\\n",
        "--exp_name q5_b2000_r0.001_lambda99"
      ],
      "metadata": {
        "id": "PKALQAqn9xjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cs285/scripts/run_hw2.py \\\n",
        "--env_name Hopper-v2 --ep_len 1000 \\\n",
        "--discount 0.99 -n 300 -l 2 -s 32 -b 2000 -lr 0.001 \\\n",
        "--reward_to_go --nn_baseline --action_noise_std 0.5 --gae_lambda 1 \\\n",
        "--exp_name q5_b2000_r0.001_lambda1"
      ],
      "metadata": {
        "id": "8zvaO2u19-2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plots"
      ],
      "metadata": {
        "id": "rjJlZMc59o7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "grCIAL0U9qzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km7LlYvhqKTl"
      },
      "outputs": [],
      "source": [
        "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
        "\n",
        "## requires tensorflow==2.3.0\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/cs285_f2021/homework_fall2021/hw2/data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw2_MCO.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}